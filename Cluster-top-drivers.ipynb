{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2b2298-1744-4a1b-9bb1-5952ec75be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-2.2.5.tar.gz (9.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from cohere) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from requests->cohere) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from requests->cohere) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from requests->cohere) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from requests->cohere) (1.26.9)\n",
      "Building wheels for collected packages: cohere\n",
      "  Building wheel for cohere (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cohere: filename=cohere-2.2.5-cp37-cp37m-macosx_10_9_x86_64.whl size=10460 sha256=913f55b575e857c8ca73ab4257071a3b8903f761caef03bc4df306d2edf18b68\n",
      "  Stored in directory: /Users/ankush.singal/Library/Caches/pip/wheels/eb/19/2d/89553daf06bc949f1aeb4cfae6e1e306d7763b22ffe91e45fa\n",
      "Successfully built cohere\n",
      "Installing collected packages: cohere\n",
      "Successfully installed cohere-2.2.5\n",
      "Requirement already satisfied: altair in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (4.4.0)\n",
      "Requirement already satisfied: entrypoints in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (0.4)\n",
      "Requirement already satisfied: numpy in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.18 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (1.3.5)\n",
      "Requirement already satisfied: toolz in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (4.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (4.11.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair) (5.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from pandas>=0.18->altair) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from pandas>=0.18->altair) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jinja2->altair) (2.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.18->altair) (1.16.0)\n",
      "Requirement already satisfied: altair-data-server in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: tornado in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair-data-server) (6.1)\n",
      "Requirement already satisfied: portpicker in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair-data-server) (1.5.2)\n",
      "Requirement already satisfied: altair in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair-data-server) (4.2.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (4.4.0)\n",
      "Requirement already satisfied: numpy in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.18 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (1.3.5)\n",
      "Requirement already satisfied: entrypoints in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (0.4)\n",
      "Requirement already satisfied: toolz in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from altair->altair-data-server) (3.0.3)\n",
      "Requirement already satisfied: psutil in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from portpicker->altair-data-server) (5.9.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair->altair-data-server) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair->altair-data-server) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair->altair-data-server) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair->altair-data-server) (4.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jsonschema>=3.0->altair->altair-data-server) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from pandas>=0.18->altair->altair-data-server) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from pandas>=0.18->altair->altair-data-server) (2022.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from jinja2->altair->altair-data-server) (2.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair->altair-data-server) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.18->altair->altair-data-server) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere\n",
    "!pip install altair\n",
    "!pip install altair-data-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8797d2c-9170-4a45-b044-8edcc43114cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annoy in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97751b2-5965-482f-9bd5-df7b834c28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries (Run this cell to execute required code) {display-mode: \"form\"}\n",
    "import cohere\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from annoy import AnnoyIndex\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic._ctfidf import ClassTFIDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d7e092-688f-4d68-9eec-237313e061d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a DataFrame with 3000 rows and an embeddings matrix of dimensions (3000, 1024)\n"
     ]
    }
   ],
   "source": [
    "#Load the embeddings matrix\n",
    "embeds = np.load('askhn3k_embeds.npy')\n",
    "df = pd.read_csv('databricks2.csv',usecols=['title'])\n",
    "df1 = df.sample(n=3000,random_state=1) \n",
    "print(f'Loaded a DataFrame with {len(df1)} rows and an embeddings matrix of dimensions {embeds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4e625e-82e4-4389-8cdb-223ebd52c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14035e42-d11f-4e0d-9eec-0057e9f52751",
   "metadata": {},
   "source": [
    "# Building a semantic search index\n",
    "For nearest-neighbor search, we can use the open-source Annoy library. Let's create a semantic search index and feed it all the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87843ca7-72f9-43c1-9caf-4cd6cbc7af03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the search index, pass the size of embedding\n",
    "search_index = AnnoyIndex(embeds.shape[1], 'angular')\n",
    "# Add all the vectors to the search index\n",
    "for i in range(len(embeds)):\n",
    "    search_index.add_item(i, embeds[i])\n",
    "\n",
    "search_index.build(10) # 10 trees\n",
    "search_index.save('askhn.ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b88588-e946-4a7b-8203-26add898113b",
   "metadata": {},
   "source": [
    "# 1- Given an existing post title, retrieve the most similar posts (nearest neighbor search using embeddings)\n",
    "We can query neighbors of a specific post using get_nns_by_item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16727870-cb1d-42ec-9a73-de6f608c0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query post:'2203220030000505'\n",
      "Nearest neighbors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post titles</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Feature Store API in Docker</td>\n",
       "      <td>0.782082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>Failure to add user to a Databricks workspace with an error</td>\n",
       "      <td>0.832201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>What cluster manager types on Spark on Databricks</td>\n",
       "      <td>0.832643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>Unable to start a cluster</td>\n",
       "      <td>0.932350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>job failure/long running without change to job.</td>\n",
       "      <td>0.967258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>Can we send a failure with the error logs when job failed</td>\n",
       "      <td>0.976471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2204250010000575 | SPN secret</td>\n",
       "      <td>0.979501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>Docker image pull failure ECR</td>\n",
       "      <td>0.987894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>[ARR][ADOBE][2203100010002839 ]Need increase TBL_NAME column size in databricks meta store</td>\n",
       "      <td>0.997030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     post titles  \\\n",
       "731                                                                  Feature Store API in Docker   \n",
       "2691                                 Failure to add user to a Databricks workspace with an error   \n",
       "1859                                           What cluster manager types on Spark on Databricks   \n",
       "1965                                                                   Unable to start a cluster   \n",
       "2123                                             job failure/long running without change to job.   \n",
       "2910                                   Can we send a failure with the error logs when job failed   \n",
       "1603                                                               2204250010000575 | SPN secret   \n",
       "2599                                                               Docker image pull failure ECR   \n",
       "1206  [ARR][ADOBE][2203100010002839 ]Need increase TBL_NAME column size in databricks meta store   \n",
       "\n",
       "      distance  \n",
       "731   0.782082  \n",
       "2691  0.832201  \n",
       "1859  0.832643  \n",
       "1965  0.932350  \n",
       "2123  0.967258  \n",
       "2910  0.976471  \n",
       "1603  0.979501  \n",
       "2599  0.987894  \n",
       "1206  0.997030  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an example (we'll retrieve others similar to it)\n",
    "example_id = 50\n",
    "\n",
    "# Retrieve nearest neighbors\n",
    "similar_item_ids = search_index.get_nns_by_item(example_id,\n",
    "                                                10, # Number of results to retrieve\n",
    "                                                include_distances=True)\n",
    "\n",
    "# Format and print the text and distances\n",
    "results = pd.DataFrame(data={'post titles': df1.iloc[similar_item_ids[0]]['title'], \n",
    "                             'distance': similar_item_ids[1]}).drop(example_id)\n",
    "\n",
    "print(f\"Query post:'{df1.iloc[example_id]['title']}'\\nNearest neighbors:\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcb80e-6711-417a-b416-51cc9ed87684",
   "metadata": {},
   "source": [
    "# 3- Plot the archive of articles by similarity\n",
    "What if we want to browse the archive instead of only searching it. Let's plot out all the questions onto a 2D chart so you're able to visualize the posts in the archive and their similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd0eb8d9-7961-4f2a-ba19-1912d21b413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (0.5.3)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (0.5.7)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (0.55.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (1.7.3)\n",
      "Requirement already satisfied: tqdm in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from umap-learn) (1.21.6)\n",
      "Requirement already satisfied: setuptools in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (61.2.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (0.38.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ankush.singal/opt/anaconda3/envs/ml_env/lib/python3.7/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271bf702-5e8a-483c-aaa8-09babafa3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP(n_neighbors=100) \n",
    "umap_embeds = reducer.fit_transform(embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d73b4f1-eeb5-43e7-b6d5-34418f5a8af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-03f0eac245bc4357a13c32d818d1392e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-03f0eac245bc4357a13c32d818d1392e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-03f0eac245bc4357a13c32d818d1392e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"background\": \"#FDF7F0\"}, \"data\": {\"name\": \"data-a5ba860f1111b8f975b0de93425c4848\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"field\": \"title\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false}, \"field\": \"x\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false}, \"field\": \"y\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}}, \"height\": 400, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Ask HN: top 3,000 posts\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a5ba860f1111b8f975b0de93425c4848\": [{\"index\": 2624, \"title\": \"Deprecated Cluster Runtime 6.4 unable to hit API'S\", \"x\": 6.804984092712402, \"y\": 4.725465297698975}, {\"index\": 5493, \"title\": \"Heartbeat issues for larger dataset\", \"x\": 6.361810684204102, \"y\": 3.2254459857940674}, {\"index\": 5565, \"title\": \"Delta files saving data type as String for Boolean column\", \"x\": 9.575385093688965, \"y\": 2.68021559715271}, {\"index\": 3553, \"title\": \"Databricks Spring Core Vulnerability Impact\", \"x\": 6.184659004211426, \"y\": 2.479959487915039}, {\"index\": 1198, \"title\": \"Not able to create JDBC connection\", \"x\": 9.715445518493652, \"y\": 6.891145706176758}, {\"index\": 1236, \"title\": \"2205100040000587 | Contributor role\", \"x\": 8.05957317352295, \"y\": 3.436418056488037}, {\"index\": 161, \"title\": \"About the specification to output the audit log to S3\", \"x\": 7.445266246795654, \"y\": 6.145352840423584}, {\"index\": 4357, \"title\": \"Issues saving spark udf applied dataframe as delta table\", \"x\": 7.310344219207764, \"y\": 2.954913377761841}, {\"index\": 354, \"title\": \"databricks workspace does not return status of all the completed jobs using REST API\", \"x\": 8.007979393005371, \"y\": 6.575601100921631}, {\"index\": 4463, \"title\": \" 2203190030000090 -  GA - Databricks Notebooks are failing with Job aborted issue 'org.apache.spark.SparkException: Job aborted'\", \"x\": 9.74201488494873, \"y\": 3.312908411026001}, {\"index\": 1745, \"title\": \"2205020030000861\", \"x\": 8.370864868164062, \"y\": 1.0210890769958496}, {\"index\": 3242, \"title\": \"How to use R studio for Databricks\", \"x\": 6.9362711906433105, \"y\": 5.121244430541992}, {\"index\": 5410, \"title\": \"2202150040006735: MLFlow model version pending to be deployed forever\", \"x\": 10.194960594177246, \"y\": 2.8430471420288086}, {\"index\": 4669, \"title\": \"Lost worker errors causing Databricks job to never finish\", \"x\": 8.690543174743652, \"y\": 3.9362239837646484}, {\"index\": 5512, \"title\": \"Incorrect scope when using named_windows in CTEs\", \"x\": 5.811020374298096, \"y\": 4.496129035949707}, {\"index\": 2850, \"title\": \"Databricks Accessing other Deployment\", \"x\": 7.674925804138184, \"y\": 3.6313083171844482}, {\"index\": 645, \"title\": \"2205160030000400\", \"x\": 7.810173988342285, \"y\": 3.49544095993042}, {\"index\": 605, \"title\": \"Having problem creating or deleting views\", \"x\": 6.477552890777588, \"y\": 4.9785308837890625}, {\"index\": 1740, \"title\": \"Streaming data processing getting stuck after few hours\", \"x\": 7.593764305114746, \"y\": 1.3847757577896118}, {\"index\": 3722, \"title\": \"cannot use pex to ship the python environment in databricks\", \"x\": 7.640949726104736, \"y\": 2.376166820526123}, {\"index\": 1242, \"title\": \"2205060010001677 | Delete tables\", \"x\": 7.307214736938477, \"y\": 2.2154858112335205}, {\"index\": 3354, \"title\": \"User accounts in Manage Contacts\", \"x\": 7.850048542022705, \"y\": 3.0421714782714844}, {\"index\": 3060, \"title\": \"Snowflake SSO python connector\", \"x\": 10.248149871826172, \"y\": 3.9073212146759033}, {\"index\": 21, \"title\": \"QA-UAT Users status is inactive\", \"x\": 5.753688335418701, \"y\": 4.080215930938721}, {\"index\": 1271, \"title\": \"2204270030000882\", \"x\": 5.281337261199951, \"y\": 1.425893783569336}, {\"index\": 1790, \"title\": \"ARR: multiple notebook run happening at the same time with different parameters:SR2204210060006686\", \"x\": 6.689145088195801, \"y\": 4.093225955963135}, {\"index\": 1697, \"title\": \"SQLEndpoint creating nodes more than active\", \"x\": 5.60343074798584, \"y\": 1.6386449337005615}, {\"index\": 5574, \"title\": \"need support for optimization of spark joins\", \"x\": 9.2677583694458, \"y\": 4.867823600769043}, {\"index\": 1840, \"title\": \"Need access to create private workspace\", \"x\": 9.800116539001465, \"y\": 5.463656425476074}, {\"index\": 2514, \"title\": \"2204200060000894 |  Procter and Gamble | ARR\", \"x\": 7.887264728546143, \"y\": 3.5335888862609863}, {\"index\": 4109, \"title\": \"Add package to a spark-submit job\", \"x\": 10.340376853942871, \"y\": 4.819302558898926}, {\"index\": 4371, \"title\": \"Spark Streaming Job Failure\", \"x\": 8.178668975830078, \"y\": 3.578970432281494}, {\"index\": 255, \"title\": \"2205250040009039 \", \"x\": 6.974679470062256, \"y\": 3.981516122817993}, {\"index\": 2846, \"title\": \"Databricks Academy Course Catalog Training Links Not Working\", \"x\": 10.244928359985352, \"y\": 2.3181140422821045}, {\"index\": 5323, \"title\": \"Job failed with error message Unexpected infra fault while creating the cluster \", \"x\": 9.782889366149902, \"y\": 6.5954694747924805}, {\"index\": 1179, \"title\": \"How to install older release of python\", \"x\": 7.633602142333984, \"y\": 5.886397838592529}, {\"index\": 141, \"title\": \"Outage May 25\", \"x\": 5.018256187438965, \"y\": 1.8493268489837646}, {\"index\": 5299, \"title\": \"VNET databricks jobs running slow - 2\", \"x\": 4.263340950012207, \"y\": 1.7725677490234375}, {\"index\": 667, \"title\": \"Databricks JDBC driver strips list of columns in insert statement\", \"x\": 8.371362686157227, \"y\": 5.492720127105713}, {\"index\": 1998, \"title\": \"Job cluster with credential passthrough fails with error \\\"Could not find ADLS Gen2 Token\\\"\", \"x\": 8.785499572753906, \"y\": 6.16834831237793}, {\"index\": 3370, \"title\": \"model serving cluster issue\", \"x\": 6.556899547576904, \"y\": 1.706602692604065}, {\"index\": 4, \"title\": \"ARR Customer CVS: Issue - Unable to grant read/list permission to Databricks service principal to KeyVault, Key Not Found\", \"x\": 10.334280014038086, \"y\": 5.085218906402588}, {\"index\": 1921, \"title\": \"AWS S3 Autoloader exception after a few run\", \"x\": 7.8364739418029785, \"y\": 3.4519944190979004}, {\"index\": 1937, \"title\": \"Databricks SQL will not load at all in Safari\", \"x\": 7.940277099609375, \"y\": 4.141211032867432}, {\"index\": 2756, \"title\": \"How do you change the owner of a job?\", \"x\": 9.93319320678711, \"y\": 5.3519134521484375}, {\"index\": 2722, \"title\": \"SQL\", \"x\": 9.137823104858398, \"y\": 6.335472106933594}, {\"index\": 2800, \"title\": \"Significant performance degradation when upgrating from 9.1 ML LTS to 10.4 ML LTS. 'Process Test Performance'\", \"x\": 9.807168006896973, \"y\": 2.3946421146392822}, {\"index\": 433, \"title\": \"ARR- ATT- Clusters with GPU won't start- 2205200040006767 \", \"x\": 10.56610107421875, \"y\": 2.9843900203704834}, {\"index\": 1926, \"title\": \"Follow up case: 00143248\", \"x\": 9.80964183807373, \"y\": 6.868714332580566}, {\"index\": 763, \"title\": \"Error importing Azure DevOps Repositories to Databricks workspace\", \"x\": 9.775660514831543, \"y\": 4.1182451248168945}, {\"index\": 4348, \"title\": \"2203220030000505\", \"x\": 8.065956115722656, \"y\": 2.011432409286499}, {\"index\": 4267, \"title\": \"We can't upgrade to current runtime version 10.x\", \"x\": 7.161105632781982, \"y\": 5.729001998901367}, {\"index\": 359, \"title\": \"All Biogen Databricks workspaces are down\", \"x\": 7.229385852813721, \"y\": 5.083556652069092}, {\"index\": 1837, \"title\": \"Ganglia Historical metrics snapshots issue\", \"x\": 6.866076469421387, \"y\": 3.9661355018615723}, {\"index\": 420, \"title\": \"Databricks Issue\", \"x\": 6.564440727233887, \"y\": 5.110802173614502}, {\"index\": 2262, \"title\": \"Billing information\", \"x\": 9.138938903808594, \"y\": 6.611952304840088}, {\"index\": 4629, \"title\": \"ARR - Time out Error with internal metastore\", \"x\": 9.167105674743652, \"y\": 6.674600601196289}, {\"index\": 5225, \"title\": \"SCIM API problems: certain workspaces hang and cannot filter on deactivated users\", \"x\": 9.703058242797852, \"y\": 5.331511497497559}, {\"index\": 4445, \"title\": \"Followup of SF ticket 00137008 \", \"x\": 7.110067367553711, \"y\": 4.04410457611084}, {\"index\": 2680, \"title\": \"2204110030000463 Connectivity issues when pulling image from ACR\", \"x\": 9.809032440185547, \"y\": 6.725615978240967}, {\"index\": 4675, \"title\": \"2203160040000832 |  Symantec Corporation | ARR\", \"x\": 8.793073654174805, \"y\": 1.7700916528701782}, {\"index\": 5107, \"title\": \"Spark streaming submitted with 300 seconds batch interval is not working as expected and by default it takes 1 second | ARR SR# 2203080040001036\", \"x\": 8.198179244995117, \"y\": 5.184549331665039}, {\"index\": 5429, \"title\": \"Not able to connect to host from databricks shard using host name\", \"x\": 10.36624526977539, \"y\": 5.064277172088623}, {\"index\": 3417, \"title\": \"Getting RPC Response Too Large Issue\", \"x\": 7.788481712341309, \"y\": 3.423362970352173}, {\"index\": 4285, \"title\": \"ARR | AT&T | 2203100040006559 | Workspace connectivity to storage account fails with HTTP Error -1TTP \", \"x\": 9.16463565826416, \"y\": 6.640218734741211}, {\"index\": 705, \"title\": \"Cannot send email from Databricks\", \"x\": 7.673836708068848, \"y\": 3.7190372943878174}, {\"index\": 1233, \"title\": \"ARR | exception when doing a vacuum| 2204290050000799\", \"x\": 5.671158313751221, \"y\": 4.244853973388672}, {\"index\": 3468, \"title\": \"2204040040005575 | synapse jdbc connector\", \"x\": 5.6794843673706055, \"y\": 3.788254737854004}, {\"index\": 4926, \"title\": \"ARR | Adobe| S500 | highly escalated | Cosmos connection issue needs packet capture\", \"x\": 6.265378952026367, \"y\": 2.3493173122406006}, {\"index\": 1899, \"title\": \"Not able to access the ADLS (cagsensedevdatahgrbhxfi ) Container 'cagsense'through Databricks\", \"x\": 8.363846778869629, \"y\": 1.0436233282089233}, {\"index\": 1045, \"title\": \"gar for https://adb-3018353714729944.4.azuredatabricks.net/\", \"x\": 9.437878608703613, \"y\": 6.137423992156982}, {\"index\": 1851, \"title\": \"Not able to open SQL UI on any other workspace\", \"x\": 8.754544258117676, \"y\": 6.768975257873535}, {\"index\": 343, \"title\": \"SQL Parse exception on SQL analytics, succeeds on Athena\", \"x\": 7.7469916343688965, \"y\": 2.047353506088257}, {\"index\": 2133, \"title\": \"Scheduled Job Unable to Run\", \"x\": 6.944088935852051, \"y\": 5.402744293212891}, {\"index\": 1676, \"title\": \"Error connecting to Trino JDBC\", \"x\": 10.417709350585938, \"y\": 3.667799711227417}, {\"index\": 2032, \"title\": \"ADB Job Getting Stuck\", \"x\": 9.190746307373047, \"y\": 3.889859676361084}, {\"index\": 2645, \"title\": \"Sql Scripts getting Errored Out during execution\", \"x\": 7.908679485321045, \"y\": 1.1233919858932495}, {\"index\": 4567, \"title\": \"how to add a spark conf for all clusters in my workspace\", \"x\": 10.358542442321777, \"y\": 5.259257793426514}, {\"index\": 3456, \"title\": \"Jobs failing due to unavailability of DBFS service\", \"x\": 6.05025577545166, \"y\": 3.2115375995635986}, {\"index\": 116, \"title\": \"connection issue with powerbi  ssl read\", \"x\": 8.975630760192871, \"y\": 3.0125203132629395}, {\"index\": 136, \"title\": \"Databricks unexplained errors\", \"x\": 10.438471794128418, \"y\": 2.3914546966552734}, {\"index\": 407, \"title\": \"2205200030001371\", \"x\": 8.415541648864746, \"y\": 1.5179811716079712}, {\"index\": 2408, \"title\": \"2204140040005042 | AT&T Services, Inc. | ARR\", \"x\": 9.756065368652344, \"y\": 6.7396626472473145}, {\"index\": 4108, \"title\": \"Create new cluster with Graviton Error\", \"x\": 6.4626078605651855, \"y\": 0.6768031716346741}, {\"index\": 4392, \"title\": \"Daily optimize job failures\", \"x\": 10.309718132019043, \"y\": 4.481673717498779}, {\"index\": 4244, \"title\": \"Cost Analysis  & Advisor Recommendations\", \"x\": 8.0545072555542, \"y\": 4.916624546051025}, {\"index\": 2388, \"title\": \"SSL connectivity to container registry from Databricks\", \"x\": 8.624353408813477, \"y\": 2.301455020904541}, {\"index\": 1832, \"title\": \"Cluster Starting Issue\", \"x\": 7.993951797485352, \"y\": 5.105624198913574}, {\"index\": 5434, \"title\": \"Job failure everytime i run a specific command\", \"x\": 10.234380722045898, \"y\": 5.691650867462158}, {\"index\": 3896, \"title\": \"Querying table with csv/text data source fails in Databricks SQL\", \"x\": 9.655591011047363, \"y\": 3.8840901851654053}, {\"index\": 5272, \"title\": \"ExecutorLostFailure - spark specialist wanted to investigate jar job causing OutOfMemory\", \"x\": 9.530599594116211, \"y\": 5.725575923919678}, {\"index\": 4441, \"title\": \"[ARR][CSS][Request to enable Delta Live Tables on workspaces]\", \"x\": 9.586404800415039, \"y\": 5.451093673706055}, {\"index\": 2237, \"title\": \"Can't see tables names in the data explorer\", \"x\": 5.3220601081848145, \"y\": 1.8063231706619263}, {\"index\": 200, \"title\": \"Outage issue\", \"x\": 4.376305103302002, \"y\": 0.937553882598877}, {\"index\": 1034, \"title\": \"Databricks Library Installation Fails\", \"x\": 10.184276580810547, \"y\": 5.736207962036133}, {\"index\": 2943, \"title\": \"2204120030002145\", \"x\": 7.740540504455566, \"y\": 5.86185359954834}, {\"index\": 3495, \"title\": \"CVE 2022-22965 (SPRING4SHELL) VULNERABILITY\", \"x\": 9.776390075683594, \"y\": 6.042448997497559}, {\"index\": 2461, \"title\": \"Ad group user not able to access\", \"x\": 6.11126184463501, \"y\": 3.3205220699310303}, {\"index\": 582, \"title\": \"Unable to change network config despite shutting down all EC2s\", \"x\": 9.725517272949219, \"y\": 5.415309906005859}, {\"index\": 1227, \"title\": \"Intermittent issue with custom library\", \"x\": 7.965188503265381, \"y\": 1.0230982303619385}, {\"index\": 5143, \"title\": \"Jobs 2.0 Update API not properly updating docker_image for cluster \", \"x\": 6.374147891998291, \"y\": 4.083110809326172}, {\"index\": 988, \"title\": \"Job were running slow and few nodes getting skipped\", \"x\": 10.254471778869629, \"y\": 3.1229190826416016}, {\"index\": 4245, \"title\": \"Rerouting cluster logs to adls gen2\", \"x\": 6.643034934997559, \"y\": 4.529275417327881}, {\"index\": 1262, \"title\": \"Trying to get the size of folders in dbfs\", \"x\": 10.67138671875, \"y\": 4.293705940246582}, {\"index\": 1560, \"title\": \" Query using odbc is getting stuck\", \"x\": 9.34127140045166, \"y\": 6.899202823638916}, {\"index\": 1771, \"title\": \"geine 2204080030001854\", \"x\": 4.328326225280762, \"y\": 0.9938497543334961}, {\"index\": 33, \"title\": \"Missing tables\", \"x\": 10.449527740478516, \"y\": 2.4191863536834717}, {\"index\": 4738, \"title\": \"Unable to read data from views created on presto\", \"x\": 7.7971510887146, \"y\": 1.5244240760803223}, {\"index\": 383, \"title\": \"2205130050001971 access to DBFS from SQL endpoint\", \"x\": 7.555556297302246, \"y\": 3.0677483081817627}, {\"index\": 795, \"title\": \"Unity Catalog integration with third-party tools\", \"x\": 7.909689426422119, \"y\": 5.952136993408203}, {\"index\": 28, \"title\": \"Command stucks when submitted\", \"x\": 7.200223922729492, \"y\": 5.875706195831299}, {\"index\": 5230, \"title\": \"DBR 9.1 - Losing 50% of executors in 1 hour\", \"x\": 8.285225868225098, \"y\": 1.151530385017395}, {\"index\": 1342, \"title\": \"Operation on deltatable is slow {Prob impacts}\", \"x\": 8.385087966918945, \"y\": 5.166886329650879}, {\"index\": 4063, \"title\": \"Service Principal duplication during multiple POST requests\", \"x\": 9.83928394317627, \"y\": 6.854384899139404}, {\"index\": 2918, \"title\": \" Error on AzureADAuthenticator.getTokenCall\", \"x\": 4.21966028213501, \"y\": 0.9129397869110107}, {\"index\": 2905, \"title\": \"Effective Routes and NSG restrictions traffic issue\", \"x\": 7.709098815917969, \"y\": 2.0046496391296387}, {\"index\": 3484, \"title\": \"Append to existing table with Delta Live Tables\", \"x\": 4.731473922729492, \"y\": 0.47686484456062317}, {\"index\": 3829, \"title\": \"<Creating for Sireesha Modumudi> AT&T - 2203290040006816\", \"x\": 7.064853191375732, \"y\": 5.951682090759277}, {\"index\": 3404, \"title\": \"steps to grant read access to a user on databricks scope and secret\", \"x\": 8.97171401977539, \"y\": 6.354265213012695}, {\"index\": 4380, \"title\": \"schema Merge Failure\", \"x\": 6.876990795135498, \"y\": 2.5790016651153564}, {\"index\": 147, \"title\": \"Delta Lake Change Feed Data error on job restart\", \"x\": 9.96955680847168, \"y\": 3.209397554397583}, {\"index\": 44, \"title\": \" R Notebook fails sometimes; fails only at specific time every day\", \"x\": 9.775067329406738, \"y\": 4.272820949554443}, {\"index\": 3679, \"title\": \"CSS-2203280040003823-Need details on tables created in DBFS path\", \"x\": 6.72706937789917, \"y\": 5.736714839935303}, {\"index\": 1289, \"title\": \"from_json: Space in column name in schema string literal\", \"x\": 10.19493579864502, \"y\": 4.678826808929443}, {\"index\": 4359, \"title\": \"Cannot authenticate to databricks cli\", \"x\": 6.535072326660156, \"y\": 2.499091386795044}, {\"index\": 4594, \"title\": \"Follow up of 00134523\", \"x\": 7.6304612159729, \"y\": 3.024056911468506}, {\"index\": 608, \"title\": \"[ARR][2205190040006377 ][Procter and Gamble]the spark driver has stopped unexpectedly and is restarting. your notebook will be automatically reattached.\", \"x\": 9.56155014038086, \"y\": 2.04677414894104}, {\"index\": 4402, \"title\": \"Subnet Issue\", \"x\": 7.81793212890625, \"y\": 3.046374797821045}, {\"index\": 2903, \"title\": \" Customer is having intermittent connectivity issue in Azure Databricks\", \"x\": 7.248920440673828, \"y\": 6.1774702072143555}, {\"index\": 2093, \"title\": \"2204260040004597\", \"x\": 9.794456481933594, \"y\": 4.005702972412109}, {\"index\": 4544, \"title\": \"All purpose cluster hangs when mutiple jobs are submitted against the cluster\", \"x\": 4.405090808868408, \"y\": 0.9187296628952026}, {\"index\": 4731, \"title\": \"not able to use databricks account from my microsoft email even after adding users to admin account in databricks cluster- Saved\", \"x\": 10.229347229003906, \"y\": 5.731827259063721}, {\"index\": 2360, \"title\": \"download all records hangs when executing command\", \"x\": 7.318474769592285, \"y\": 2.8089351654052734}, {\"index\": 1723, \"title\": \"2205020040005060\", \"x\": 7.601454734802246, \"y\": 4.461893558502197}, {\"index\": 1169, \"title\": \"Databricks Cluster reports ModuleNotFoundError: No module named 'pmdarima'\", \"x\": 4.255585193634033, \"y\": 0.8830000758171082}, {\"index\": 2740, \"title\": \"Can we set  \\\"spark.databricks.photon.parquetWriter.enabled false\\\" in cluster policy?\", \"x\": 8.787946701049805, \"y\": 4.705909252166748}, {\"index\": 3583, \"title\": \"Databricks Jobs are failing intermitantly \", \"x\": 9.826698303222656, \"y\": 6.823679447174072}, {\"index\": 2414, \"title\": \"Not able to access other region S3 bucket in US\", \"x\": 6.554471015930176, \"y\": 3.7655599117279053}, {\"index\": 3190, \"title\": \"Unable to Create Cluster with GPU EC2 Instances\", \"x\": 8.103433609008789, \"y\": 4.190456867218018}, {\"index\": 81, \"title\": \"2205240040004222 | ARR | Jobs are running slow than the previous runs\", \"x\": 7.9144062995910645, \"y\": 3.9410548210144043}, {\"index\": 2355, \"title\": \"Secrets not being redacted in driver log 2204140010001013 - 2112120010000141\", \"x\": 7.6747236251831055, \"y\": 5.943244934082031}, {\"index\": 4861, \"title\": \"Gennie\", \"x\": 4.248246669769287, \"y\": 0.9284744262695312}, {\"index\": 4339, \"title\": \"Databricks jobs hung condition for last 5 hours\", \"x\": 7.1267852783203125, \"y\": 1.026076316833496}, {\"index\": 1011, \"title\": \"Out of memory support \", \"x\": 9.807791709899902, \"y\": 5.483180522918701}, {\"index\": 3169, \"title\": \"Unity Catalog\", \"x\": 4.347143173217773, \"y\": 0.9472790956497192}, {\"index\": 871, \"title\": \"Jobs API giving error\", \"x\": 7.22322416305542, \"y\": 6.112848281860352}, {\"index\": 634, \"title\": \"Help me debug a job run that timed out\", \"x\": 5.704465866088867, \"y\": 4.723028182983398}, {\"index\": 3351, \"title\": \"[ARR] SR#2204050040010569-Please help me to get the Ganglia UI report images to dig into the detailed cluster report\", \"x\": 9.842449188232422, \"y\": 2.3625271320343018}, {\"index\": 5418, \"title\": \"Informatica Enterprise Data Catalog tool (version 10.5.1) connects to Databricks using JDBC driver and scans the delta tables for metadata. This scan is working for certain schemas in Databricks but errors out when scanning larger schema objects.\", \"x\": 5.540361404418945, \"y\": 2.0883378982543945}, {\"index\": 5402, \"title\": \"Node failures in databricks PROD\", \"x\": 10.21635913848877, \"y\": 5.3737359046936035}, {\"index\": 4110, \"title\": \"How to assign default cluster policy to a user or group\", \"x\": 9.783405303955078, \"y\": 6.822545528411865}, {\"index\": 4365, \"title\": \"[ARR] [Sev B] SR-2203210030001690 How_to_Mount_Azure_File_Share_to_Databricks\", \"x\": 6.92974328994751, \"y\": 4.936801433563232}, {\"index\": 1720, \"title\": \"SQL access\", \"x\": 4.034136772155762, \"y\": 1.0891568660736084}, {\"index\": 1230, \"title\": \"coalesce causing file already exists error when running a large dataset  \", \"x\": 4.263367176055908, \"y\": 0.8995407223701477}, {\"index\": 2282, \"title\": \"BAI Sales Random Data Anomaly\", \"x\": 10.073809623718262, \"y\": 2.9968786239624023}, {\"index\": 2343, \"title\": \"Can't access management console to upgrade account\", \"x\": 7.882631778717041, \"y\": 4.010155200958252}, {\"index\": 1172, \"title\": \" not abale to create cluster through azure databricks\", \"x\": 10.4109468460083, \"y\": 5.114193916320801}, {\"index\": 858, \"title\": \"Customer is unable to write streaming data to Event Hub\", \"x\": 5.246570110321045, \"y\": 1.4870392084121704}, {\"index\": 1488, \"title\": \"Issue with length of Job ID\", \"x\": 7.980426788330078, \"y\": 6.502154350280762}, {\"index\": 2697, \"title\": \"Job cluster is failing with Unexpected error\", \"x\": 7.752096652984619, \"y\": 2.551332473754883}, {\"index\": 5450, \"title\": \"2202280030001323\", \"x\": 8.652356147766113, \"y\": 3.4740281105041504}, {\"index\": 308, \"title\": \"SQL joins gives wrong result\", \"x\": 9.88022232055664, \"y\": 4.688626289367676}, {\"index\": 899, \"title\": \"[ARR] [Sev B] SR-2204200030000408 -Databricks cannot access key vault randomly\", \"x\": 8.162839889526367, \"y\": 3.8710672855377197}, {\"index\": 447, \"title\": \"Job marked finished but output not written.  Still \", \"x\": 8.304420471191406, \"y\": 1.0377671718597412}, {\"index\": 5332, \"title\": \"Anaconda terms of service change and impact on MLflow inference usage\", \"x\": 9.017695426940918, \"y\": 5.287027359008789}, {\"index\": 1159, \"title\": \"Cluster terminated.Reason:Azure Vm Extension Failure\", \"x\": 4.082873821258545, \"y\": 1.2326374053955078}, {\"index\": 1155, \"title\": \"The instance of the SQL Server Database Engine cannot obtain a LOCK resource at this time\", \"x\": 4.100720405578613, \"y\": 0.9720040559768677}, {\"index\": 3943, \"title\": \"Streams Taking a lot of time to resume\", \"x\": 4.1179118156433105, \"y\": 1.0367543697357178}, {\"index\": 3390, \"title\": \"Cluster Configuration changed with new Hardware Types\", \"x\": 9.159379959106445, \"y\": 2.450942277908325}, {\"index\": 3111, \"title\": \"Getting refuse to connect when launching tensorboard \", \"x\": 4.224714756011963, \"y\": 0.9248276352882385}, {\"index\": 3684, \"title\": \" maxFilesPerTrigger and maxBytesPerTrigger spark settings are not working as expected\", \"x\": 7.158065319061279, \"y\": 4.974153995513916}, {\"index\": 5600, \"title\": \"After purging cluster logs, there is no significant change in size. What are the other items that constitute the storage?\", \"x\": 4.048117160797119, \"y\": 1.0601288080215454}, {\"index\": 4487, \"title\": \"git version\", \"x\": 4.014827251434326, \"y\": 1.0378823280334473}, {\"index\": 623, \"title\": \"Databricks Cluster Slowed down \", \"x\": 7.863204479217529, \"y\": 4.127918720245361}, {\"index\": 250, \"title\": \"2205110040001203 \", \"x\": 5.954125881195068, \"y\": 2.8435845375061035}, {\"index\": 2893, \"title\": \"Job cluster did not shut down\", \"x\": 7.967172145843506, \"y\": 1.7993437051773071}, {\"index\": 1005, \"title\": \" Jobs in Production and QA Databrciks are failing due to timeout error\", \"x\": 7.880032062530518, \"y\": 2.092073678970337}, {\"index\": 3791, \"title\": \"ARR | Issue with steaming job | 2203300030001221 | TATA DIGITAL\", \"x\": 9.717996597290039, \"y\": 6.788023471832275}, {\"index\": 2204, \"title\": \"Databricks jobs are failing to trigger the automation account Runbooks\", \"x\": 10.564870834350586, \"y\": 2.5595295429229736}, {\"index\": 3672, \"title\": \"Looking for help in transforming dataframe with complex structure\", \"x\": 7.3536858558654785, \"y\": 3.8673181533813477}, {\"index\": 4236, \"title\": \"Command stuck on levi-dsp-preprod -  No alive workers\", \"x\": 10.04692268371582, \"y\": 2.5628302097320557}, {\"index\": 3472, \"title\": \"Databricks cluster not able to start\", \"x\": 6.261743545532227, \"y\": 2.945889472961426}, {\"index\": 4283, \"title\": \"Ganglia UI is somewhat broken\", \"x\": 9.956670761108398, \"y\": 3.086348056793213}, {\"index\": 2023, \"title\": \"[ARR][AIA][Storage Image Download Falure]\", \"x\": 6.910891532897949, \"y\": 5.245024681091309}, {\"index\": 2593, \"title\": \"Jobs are getting stuck\", \"x\": 7.956759452819824, \"y\": 3.183467149734497}, {\"index\": 4683, \"title\": \"ARR - Not able to read file\", \"x\": 4.193549156188965, \"y\": 0.955515444278717}, {\"index\": 1731, \"title\": \"Databricks clusters with DBR 7.3 are not connecting to the external metastore\", \"x\": 10.247239112854004, \"y\": 2.9735076427459717}, {\"index\": 5139, \"title\": \"Could not create a Table in the metastore specifying a location on ADLS  as the Databases was created specifying a location on a mount\", \"x\": 9.34037971496582, \"y\": 6.748015403747559}, {\"index\": 2036, \"title\": \"Failure to write to cosmos db\", \"x\": 7.321705341339111, \"y\": 4.518798828125}, {\"index\": 3251, \"title\": \"Databricks log capture user command behavior and table access control question\", \"x\": 4.432258605957031, \"y\": 0.9763217568397522}, {\"index\": 440, \"title\": \"ARR - ATT - Invalid fs.azure.account.key error - while trying to save data to ADLS container from Databricks- 2205180040008310 \", \"x\": 10.119482040405273, \"y\": 4.8029327392578125}, {\"index\": 1831, \"title\": \"By default stats are generated for the delta table while running optimize\", \"x\": 7.572904586791992, \"y\": 2.027474880218506}, {\"index\": 2879, \"title\": \"Follow up (00138540) Performance issues.\", \"x\": 8.942765235900879, \"y\": 3.91670560836792}, {\"index\": 4253, \"title\": \"Help to check the backend logs \", \"x\": 6.273488998413086, \"y\": 3.426039934158325}, {\"index\": 1410, \"title\": \"2204220010000657 | Cluster startup\", \"x\": 10.332663536071777, \"y\": 2.4751970767974854}, {\"index\": 984, \"title\": \"Advise on the steps to deploy a model stored in databricks registery in ACI\", \"x\": 5.132058620452881, \"y\": 1.8593467473983765}, {\"index\": 3901, \"title\": \"Job failing due to shuffle fetch failed exception | ARR SR# 2203280040004004\", \"x\": 9.377655029296875, \"y\": 6.107194423675537}, {\"index\": 3706, \"title\": \"CSS-ARR-SFMC-2203310030001247-Job are running for long \", \"x\": 5.7548136711120605, \"y\": 3.902371883392334}, {\"index\": 2708, \"title\": \"Private Databricks workspace\", \"x\": 10.434697151184082, \"y\": 3.5911505222320557}, {\"index\": 169, \"title\": \"ARR | 2205260040001988 | Workspace folders were deleted in production\", \"x\": 6.387706279754639, \"y\": 3.4049103260040283}, {\"index\": 4524, \"title\": \"[Microsoft][Hardy] (34) Error from server: SSL_connect: wrong version\", \"x\": 9.513915061950684, \"y\": 6.538462162017822}, {\"index\": 1718, \"title\": \"Databricks cluster startup - 2204250010002422\", \"x\": 4.3944854736328125, \"y\": 0.9377248287200928}, {\"index\": 4513, \"title\": \"2203160010001296 | ARR | T-Mobile USA | .table function stopped working \", \"x\": 7.758689880371094, \"y\": 5.655189514160156}, {\"index\": 5152, \"title\": \" Jobs taking longer than usual on USC- 2202080010002833 \", \"x\": 9.04927921295166, \"y\": 6.440532684326172}, {\"index\": 314, \"title\": \"Spark streaming job continues in background while status is failed\", \"x\": 9.274506568908691, \"y\": 5.039040565490723}, {\"index\": 574, \"title\": \"The library named of msodbcsql17 is installed however pyodbc failed with error '('01000', '[01000] [unixODBC] Can't open  'ODBC Driver''\", \"x\": 3.8858015537261963, \"y\": 1.1563409566879272}, {\"index\": 2946, \"title\": \"Prod databricks hard Down\", \"x\": 8.412240028381348, \"y\": 1.7230312824249268}, {\"index\": 4576, \"title\": \"Data drop on the xpopd data service\", \"x\": 8.481738090515137, \"y\": 4.337446212768555}, {\"index\": 1272, \"title\": \"Multiple Production job failure due to RuntimeException\", \"x\": 7.07241153717041, \"y\": 5.380856990814209}, {\"index\": 1687, \"title\": \"Issue with databricks training set\", \"x\": 10.330249786376953, \"y\": 4.660820484161377}, {\"index\": 386, \"title\": \"ARR - EY - 2205200040005272 - Databricks jobs failed\", \"x\": 8.267516136169434, \"y\": 2.891225814819336}, {\"index\": 4300, \"title\": \"take too much time to complete notebook\", \"x\": 9.154557228088379, \"y\": 4.7250847816467285}, {\"index\": 4911, \"title\": \"Issue connecting to ADO repo\", \"x\": 7.757061958312988, \"y\": 3.8378851413726807}, {\"index\": 4360, \"title\": \"2203180040005130  Autoscaling removed the node leads to execution failure\", \"x\": 9.303239822387695, \"y\": 4.567174911499023}, {\"index\": 5104, \"title\": \"Unable to connect via azure vnet. ||S500 ACE cx.\", \"x\": 9.06861400604248, \"y\": 6.476308822631836}, {\"index\": 428, \"title\": \"Behaviour Difference in Photon\", \"x\": 4.093189716339111, \"y\": 0.9839762449264526}, {\"index\": 1758, \"title\": \"Please help to get the VM service log for a succssful run.\", \"x\": 7.853810787200928, \"y\": 2.130567789077759}, {\"index\": 82, \"title\": \"\\u76e3\\u67fb\\u30ed\\u30b0\\u306e\\u69cb\\u6210\\u306b\\u5229\\u7528\\u3059\\u308bS3\\u30d0\\u30b1\\u30c3\\u30c8\\u306e\\u8a2d\\u5b9a\\u306b\\u3064\\u3044\\u3066\", \"x\": 4.107601165771484, \"y\": 1.0239347219467163}, {\"index\": 5449, \"title\": \"Issues connecting to Spark Datasource: HTTP Connection error.\", \"x\": 6.553306579589844, \"y\": 4.733703136444092}, {\"index\": 1624, \"title\": \"User is not able to query RedShift data\", \"x\": 8.103924751281738, \"y\": 2.1707279682159424}, {\"index\": 4046, \"title\": \"Spark Optimization\", \"x\": 9.708789825439453, \"y\": 4.226444244384766}, {\"index\": 79, \"title\": \"Recover the deleted notebooks that are not committed from repos\", \"x\": 8.505334854125977, \"y\": 4.444826126098633}, {\"index\": 222, \"title\": \"2205200030001371\", \"x\": 5.8199944496154785, \"y\": 4.76633882522583}, {\"index\": 285, \"title\": \"Invalid Bucket Error\", \"x\": 7.031755447387695, \"y\": 2.5487160682678223}, {\"index\": 72, \"title\": \"Follow up of (00146728)Databricks - Cluster Upgrade with gdal\", \"x\": 4.3770222663879395, \"y\": 1.0102187395095825}, {\"index\": 4706, \"title\": \"Databricks SQL - caching queries with LIMIT applied\", \"x\": 6.348845958709717, \"y\": 4.410926342010498}, {\"index\": 592, \"title\": \"Terraform pipeline fails while creating cluster\", \"x\": 5.6888909339904785, \"y\": 4.169474124908447}, {\"index\": 1344, \"title\": \"Data plane traffic from databricks compute machine to s3 is going via NAT Gateway which is causing huge bill on Nat Gateway of AWS\", \"x\": 9.156765937805176, \"y\": 6.128313064575195}, {\"index\": 4448, \"title\": \"[ARR] SR-2203160030001662 Job Failed\", \"x\": 8.24646282196045, \"y\": 4.506685256958008}, {\"index\": 3568, \"title\": \" Dataframe become empty when try to read a json\", \"x\": 9.120741844177246, \"y\": 3.9583921432495117}, {\"index\": 2475, \"title\": \"cluster terminated in databricks, however ec2 instance in aws is still running\", \"x\": 7.667079448699951, \"y\": 3.817805767059326}, {\"index\": 4726, \"title\": \"Job fails without error message in logs\", \"x\": 5.637322902679443, \"y\": 1.7415493726730347}, {\"index\": 1122, \"title\": \"I got stuck to an error in data bricks , the error is ' Command result size exceeds limit: Exceeded 20971520 bytes (current = 20972463)'\", \"x\": 9.01391887664795, \"y\": 6.66256856918335}, {\"index\": 4224, \"title\": \"Jobs failing with library not being installed on dataclusters\", \"x\": 8.388076782226562, \"y\": 6.051464557647705}, {\"index\": 5404, \"title\": \"Databricks-Dash Library\", \"x\": 8.067296028137207, \"y\": 4.123533725738525}, {\"index\": 2940, \"title\": \"SSO between applications\", \"x\": 9.008415222167969, \"y\": 5.743414878845215}, {\"index\": 4370, \"title\": \"ARR | 2203220040000035 | java.lang.OutOfMemoryError even though increasing the cluster size\", \"x\": 8.941882133483887, \"y\": 4.184484481811523}, {\"index\": 473, \"title\": \"[ARR] [Sev B] SR-2205130030001214 Getting the following error while reading file Unable to infer schema for Parquet. It must be specified manually\", \"x\": 6.81282901763916, \"y\": 4.233603477478027}, {\"index\": 545, \"title\": \" Delta Lake Change Feed Data error on job restart\", \"x\": 9.040436744689941, \"y\": 2.574967384338379}, {\"index\": 1759, \"title\": \" Unable to create empty delta tables in the ADLS stoarage\", \"x\": 9.46562671661377, \"y\": 5.611476421356201}, {\"index\": 119, \"title\": \"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2205270040006933\", \"x\": 4.405786037445068, \"y\": 0.9305446147918701}, {\"index\": 125, \"title\": \"cannot get debug log level \", \"x\": 6.118368148803711, \"y\": 4.853632926940918}, {\"index\": 554, \"title\": \"2205130040002413\", \"x\": 6.742743015289307, \"y\": 4.3048553466796875}, {\"index\": 808, \"title\": \"Job should be failed but still running for more than 1 hour\", \"x\": 6.5760416984558105, \"y\": 3.8622517585754395}, {\"index\": 1256, \"title\": \"testing case for email thread merg\", \"x\": 7.073203086853027, \"y\": 4.6956353187561035}, {\"index\": 3274, \"title\": \"[ARR] SR#2204060030003103 The Azure Databricks \\u2013 Connect to Azure Synapse - Dedicated SQL Pool issue\", \"x\": 6.9449687004089355, \"y\": 0.38650408387184143}, {\"index\": 644, \"title\": \"ARR | 2205120030000229 | Was there any change recently in Databricks in regard to installing libraries when running a job from ADF\", \"x\": 10.113934516906738, \"y\": 4.814874172210693}, {\"index\": 2820, \"title\": \"[ARR] [Sev A] SR-2204130040012512  Scheduled jobs are taking more than usual times. Jobs which used to take minutes took hours or running for forever.\", \"x\": 8.039363861083984, \"y\": 3.7555065155029297}, {\"index\": 1060, \"title\": \"Integration with AWS Glue and Redshft\", \"x\": 8.24648380279541, \"y\": 3.8560619354248047}, {\"index\": 4130, \"title\": \"[ARR][Spark Streaming For each batch join]\", \"x\": 4.0446624755859375, \"y\": 1.3328619003295898}, {\"index\": 3655, \"title\": \"Data is not up-to-date in DBSQL\", \"x\": 9.73370361328125, \"y\": 6.71345853805542}, {\"index\": 580, \"title\": \"Libraries error after restart cluter\", \"x\": 4.169863700866699, \"y\": 0.9579959511756897}, {\"index\": 259, \"title\": \" Kafka streaming tasks are failing continuously without any data transmission\", \"x\": 9.779159545898438, \"y\": 6.888585567474365}, {\"index\": 4586, \"title\": \"CSS-ARR-SR#2203160030000641 Not able to use scheme registry\", \"x\": 8.485416412353516, \"y\": 5.192184925079346}, {\"index\": 2128, \"title\": \"2204260060000060 \", \"x\": 8.041299819946289, \"y\": 3.6326725482940674}, {\"index\": 3194, \"title\": \"Merge Legacy E2 Account  into Current E2 Account\", \"x\": 4.211580753326416, \"y\": 0.9505841732025146}, {\"index\": 196, \"title\": \"Exception equests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://centralus.azuredatabricks.net/api/2.0/jobs/list\", \"x\": 4.725186347961426, \"y\": 0.5276463031768799}, {\"index\": 3694, \"title\": \"[ARR] Read and write into ADLS gen1  from spark failures\", \"x\": 7.963406562805176, \"y\": 6.5628275871276855}, {\"index\": 4963, \"title\": \"All jobs jobs stuck in pending state for more than 48 minutes \", \"x\": 6.597141265869141, \"y\": 5.202764987945557}, {\"index\": 1586, \"title\": \"2205040030001055 |  ARR | Adobe \", \"x\": 7.21893310546875, \"y\": 1.4525744915008545}, {\"index\": 5528, \"title\": \"connectivity issue to Azure SQL Server instance from DBR 9.1 LTS by a JDBC connection | ARR SR #2202110030000677001\", \"x\": 6.877912521362305, \"y\": 2.7044296264648438}, {\"index\": 2723, \"title\": \"Glue catalog access from passthrough cluster\", \"x\": 4.716094017028809, \"y\": 0.5097869038581848}, {\"index\": 5220, \"title\": \"Vacuum operation seems to have potentially produced a data loss\", \"x\": 9.672065734863281, \"y\": 3.148862361907959}, {\"index\": 5167, \"title\": \"Run Databricks Cluster Through Docker Container\", \"x\": 7.953667163848877, \"y\": 2.918177604675293}, {\"index\": 2076, \"title\": \"Subprocessors\", \"x\": 9.783031463623047, \"y\": 3.701765298843384}, {\"index\": 5554, \"title\": \"sql data tab does not show databases\", \"x\": 4.706050395965576, \"y\": 0.5173789858818054}, {\"index\": 929, \"title\": \"Adding service principals to group\", \"x\": 7.052933216094971, \"y\": 0.6938155889511108}, {\"index\": 2164, \"title\": \"ARR Customer: Unilever - Unable to Access ADB Environment\", \"x\": 8.118814468383789, \"y\": 4.925700664520264}, {\"index\": 503, \"title\": \"Error with Model Deployment which used to be able to be deployed\", \"x\": 7.535621166229248, \"y\": 3.3511807918548584}, {\"index\": 179, \"title\": \"Azure databricks notebook execution duration is extremely long (40+ minutes), and it exceeds the 10 minute trigger window\", \"x\": 6.5260491371154785, \"y\": 2.405425548553467}, {\"index\": 5301, \"title\": \"Job run history does not show who executed the job\", \"x\": 10.030842781066895, \"y\": 3.1918082237243652}, {\"index\": 1603, \"title\": \"2205040030000641 | Production jobs are aborted | Air Canada\", \"x\": 5.455660343170166, \"y\": 2.0563852787017822}, {\"index\": 1552, \"title\": \"Multiple Library installation failure on prod cluster\", \"x\": 7.7848896980285645, \"y\": 2.089463472366333}, {\"index\": 3205, \"title\": \"\\\"Couldn't find all part files of the checkpoint version\\\" errors when querying the table\", \"x\": 9.308972358703613, \"y\": 1.925840973854065}, {\"index\": 5392, \"title\": \"Unexpected infra fault while creating the cluster for the job\", \"x\": 8.099244117736816, \"y\": 2.223752975463867}, {\"index\": 847, \"title\": \"Cluster is not running\", \"x\": 8.931768417358398, \"y\": 6.1101789474487305}, {\"index\": 2997, \"title\": \"debugfs on worker nodes not owned by root\", \"x\": 8.322386741638184, \"y\": 5.852742671966553}, {\"index\": 4076, \"title\": \"Cannot drop delta table by location specifier\", \"x\": 8.119452476501465, \"y\": 5.370786190032959}, {\"index\": 3602, \"title\": \"Databricks & Airflow connection timeout issue\", \"x\": 7.950347423553467, \"y\": 6.650097846984863}, {\"index\": 3286, \"title\": \"Job failure without underlying cause\", \"x\": 4.3871636390686035, \"y\": 0.9284265041351318}, {\"index\": 3690, \"title\": \"The spark context has stopped and the driver is restarting\", \"x\": 9.329349517822266, \"y\": 3.950197458267212}, {\"index\": 2438, \"title\": \"Follow-up on 00135798: Job fails when worker nodes on specific SKU\", \"x\": 3.9773547649383545, \"y\": 0.9977035522460938}, {\"index\": 325, \"title\": \"Performance issue on the notebooks execution. \", \"x\": 7.709405899047852, \"y\": 3.0740556716918945}, {\"index\": 1573, \"title\": \"SQL Endpoints not Starting\", \"x\": 9.44025707244873, \"y\": 2.891695022583008}, {\"index\": 694, \"title\": \"mlflow run on databricks-connect\", \"x\": 7.98328971862793, \"y\": 6.580522537231445}, {\"index\": 4277, \"title\": \"Not receiving email notifications on job runs\", \"x\": 9.98762321472168, \"y\": 3.2815744876861572}, {\"index\": 636, \"title\": \"The error 'ModuleNotFoundError' occurred, but the libs were installed on databricks\", \"x\": 8.796855926513672, \"y\": 2.0149590969085693}, {\"index\": 870, \"title\": \"2205140050000228 \", \"x\": 4.096121311187744, \"y\": 1.018220067024231}, {\"index\": 2887, \"title\": \"ConnectException: Connection refused (Connection refused)\", \"x\": 7.157242774963379, \"y\": 3.7044200897216797}, {\"index\": 5594, \"title\": \"Error in data bricks - ConnectException: Connection refused (Connection refused)  Error while obtaining a new communication channel\", \"x\": 7.3007588386535645, \"y\": 2.545217990875244}, {\"index\": 4599, \"title\": \"encoding parsing issue\", \"x\": 4.013635158538818, \"y\": 1.1519500017166138}, {\"index\": 1264, \"title\": \"CSS-ARR-2205100030000546-Job Aborted/Job Delay during scheduled run\", \"x\": 8.230128288269043, \"y\": 1.2746107578277588}, {\"index\": 687, \"title\": \"follow up case CASE NUMBER # 00140608  - job failing with the are due to No Space left on the device.\", \"x\": 7.651501178741455, \"y\": 3.793199062347412}, {\"index\": 2927, \"title\": \"Notebooks getting cancelled and Intermittent blob operation not supported when executing deltaTable.delete\", \"x\": 6.855557441711426, \"y\": 4.5760087966918945}, {\"index\": 3819, \"title\": \"S3 Databricks Root Bucket Access Issues\", \"x\": 9.79495620727539, \"y\": 2.6268723011016846}, {\"index\": 642, \"title\": \"Errors on Azure Databricks Feature store\", \"x\": 8.104735374450684, \"y\": 5.864748477935791}, {\"index\": 1297, \"title\": \"Link data content to access history on Databricks SQL\", \"x\": 7.357893943786621, \"y\": 3.4960415363311768}, {\"index\": 1073, \"title\": \"Jar file init script installation\", \"x\": 8.985416412353516, \"y\": 1.8901288509368896}, {\"index\": 792, \"title\": \"Jobs are failing around the same time\", \"x\": 4.21022891998291, \"y\": 0.951471209526062}, {\"index\": 1892, \"title\": \"ARR 2203030050000273 -  Customer wants to understand failures and retries on Spark tasks\", \"x\": 8.7694730758667, \"y\": 4.3700385093688965}, {\"index\": 4736, \"title\": \"Inconsistencies with Spark jobs run on similar clusters\", \"x\": 9.620894432067871, \"y\": 3.6322548389434814}, {\"index\": 3768, \"title\": \" maxFilesPerTrigger and maxBytesPerTrigger spark settings are not working as expected\", \"x\": 7.181341648101807, \"y\": 0.8497862219810486}, {\"index\": 3037, \"title\": \"Query on cluster access\", \"x\": 8.263518333435059, \"y\": 5.142244338989258}, {\"index\": 1991, \"title\": \"SQL constantly fails after previously running successfully. \", \"x\": 8.344474792480469, \"y\": 3.215975284576416}, {\"index\": 4508, \"title\": \"Clarification on Databricks Managed Service Encryption & Access\", \"x\": 6.8649702072143555, \"y\": 3.2146530151367188}, {\"index\": 2863, \"title\": \"Whitelist Verisk IP ranges to access Databricks Endpoint\", \"x\": 8.797648429870605, \"y\": 2.1413238048553467}, {\"index\": 1936, \"title\": \"[ARR] SR-2204270050000908 DataBricks cluster performance issue\", \"x\": 5.1676554679870605, \"y\": 1.6254581212997437}, {\"index\": 2390, \"title\": \"Audit Logging\", \"x\": 8.859174728393555, \"y\": 6.369947910308838}, {\"index\": 2847, \"title\": \"ipywidgets is broken! not rendering. \", \"x\": 8.99100112915039, \"y\": 6.749122142791748}, {\"index\": 3068, \"title\": \"ARR | 2204080030001938 | Workspace creation is failing | Standard Chartered Bank\", \"x\": 7.758835315704346, \"y\": 2.615074634552002}, {\"index\": 2139, \"title\": \"Executor nodes not restarting\", \"x\": 4.729903221130371, \"y\": 0.5380670428276062}, {\"index\": 976, \"title\": \"Databricks is failing when we are writing more csv files.\", \"x\": 8.032071113586426, \"y\": 3.7737326622009277}, {\"index\": 3992, \"title\": \"Databrick Notebooks Haging for Long Periods of Time- Saved\", \"x\": 7.57660436630249, \"y\": 6.146408557891846}, {\"index\": 4973, \"title\": \" Job aborted due to stage failure\", \"x\": 8.798489570617676, \"y\": 3.960264205932617}, {\"index\": 4152, \"title\": \"Saprk Streaming Drive getting the OOM Error\", \"x\": 6.599273204803467, \"y\": 2.848545551300049}, {\"index\": 4968, \"title\": \"what is the current status of Databricks sql serverless compute for Azure\", \"x\": 5.730875015258789, \"y\": 1.8401119709014893}, {\"index\": 1681, \"title\": \"changing default timestamp for hive tables and databricks workspace to CET\", \"x\": 6.9525299072265625, \"y\": 4.594222068786621}, {\"index\": 3810, \"title\": \"Not able to connect to s3 bucket\", \"x\": 9.432059288024902, \"y\": 4.7088751792907715}, {\"index\": 4631, \"title\": \"HUDI missing libraries when using Spark SQL from Superset\", \"x\": 8.88938045501709, \"y\": 5.602972984313965}, {\"index\": 2471, \"title\": \"Describe table differences in DBR 10.4\", \"x\": 8.405893325805664, \"y\": 2.8985695838928223}, {\"index\": 1145, \"title\": \"CREATE WIDGET fails with block comments in the same cell\", \"x\": 6.717441558837891, \"y\": 3.468456268310547}, {\"index\": 4257, \"title\": \"intermittently gets 'The INSERT statement conflicted with the FOREIGN KEY constraint' error\", \"x\": 10.424301147460938, \"y\": 3.7305192947387695}, {\"index\": 4089, \"title\": \"[spark support]Databricks SQL endpoint\", \"x\": 7.353546619415283, \"y\": 3.979294776916504}, {\"index\": 3620, \"title\": \"When I run a Databricks job from Airflow, I get an error\", \"x\": 7.173326015472412, \"y\": 2.597118377685547}, {\"index\": 5505, \"title\": \"[ACTION REQUIRED] JobId non-sequential identifier\", \"x\": 8.101442337036133, \"y\": 2.218705415725708}, {\"index\": 3527, \"title\": \"ARR - Cummins - 2203310030003446 - Job hung from 5 months\", \"x\": 7.379413604736328, \"y\": 3.43050479888916}, {\"index\": 1519, \"title\": \"Not able to export users' notebooks in this workspace\", \"x\": 4.074525833129883, \"y\": 1.3094662427902222}, {\"index\": 3798, \"title\": \"SELF_BOOTSTRAP_FAILURE(SUCCESS)\", \"x\": 4.7700276374816895, \"y\": 0.5524615049362183}, {\"index\": 2179, \"title\": \"DLT feature\", \"x\": 4.247834205627441, \"y\": 0.9144350290298462}, {\"index\": 4232, \"title\": \"Fixing Databricks Job (Cross-Visitation)\", \"x\": 6.43524169921875, \"y\": 3.799340009689331}, {\"index\": 684, \"title\": \"2204280030000904 Power BI  databricks connector new release issue\", \"x\": 6.3973493576049805, \"y\": 2.499150276184082}, {\"index\": 881, \"title\": \"2205120030001564 \", \"x\": 4.4349236488342285, \"y\": 0.9316139221191406}, {\"index\": 1375, \"title\": \"Unable to connect from Databricks (dbw-smartfleet) to mongo instance running in int-hal-pe-prd subscription.\", \"x\": 8.909942626953125, \"y\": 5.188836574554443}, {\"index\": 796, \"title\": \"CSS-ARR-S500-SR#2205170050000139-Databricks job getting failed.\", \"x\": 9.50668716430664, \"y\": 4.271750450134277}, {\"index\": 3344, \"title\": \"[ARR] [Sev C] SR-2204060030000215 the reason why Job Alert mail did not work\", \"x\": 8.696159362792969, \"y\": 2.4309659004211426}, {\"index\": 5348, \"title\": \"Databricks job fails after running 150-180 hours\", \"x\": 7.9044294357299805, \"y\": 3.9154250621795654}, {\"index\": 126, \"title\": \"ARR|AT&T| Running jobs using databricks sql connector-\", \"x\": 4.416705131530762, \"y\": 0.9794460535049438}, {\"index\": 4603, \"title\": \"Issue while parsing csv in databricks\", \"x\": 4.172621250152588, \"y\": 1.107168197631836}, {\"index\": 4828, \"title\": \"Dashboard schedules break after time change\", \"x\": 7.703923225402832, \"y\": 1.8921951055526733}, {\"index\": 1379, \"title\": \"Unable to run Synapse connector code, Job fails\", \"x\": 9.141051292419434, \"y\": 6.719830513000488}, {\"index\": 2661, \"title\": \"Jobs taking too long - performance issue\", \"x\": 10.646432876586914, \"y\": 2.9496095180511475}, {\"index\": 4387, \"title\": \"job permissions\", \"x\": 5.955122470855713, \"y\": 2.2208335399627686}, {\"index\": 1662, \"title\": \"Use YOLOV5 on Databricks\", \"x\": 9.912957191467285, \"y\": 5.353833198547363}, {\"index\": 2785, \"title\": \"Accounts Console SSO Not Working\", \"x\": 9.829821586608887, \"y\": 4.297887325286865}, {\"index\": 3301, \"title\": \"Error running queries on Coreviz Cluster in Live(450)\", \"x\": 6.434910297393799, \"y\": 3.0549395084381104}, {\"index\": 4900, \"title\": \"Databricks write optimization- Saved\", \"x\": 7.1263885498046875, \"y\": 6.023648262023926}, {\"index\": 799, \"title\": \"HElp us to install Library in 10LTS ML\", \"x\": 9.602044105529785, \"y\": 2.4154152870178223}, {\"index\": 486, \"title\": \"2205190030002256 \", \"x\": 4.137916088104248, \"y\": 1.019213318824768}, {\"index\": 206, \"title\": \"2205260040004092 | Repo\", \"x\": 7.50468111038208, \"y\": 3.4571874141693115}, {\"index\": 3376, \"title\": \"need new firewall rule to public S3 bucket with databricks-datasetes\", \"x\": 4.743219375610352, \"y\": 0.5106276869773865}, {\"index\": 4252, \"title\": \"cluster is in pending\", \"x\": 5.639374732971191, \"y\": 2.0791969299316406}, {\"index\": 2420, \"title\": \"job keeps failing with FetchFailedException  \", \"x\": 8.215254783630371, \"y\": 1.9091429710388184}, {\"index\": 2369, \"title\": \"Databricks Job Clusters not starting\", \"x\": 4.157096862792969, \"y\": 1.0675114393234253}, {\"index\": 783, \"title\": \"Job is Stuck \", \"x\": 8.253589630126953, \"y\": 6.101567268371582}, {\"index\": 2821, \"title\": \"AWS Cross Account S3 access issue\", \"x\": 7.879553318023682, \"y\": 3.014497756958008}, {\"index\": 4619, \"title\": \"0378 - g - ARR - CrediCorp - Job failing\", \"x\": 7.859414577484131, \"y\": 1.5679055452346802}, {\"index\": 3804, \"title\": \"Data tab slowing table preview\", \"x\": 9.061877250671387, \"y\": 6.707021713256836}, {\"index\": 4954, \"title\": \"2203100030001867\", \"x\": 10.006280899047852, \"y\": 3.159374952316284}, {\"index\": 1412, \"title\": \"2205050030001596_Need assistance to recover MICROSOFT.DATABRICKS/WORKSPACES/DBWORKSPACES deleted at 4/29\", \"x\": 6.399322032928467, \"y\": 3.6769182682037354}, {\"index\": 2332, \"title\": \"Permissions issues with accessing tables in workspace\", \"x\": 6.118753433227539, \"y\": 3.413038969039917}, {\"index\": 3387, \"title\": \"Create New Admin\", \"x\": 7.6957316398620605, \"y\": 1.2594809532165527}, {\"index\": 2784, \"title\": \"Databricks job failed with error message\", \"x\": 10.316933631896973, \"y\": 5.56561803817749}, {\"index\": 2856, \"title\": \"Explore paramater for json sizing within Databricks\", \"x\": 8.096748352050781, \"y\": 2.9516761302948}, {\"index\": 758, \"title\": \"Databricks cluster is in pending state from long time\", \"x\": 9.68444538116455, \"y\": 3.5842766761779785}, {\"index\": 1995, \"title\": \"ARR | 2204110030000436 - cluster starting time is took longer time than usual\", \"x\": 7.692942142486572, \"y\": 3.9388279914855957}, {\"index\": 4915, \"title\": \"ARR:Unable to use defaultvalue in the cluster policy definition as per documentation:SR 2203100040006979 \", \"x\": 6.349310874938965, \"y\": 3.4269728660583496}, {\"index\": 1584, \"title\": \"broadcast the table that is larger than 8GB: 25 GB\", \"x\": 8.411959648132324, \"y\": 3.007739543914795}, {\"index\": 4100, \"title\": \"Error on cluster\", \"x\": 10.488189697265625, \"y\": 3.9683687686920166}, {\"index\": 3039, \"title\": \"Job fails\", \"x\": 7.170141220092773, \"y\": 6.08671236038208}, {\"index\": 313, \"title\": \"Error when trying to read from s3\", \"x\": 8.248823165893555, \"y\": 3.9486424922943115}, {\"index\": 1422, \"title\": \"Deprecating support for updating regions on Workspaces\", \"x\": 8.399151802062988, \"y\": 2.1016926765441895}, {\"index\": 2912, \"title\": \"We cannot connect to Databricks workspaces!\", \"x\": 9.824604988098145, \"y\": 3.6465587615966797}, {\"index\": 4597, \"title\": \"Questions on Credential Passthrough clusters\", \"x\": 9.272602081298828, \"y\": 3.889279365539551}, {\"index\": 2684, \"title\": \"databricks scala scripts cannot access object store using java module\", \"x\": 7.137147426605225, \"y\": 5.736241817474365}, {\"index\": 5341, \"title\": \"Withstand Databricks Job failures due to intermittent network packet drops.\", \"x\": 4.259408950805664, \"y\": 0.9655632376670837}, {\"index\": 5572, \"title\": \"2202280040003799 | Job Error\", \"x\": 10.257802963256836, \"y\": 4.493779182434082}, {\"index\": 4557, \"title\": \"connection between RDS and Databricks is not working\", \"x\": 7.686188220977783, \"y\": 1.1614950895309448}, {\"index\": 4004, \"title\": \"The Cluster terminates every time I try to start it. I've tried different configurations and they all fail.- Saved\", \"x\": 8.692727088928223, \"y\": 5.030463218688965}, {\"index\": 3492, \"title\": \"Databricks notebook failing to create hive table in dbfs.\", \"x\": 8.85679817199707, \"y\": 3.5319392681121826}, {\"index\": 1615, \"title\": \"User-specified schema incompatible with the schema  inferred from its query\", \"x\": 4.679379463195801, \"y\": 0.4589296281337738}, {\"index\": 951, \"title\": \"Unable to Spin a new workspace using Terraform\", \"x\": 6.52931547164917, \"y\": 0.7310441732406616}, {\"index\": 3372, \"title\": \"2203090040006252 | Job Failure\", \"x\": 7.65169095993042, \"y\": 3.5800955295562744}, {\"index\": 3465, \"title\": \" Init script executed twice when starting the cluster\", \"x\": 9.122036933898926, \"y\": 4.950291156768799}, {\"index\": 943, \"title\": \"UnCaught Syntax error\", \"x\": 6.868680000305176, \"y\": 0.7369587421417236}, {\"index\": 3518, \"title\": \"Private libraries not installing on databricks clusters\", \"x\": 7.740015029907227, \"y\": 4.230358600616455}, {\"index\": 3491, \"title\": \"unable to locate credentials (temp cred's for instane profile)\", \"x\": 7.582223415374756, \"y\": 5.993035316467285}, {\"index\": 4146, \"title\": \"InvalidPrivateWorkspaceForSubscription\", \"x\": 8.74245834350586, \"y\": 6.286151885986328}, {\"index\": 3784, \"title\": \"Reg the criedential passthrough limitation\", \"x\": 4.738655090332031, \"y\": 0.5444751977920532}, {\"index\": 5375, \"title\": \"jobs are failing on https://tpx-sse-ds-comcast.cloud.databricks.com workspace\", \"x\": 7.988001823425293, \"y\": 3.941211462020874}, {\"index\": 1217, \"title\": \"URGENT - Cluster Failing To Launch\", \"x\": 8.052526473999023, \"y\": 4.740050315856934}, {\"index\": 850, \"title\": \"Workspace limit exceeded, only 20 workspaces are allowed for your account\", \"x\": 7.38771915435791, \"y\": 1.26668119430542}, {\"index\": 1961, \"title\": \"Follow-up ticket on previous 00140987\", \"x\": 8.005585670471191, \"y\": 1.1312026977539062}, {\"index\": 1070, \"title\": \"Could not find ADLS Gen2 Token\", \"x\": 6.366970539093018, \"y\": 3.521911859512329}, {\"index\": 4966, \"title\": \"Delta tables not accesible\", \"x\": 8.727625846862793, \"y\": 4.068243980407715}, {\"index\": 1877, \"title\": \"Accessing AWS Location Services from Databricks using boto3\", \"x\": 8.077469825744629, \"y\": 2.064763307571411}, {\"index\": 3005, \"title\": \"Okta Databricks Configuration\", \"x\": 4.441687107086182, \"y\": 0.9673575162887573}, {\"index\": 2488, \"title\": \"unable to start jobs clusters\", \"x\": 3.994415760040283, \"y\": 1.2253693342208862}, {\"index\": 1456, \"title\": \"Databricks SQL Error\", \"x\": 7.8141889572143555, \"y\": 2.5658137798309326}, {\"index\": 2327, \"title\": \"Java vulnerabilities in DBR\", \"x\": 4.03373384475708, \"y\": 1.048567295074463}, {\"index\": 1232, \"title\": \"Some users with \\\"Can Manage\\\" access have lost the ability to clone folders/notebooks\", \"x\": 6.551960468292236, \"y\": 0.7608000040054321}, {\"index\": 5183, \"title\": \"Picking Up New Runtime version on Job Cluster\", \"x\": 8.708906173706055, \"y\": 3.132422685623169}, {\"index\": 2637, \"title\": \"spark exception\", \"x\": 8.514613151550293, \"y\": 2.7590861320495605}, {\"index\": 5255, \"title\": \"issue connecting to neo4j from HCC cluster\", \"x\": 5.685356140136719, \"y\": 3.6918723583221436}, {\"index\": 67, \"title\": \"VNET injected workspace is slow\", \"x\": 8.460176467895508, \"y\": 5.9596734046936035}, {\"index\": 3590, \"title\": \"Failed to find data source\", \"x\": 10.458931922912598, \"y\": 3.056446075439453}, {\"index\": 1564, \"title\": \"User is getting failure message in a legacy shard when connecting to the metastore.\", \"x\": 4.017154693603516, \"y\": 1.2739300727844238}, {\"index\": 1238, \"title\": \"ARR - follow-up to 00142942\", \"x\": 9.84830093383789, \"y\": 3.162977457046509}, {\"index\": 192, \"title\": \"Access denied while creating a global init script\", \"x\": 8.583072662353516, \"y\": 3.9525771141052246}, {\"index\": 4777, \"title\": \"Azure databrikcs isn't able to fetch the Hive metastore 3.1.0 JARs from Maven\", \"x\": 7.079994201660156, \"y\": 3.6331515312194824}, {\"index\": 1528, \"title\": \"char/varchar type length limitation after upgrading to 10.5 runtime\", \"x\": 7.729418754577637, \"y\": 3.1764841079711914}, {\"index\": 1833, \"title\": \"Databricks cluster creation failure on all subscription\", \"x\": 9.773785591125488, \"y\": 6.786327362060547}, {\"index\": 276, \"title\": \"MLFlow Infinite Recursion Error when unselecting metrics\", \"x\": 4.033595561981201, \"y\": 1.0577387809753418}, {\"index\": 2362, \"title\": \"Projects files as dependency for DBX jobs\", \"x\": 7.448919773101807, \"y\": 1.7768336534500122}, {\"index\": 4609, \"title\": \"Databricks Job - Structured Streaming\", \"x\": 8.165055274963379, \"y\": 3.631047248840332}, {\"index\": 729, \"title\": \"Optimize Parallelization on Snowflake I/O and HuggingFace BERT-based model\", \"x\": 7.241691589355469, \"y\": 5.890981674194336}, {\"index\": 5011, \"title\": \"We would need to know exactly what kind of data are transferred to the U.S.\", \"x\": 10.348589897155762, \"y\": 5.576916217803955}, {\"index\": 3438, \"title\": \"2203300040006487SF || Refrencing Databricks secrets within global init scripts\", \"x\": 6.76814603805542, \"y\": 4.063268661499023}, {\"index\": 1356, \"title\": \"Display visualizations not working with python notebooks\", \"x\": 10.151981353759766, \"y\": 5.406894207000732}, {\"index\": 1038, \"title\": \"Cannot start a cluster on acxiom-di-datahub-appdev-admin.cloud.databricks.com\", \"x\": 6.5962677001953125, \"y\": 2.0910964012145996}, {\"index\": 1727, \"title\": \"Delta Live Tables with Autoloader\", \"x\": 10.097890853881836, \"y\": 2.7089219093322754}, {\"index\": 3607, \"title\": \"Customer got high bill in the month of november 2021\", \"x\": 4.021023750305176, \"y\": 1.3535772562026978}, {\"index\": 4516, \"title\": \"Can't connect to AWS MSK through Spark\", \"x\": 8.027338027954102, \"y\": 6.080865859985352}, {\"index\": 3392, \"title\": \"what this message mean  in stdout: GC (Allocation Failure)\", \"x\": 9.769205093383789, \"y\": 6.804094314575195}, {\"index\": 301, \"title\": \"Connection between databricks workspace and datalake\", \"x\": 4.047239780426025, \"y\": 1.2386080026626587}, {\"index\": 3696, \"title\": \"Jobs failed in production without  errors in code\", \"x\": 7.2742109298706055, \"y\": 1.1688740253448486}, {\"index\": 4178, \"title\": \"Databricks can't find getArgument() function in Spark SQL when executed as an ADF Notebook Activity\", \"x\": 9.664008140563965, \"y\": 3.0051681995391846}, {\"index\": 2762, \"title\": \"DatabricksServiceException: INTERNAL_ERROR: NullPointerException \", \"x\": 4.752728462219238, \"y\": 0.5481939315795898}, {\"index\": 58, \"title\": \"Two runs and one one just took 2 mins and others took 4h\", \"x\": 8.279332160949707, \"y\": 3.327604293823242}, {\"index\": 3532, \"title\": \"ARR || AT&T || 2204010040006385 || RCA for cluster terminated due to self bootstrap failure\", \"x\": 6.950405120849609, \"y\": 5.335873126983643}, {\"index\": 3327, \"title\": \"Clusters 'Running command' but not doing anything\", \"x\": 7.76743745803833, \"y\": 2.2027275562286377}, {\"index\": 5567, \"title\": \" Lost worker errors causing Databricks job to never finish\", \"x\": 8.795233726501465, \"y\": 3.7377543449401855}, {\"index\": 2677, \"title\": \"Unable to install pythonnet on cluster\", \"x\": 7.3575439453125, \"y\": 2.492478370666504}, {\"index\": 4305, \"title\": \"User did not receive workspace invite\", \"x\": 7.785009860992432, \"y\": 1.4169648885726929}, {\"index\": 2555, \"title\": \"ARR Customer Air Canada - Databricks job aborted\", \"x\": 6.077185153961182, \"y\": 2.850613832473755}, {\"index\": 4205, \"title\": \"There is a pipeline on ADF which has databricks notebooks that used to run for 6 hours now for even a \", \"x\": 7.528086185455322, \"y\": 4.131501197814941}, {\"index\": 840, \"title\": \"CSV file load issue using COPY INTO command\", \"x\": 8.940321922302246, \"y\": 2.378636598587036}, {\"index\": 3295, \"title\": \"AWS private link\", \"x\": 8.331299781799316, \"y\": 4.400831699371338}, {\"index\": 1854, \"title\": \"2204290060000134 \", \"x\": 8.467812538146973, \"y\": 3.0079281330108643}, {\"index\": 4727, \"title\": \"Failed to load user identity when calling dbfs\", \"x\": 8.226899147033691, \"y\": 3.660097360610962}, {\"index\": 4219, \"title\": \"User unable to refresh Extract on Tableau Online with a Databricks connection\", \"x\": 8.227749824523926, \"y\": 6.098550796508789}, {\"index\": 2394, \"title\": \"2204130030001378 | ARR | Not Able to Preview .py files in Databricks\", \"x\": 6.769157409667969, \"y\": 3.966858148574829}, {\"index\": 662, \"title\": \"Not able Manage Account and Devops \", \"x\": 8.096949577331543, \"y\": 5.548285007476807}, {\"index\": 1959, \"title\": \"ARR Customer: St Joseph - There is an issue with Databricks. we are not able to run any queries in Databricks workspace\", \"x\": 4.046014785766602, \"y\": 1.1335989236831665}, {\"index\": 1064, \"title\": \"File read from Container is taking very long time\", \"x\": 4.0152482986450195, \"y\": 1.1198341846466064}, {\"index\": 2334, \"title\": \"How to throw exception in a job running a shell script?\", \"x\": 7.446960926055908, \"y\": 3.5197482109069824}, {\"index\": 3398, \"title\": \"java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (integer overflow)\", \"x\": 3.996925115585327, \"y\": 1.2774296998977661}, {\"index\": 4801, \"title\": \"3121 - g - ARR - GAP - Jobs not ending\", \"x\": 9.34384822845459, \"y\": 5.534725666046143}, {\"index\": 1812, \"title\": \"Jobs Failing\", \"x\": 8.841536521911621, \"y\": 5.726806163787842}, {\"index\": 4955, \"title\": \"ARR |  Standard Chartered Bank | bokeh interactive chart cannot be displayed in databricks notebook| SR:2203090030001237\", \"x\": 9.691919326782227, \"y\": 5.796093940734863}, {\"index\": 2636, \"title\": \"NullPointer exception with Streamsets pipeline\", \"x\": 6.908810615539551, \"y\": 2.3441970348358154}, {\"index\": 3990, \"title\": \"We would like to limit the number of requests send to Databricks API from our web application. Is there a way to throttle the request, let's say 10 times with a 5 minute internal?\", \"x\": 7.652029037475586, \"y\": 4.654601573944092}, {\"index\": 1074, \"title\": \"Driver logs on UI are not showing correct creation time\", \"x\": 8.648212432861328, \"y\": 5.499258041381836}, {\"index\": 2328, \"title\": \"\\\"Run\\\" button greyed out in Databricks SQL\", \"x\": 7.5117597579956055, \"y\": 4.679015159606934}, {\"index\": 1847, \"title\": \"Follow up ticket for 00143692 | MSFT case 2204270030000780 \", \"x\": 8.440794944763184, \"y\": 5.3819379806518555}, {\"index\": 4971, \"title\": \"ARR | 2203110030000203 | Notebook activity taking more than the usual time and had to cancel the job\", \"x\": 3.9726250171661377, \"y\": 1.3228819370269775}, {\"index\": 3422, \"title\": \"migrate all the databases and tables form one workspace to another one\", \"x\": 6.846835136413574, \"y\": 5.439857482910156}, {\"index\": 3318, \"title\": \"MLflow autolog sklearn\", \"x\": 5.937690258026123, \"y\": 2.635552406311035}, {\"index\": 5030, \"title\": \"Databricks issue\", \"x\": 6.885173797607422, \"y\": 2.9156408309936523}, {\"index\": 4881, \"title\": \"Databricks SQL endpoint instability\", \"x\": 4.011765003204346, \"y\": 1.104623556137085}, {\"index\": 280, \"title\": \"Repo Error\", \"x\": 7.491517543792725, \"y\": 3.3091747760772705}, {\"index\": 5218, \"title\": \"[ARR] SR-#2202240030000132 Getting error while using databricks runtime version 9.1 LTS and 10.0\", \"x\": 6.242647647857666, \"y\": 4.183742046356201}, {\"index\": 3585, \"title\": \"Job parameters start_time and previous_run_start_time\", \"x\": 8.662444114685059, \"y\": 1.714424967765808}, {\"index\": 2579, \"title\": \"The cluster is not starting\", \"x\": 9.222671508789062, \"y\": 4.948681354522705}, {\"index\": 3338, \"title\": \"Spot fall back to On-demand Option on Job Cluster\", \"x\": 3.956117868423462, \"y\": 1.1074554920196533}, {\"index\": 1248, \"title\": \"2205090030001714\", \"x\": 3.9852190017700195, \"y\": 1.3143768310546875}, {\"index\": 231, \"title\": \"ARR | 2205160030001741 | AzureADAuthenticator.getTokenCall threw java.net.SocketTimeoutException : Read timed \", \"x\": 3.999908447265625, \"y\": 1.286482334136963}, {\"index\": 4388, \"title\": \"Not found Url error when loading the model \", \"x\": 7.311946868896484, \"y\": 1.9565683603286743}, {\"index\": 2909, \"title\": \"jobs hang or fail after upgrading to runtime 10.4 \", \"x\": 4.879593849182129, \"y\": 0.639155924320221}, {\"index\": 4167, \"title\": \"Cluster Not starting\", \"x\": 7.850379467010498, \"y\": 6.691019535064697}, {\"index\": 4793, \"title\": \"Unexpected user error while creating the cluster for the job. Cause: PERMISSION_DENIED: You are not authorized to create clusters\", \"x\": 10.577045440673828, \"y\": 3.4182684421539307}, {\"index\": 724, \"title\": \"Job aborted\", \"x\": 10.496429443359375, \"y\": 2.3633792400360107}, {\"index\": 5119, \"title\": \"ARR | 2203090040001426 | HP.com | Notebooks are cancelling with GPU errors\", \"x\": 3.955799102783203, \"y\": 1.1493871212005615}, {\"index\": 3017, \"title\": \"Setting job description in spark UI not working\", \"x\": 9.451946258544922, \"y\": 4.4618682861328125}, {\"index\": 2011, \"title\": \"ARR | 2203210050001428 | Scroll bars are not working in Firefox\", \"x\": 5.797954082489014, \"y\": 4.274633884429932}, {\"index\": 2415, \"title\": \"databricks sql connector 2.0.0 cannot run on DBR 7.3\", \"x\": 7.973517417907715, \"y\": 6.557327747344971}, {\"index\": 3903, \"title\": \"Fix ConcurrentAppendException \", \"x\": 8.383337020874023, \"y\": 3.208395481109619}, {\"index\": 2930, \"title\": \"Enable dbutils.secrets.get\", \"x\": 5.27310037612915, \"y\": 0.9735837578773499}, {\"index\": 2345, \"title\": \"Databricks QA jobs triggered by Tidal Scheduler are failing\", \"x\": 9.79084300994873, \"y\": 6.8637375831604}, {\"index\": 457, \"title\": \"SR 2205190050002600 - Photon | Spark | There is no space for new record\", \"x\": 7.334611415863037, \"y\": 6.047698974609375}, {\"index\": 5362, \"title\": \"Failure in launching Job cluster\", \"x\": 8.762069702148438, \"y\": 5.392348289489746}, {\"index\": 5386, \"title\": \"Production jobs failing\", \"x\": 8.693580627441406, \"y\": 6.612832069396973}, {\"index\": 1240, \"title\": \"Quota request for Other Requests - Unable to start the cluster\", \"x\": 6.349682331085205, \"y\": 1.4000413417816162}, {\"index\": 3167, \"title\": \"Autoloader failed to load parquet files when wildcard is used\", \"x\": 8.632050514221191, \"y\": 2.40175724029541}, {\"index\": 1848, \"title\": \"Databricks Notebook taking longer time moveing the data from ADLS to ADLS \", \"x\": 7.023883819580078, \"y\": 5.419910907745361}, {\"index\": 1596, \"title\": \"CSS-ARR-SR#2205040030000589-Jobs failing due to TCP/IP connection error\", \"x\": 6.372872352600098, \"y\": 3.8641867637634277}, {\"index\": 5473, \"title\": \"Limiting number of commits from job\", \"x\": 6.056581974029541, \"y\": 2.1696054935455322}, {\"index\": 2226, \"title\": \"DLT taking 30 minutes to start a triggered run\", \"x\": 6.567923069000244, \"y\": 3.1795308589935303}, {\"index\": 496, \"title\": \"Not able to create cluster pool with DBR Light 2.4 Extended\", \"x\": 8.45768928527832, \"y\": 2.790635824203491}, {\"index\": 1309, \"title\": \"Issues with model serve from mlflow\", \"x\": 6.405172824859619, \"y\": 4.558082580566406}, {\"index\": 2738, \"title\": \"2204150030000081\", \"x\": 7.934093475341797, \"y\": 1.443520188331604}, {\"index\": 5587, \"title\": \"2201250060001153\", \"x\": 8.120603561401367, \"y\": 4.233768463134766}, {\"index\": 2289, \"title\": \"The cluster is getting terminated automatically\", \"x\": 9.505733489990234, \"y\": 4.56497049331665}, {\"index\": 5480, \"title\": \"Ignition JDBC connection failure\", \"x\": 7.502756118774414, \"y\": 6.283663272857666}, {\"index\": 1385, \"title\": \"Need help with SHOW CREATE TABLE in spark higher version\", \"x\": 6.223167896270752, \"y\": 3.285046100616455}, {\"index\": 1735, \"title\": \"RDS error on control plane\", \"x\": 8.444693565368652, \"y\": 2.3997902870178223}, {\"index\": 5346, \"title\": \"Cannot authorize python API for MLFlow\", \"x\": 6.971327781677246, \"y\": 3.7143895626068115}, {\"index\": 4582, \"title\": \"Databricks\", \"x\": 8.26184368133545, \"y\": 3.905555486679077}, {\"index\": 566, \"title\": \"Azure Databricks autorun fails\", \"x\": 8.902030944824219, \"y\": 3.7699413299560547}, {\"index\": 214, \"title\": \"[Arr][Storage Latency]\", \"x\": 8.48216724395752, \"y\": 4.024469375610352}, {\"index\": 2470, \"title\": \"DB-004 Instructor-Led courses are not available for Ezra\", \"x\": 10.523831367492676, \"y\": 3.3782505989074707}, {\"index\": 1579, \"title\": \"Read csv/zip files from Github\", \"x\": 7.761621952056885, \"y\": 1.0099984407424927}, {\"index\": 4700, \"title\": \"Error logging in Databricks\", \"x\": 8.67042064666748, \"y\": 5.09749698638916}, {\"index\": 2048, \"title\": \"SSO issue with new databrick E2 implementation\", \"x\": 7.626723289489746, \"y\": 3.1090571880340576}, {\"index\": 1051, \"title\": \"2205110030000348\", \"x\": 6.208728313446045, \"y\": 2.8931500911712646}, {\"index\": 1660, \"title\": \"Adding Users through Okta Resets Group Entitlements\", \"x\": 5.828433990478516, \"y\": 3.2276387214660645}, {\"index\": 2157, \"title\": \"How to allow users with \\\"Can Restart\\\" permissions on a cluster to change Runtime Version and install libraries\", \"x\": 5.681156635284424, \"y\": 3.010453939437866}, {\"index\": 4842, \"title\": \"Job is running for longer, still executing post updating the DBR version Photon 10.0\", \"x\": 6.81205415725708, \"y\": 0.6646056175231934}, {\"index\": 2815, \"title\": \"multiple clusters show \\\"DBFS is down\\\"\", \"x\": 8.436253547668457, \"y\": 1.7223750352859497}, {\"index\": 4873, \"title\": \"com.springml.spark.salesforce\", \"x\": 7.220627784729004, \"y\": 5.095428943634033}, {\"index\": 1987, \"title\": \"2204270030001749 | Streaming job failure\", \"x\": 6.887202262878418, \"y\": 3.408508062362671}, {\"index\": 2294, \"title\": \"Error in writing data to synapse table(error occurred while authenticating against Managed Service Identity)\", \"x\": 3.9668984413146973, \"y\": 1.323079228401184}, {\"index\": 2981, \"title\": \"Upgrade account to E2\", \"x\": 8.286486625671387, \"y\": 4.656707286834717}, {\"index\": 4929, \"title\": \"Air Canada -  Clear Cache command not working in Databricks 9.1 LTS\", \"x\": 8.617093086242676, \"y\": 2.1281991004943848}, {\"index\": 1081, \"title\": \"memory error and Deprecated runtime\", \"x\": 8.966729164123535, \"y\": 3.2798736095428467}, {\"index\": 1163, \"title\": \"Can't able to access /mnt/advancedanalytics mount files in HC cluster\", \"x\": 5.950841903686523, \"y\": 4.126193523406982}, {\"index\": 4035, \"title\": \"Users unable to login\", \"x\": 7.987231254577637, \"y\": 3.9322996139526367}, {\"index\": 2949, \"title\": \"gar for 2203180050000275 \", \"x\": 4.011158466339111, \"y\": 1.1190320253372192}, {\"index\": 3339, \"title\": \"About SQL execute error after Databricks workspace creation\", \"x\": 6.616189479827881, \"y\": 4.734166145324707}, {\"index\": 1387, \"title\": \"I cannot make edits to instance pools\", \"x\": 8.43909740447998, \"y\": 1.5521316528320312}, {\"index\": 3358, \"title\": \"Performance tuning spark sql job\", \"x\": 9.273898124694824, \"y\": 3.1816163063049316}, {\"index\": 5109, \"title\": \"Can't write table into SQL Data Warehouse (Synapse Analytics) from Databricks\", \"x\": 5.401249885559082, \"y\": 2.144277572631836}, {\"index\": 557, \"title\": \"[ARR] [Sev C] SR-2205190030001704 Getting error on doing dbutils.fs.ls('dbfs:/mnt/uad/gokul/') \", \"x\": 6.639007568359375, \"y\": 2.876286745071411}, {\"index\": 501, \"title\": \"MS CSS\", \"x\": 8.070934295654297, \"y\": 3.514552593231201}, {\"index\": 1990, \"title\": \"Notebook command shows 'Running command' but nothing seems to be happening\", \"x\": 9.545732498168945, \"y\": 6.741820335388184}, {\"index\": 5233, \"title\": \"Please give access to submit support cases to team member\", \"x\": 4.037569999694824, \"y\": 1.2806377410888672}, {\"index\": 3052, \"title\": \"ARR | 2204110040002270 | Request access to Databricks Secrets using dbutils\", \"x\": 8.98453140258789, \"y\": 6.447056293487549}, {\"index\": 4859, \"title\": \"job\\u4efb\\u52a1oversea_meiju_device_active_day\\u62a5\\u9519\\uff0c\\u63d0\\u793asqlserver\\u7684\\u5185\\u5b58\\u4e0d\\u8db3\\uff0c\\u4f46\\u662f\\u67e5\\u8be2\\u63d2\\u5165\\u7684sqlserver\\u5e93\\u5185\\u5b58\\u662f\\u5145\\u8db3\\u7684\\uff0c\\u4e5f\\u6ca1\\u6709\\u63d0\\u793a\\u9884\\u8b66\", \"x\": 7.741828441619873, \"y\": 3.7832629680633545}, {\"index\": 3420, \"title\": \"Merge schema setting not working for nested columns on delta tables\", \"x\": 10.349127769470215, \"y\": 2.4964277744293213}, {\"index\": 1915, \"title\": \"Job Failures\", \"x\": 6.990549087524414, \"y\": 4.559551239013672}, {\"index\": 4449, \"title\": \"[ARR] SR-2203200030000229 Best way to do ML model monitoring on databricks\", \"x\": 9.042266845703125, \"y\": 4.607760906219482}, {\"index\": 2429, \"title\": \"Bronze DLT pipeline cancelled\", \"x\": 9.147822380065918, \"y\": 3.05015230178833}, {\"index\": 1802, \"title\": \"Several of DI\\u2019s jobs threw errors related to S3 path issues\", \"x\": 10.57247257232666, \"y\": 2.601318836212158}, {\"index\": 2950, \"title\": \"RuntimeError: could not open socket: [\\\"tried to connect to ('127.0.0.1', 33569), but an error occurred: [Errno 104] Connection reset by peer\\\"]\", \"x\": 6.984545707702637, \"y\": 4.7722578048706055}, {\"index\": 3589, \"title\": \"stream process fail on missing file\", \"x\": 9.167502403259277, \"y\": 4.515404224395752}, {\"index\": 745, \"title\": \"Databricks job failue OAUTH\", \"x\": 7.656978130340576, \"y\": 1.638820767402649}, {\"index\": 2989, \"title\": \"Issues in viewing the MLFLOW Experiments from Databricks MLFlow UI\", \"x\": 6.521998882293701, \"y\": 3.9168171882629395}, {\"index\": 1016, \"title\": \"Data governance gap for streaming workloads\", \"x\": 10.312066078186035, \"y\": 3.306335926055908}, {\"index\": 507, \"title\": \"Error Message: 'JavaPackage' object is not callable\", \"x\": 7.5035576820373535, \"y\": 3.3714821338653564}, {\"index\": 4548, \"title\": \"Change Feature Store table permission\", \"x\": 4.068984031677246, \"y\": 1.11663818359375}, {\"index\": 4558, \"title\": \"Getting SECURITY_DAEMON_REGISTRATION_EXCEPTION\", \"x\": 6.487342357635498, \"y\": 0.7452439665794373}, {\"index\": 98, \"title\": \"Not able to connect to ADLS with External HMS\", \"x\": 10.080936431884766, \"y\": 2.719449520111084}, {\"index\": 2567, \"title\": \"Changing/deleting our Account Owner\", \"x\": 8.781085014343262, \"y\": 2.7702088356018066}, {\"index\": 3427, \"title\": \"Databricks Authentication/Connectivity to Storage Account-\", \"x\": 8.224411010742188, \"y\": 1.9214856624603271}, {\"index\": 3886, \"title\": \"Duplicates records from sql server side\", \"x\": 4.7045512199401855, \"y\": 0.5348349213600159}, {\"index\": 2896, \"title\": \"Failure to write to cosmos db\", \"x\": 5.521060943603516, \"y\": 2.1863598823547363}, {\"index\": 906, \"title\": \"Job has been failing with xlrd error\", \"x\": 9.775099754333496, \"y\": 3.915611982345581}, {\"index\": 4625, \"title\": \"DNS failure\", \"x\": 3.997075080871582, \"y\": 1.236520528793335}, {\"index\": 1875, \"title\": \"ARR - follow-up to 00142942\", \"x\": 7.665237903594971, \"y\": 1.2367515563964844}, {\"index\": 927, \"title\": \"Central Model Registry Communication\", \"x\": 6.413888454437256, \"y\": 1.3842527866363525}, {\"index\": 5589, \"title\": \"Sample case for testing\", \"x\": 6.426271438598633, \"y\": 2.4380483627319336}, {\"index\": 2084, \"title\": \"PipeLine Run Fails\", \"x\": 6.130710124969482, \"y\": 4.1884284019470215}, {\"index\": 5304, \"title\": \"Delta lake schema evolution errors\", \"x\": 4.0121636390686035, \"y\": 1.2940940856933594}, {\"index\": 1914, \"title\": \"no access to unity catalog UI\", \"x\": 8.09189510345459, \"y\": 6.098756790161133}, {\"index\": 4750, \"title\": \"API: \\\"workspace-conf\\\" getting empty response \", \"x\": 6.8969902992248535, \"y\": 4.5182390213012695}, {\"index\": 2600, \"title\": \"GDM prod pcmdty value change issue\", \"x\": 8.753847122192383, \"y\": 4.766984462738037}, {\"index\": 3435, \"title\": \"Apache Spark Programming - Dataframes videos do not work - Academy lesson.\", \"x\": 8.970157623291016, \"y\": 6.3493971824646}, {\"index\": 5140, \"title\": \"Performance Dropdown when execute map partition\", \"x\": 8.871445655822754, \"y\": 3.6418263912200928}, {\"index\": 4431, \"title\": \"Performance issues with databricks \", \"x\": 10.200703620910645, \"y\": 5.127162456512451}, {\"index\": 556, \"title\": \"Event Hub To Databricks - 20min connectivity unexplainable throughput drop - RCA assistance requested\", \"x\": 10.03958797454834, \"y\": 4.133025169372559}, {\"index\": 3658, \"title\": \"Spring Core vulnerability and affected services\", \"x\": 7.531380653381348, \"y\": 4.1717400550842285}, {\"index\": 1953, \"title\": \"Job error out after few hours of executio\", \"x\": 9.066659927368164, \"y\": 4.215702533721924}, {\"index\": 2031, \"title\": \"Default R-package repo to be in sync with DBR runtime's Microsoft CRAN\", \"x\": 4.0384650230407715, \"y\": 1.3123079538345337}, {\"index\": 4015, \"title\": \"running into an issue with UDFs parsing protobufs\", \"x\": 6.592343807220459, \"y\": 4.077356338500977}, {\"index\": 538, \"title\": \"2205200030000589\", \"x\": 7.722987174987793, \"y\": 4.941447734832764}, {\"index\": 3779, \"title\": \"trt\", \"x\": 9.750961303710938, \"y\": 6.8249616622924805}, {\"index\": 5552, \"title\": \"Driver is up but is not responsive, likely due to GC;Spark exception received from driver. Driver down;\", \"x\": 8.131762504577637, \"y\": 6.216325283050537}, {\"index\": 4993, \"title\": \"sql query hangs forever\", \"x\": 9.687896728515625, \"y\": 6.895482063293457}, {\"index\": 5127, \"title\": \"Can't save table from S3 delta files\", \"x\": 4.449122905731201, \"y\": 1.7072232961654663}, {\"index\": 4028, \"title\": \"Failures of scheduled jobs: Unable to start or continue the running clusters. Inconsistent\", \"x\": 7.859147548675537, \"y\": 5.642616271972656}, {\"index\": 3132, \"title\": \"Process failed in production\", \"x\": 6.889092922210693, \"y\": 0.4910654127597809}, {\"index\": 1235, \"title\": \"2205090040003848001\", \"x\": 5.930753707885742, \"y\": 4.388935565948486}, {\"index\": 3407, \"title\": \"2203310040003551 GENIE\", \"x\": 5.884531021118164, \"y\": 4.873479843139648}, {\"index\": 5064, \"title\": \"How do we simulate run_id int64 change impact before 3/12\", \"x\": 7.021156311035156, \"y\": 4.761536121368408}, {\"index\": 3265, \"title\": \"How to provide SQL Access to a Group for running queries, explore data\", \"x\": 7.847067832946777, \"y\": 4.752682209014893}, {\"index\": 2764, \"title\": \"ARR Customer GEICO - NoSuchMethodError exception suspect to be the classpath issue\", \"x\": 4.049444675445557, \"y\": 1.2868797779083252}, {\"index\": 2000, \"title\": \"alter database  set location\", \"x\": 7.349018573760986, \"y\": 3.969489574432373}, {\"index\": 1050, \"title\": \"Expired personal acces token (PAT) in combination with IP-adress whilelist for Databricks workspace\", \"x\": 7.719091415405273, \"y\": 3.428764820098877}, {\"index\": 3003, \"title\": \"SQL access\", \"x\": 6.399256229400635, \"y\": 0.8793084025382996}, {\"index\": 4819, \"title\": \"Need to get number of pinned clusters increased to 150\", \"x\": 5.623859882354736, \"y\": 2.4182794094085693}, {\"index\": 2569, \"title\": \"Instance fleet is disabled\", \"x\": 4.111917972564697, \"y\": 1.2399160861968994}, {\"index\": 2535, \"title\": \"Workspace SSO authentication access and admin access not working\", \"x\": 6.614612579345703, \"y\": 1.3235584497451782}, {\"index\": 1751, \"title\": \"ARR | T-Mobile USA | Reading to df got stuck | SR: 2205020040000453 \", \"x\": 7.9424052238464355, \"y\": 1.7302912473678589}, {\"index\": 3849, \"title\": \"support for gs:// sources for GCP\", \"x\": 10.015068054199219, \"y\": 2.7570481300354004}, {\"index\": 5, \"title\": \"Create Delta Table Statements Fail Intermittently\", \"x\": 10.170601844787598, \"y\": 2.480238914489746}, {\"index\": 4907, \"title\": \"ARR : ATT : 2203110040000165 -  Job aborted error trying to write to Azure storage blob\", \"x\": 7.364166736602783, \"y\": 3.9422831535339355}, {\"index\": 4769, \"title\": \"Enforce tags EC2: IAM Policy + Cluster Policy\", \"x\": 8.56332778930664, \"y\": 4.172175884246826}, {\"index\": 544, \"title\": \"Delta Lake Change Feed Data error on job restart\", \"x\": 8.461893081665039, \"y\": 3.5827624797821045}, {\"index\": 1404, \"title\": \"AutoLoader Performance Issues\", \"x\": 8.727128028869629, \"y\": 4.173280715942383}, {\"index\": 575, \"title\": \"Workspace unavailable\", \"x\": 9.164560317993164, \"y\": 3.2403199672698975}, {\"index\": 4526, \"title\": \"databricks privatelink\", \"x\": 8.340617179870605, \"y\": 1.076167345046997}, {\"index\": 1794, \"title\": \"7166646288419298 - 051ce839-3786-4339-b3fd-df3efda185de - 2204270040005587\", \"x\": 5.863001823425293, \"y\": 4.8515424728393555}, {\"index\": 548, \"title\": \"GC driver - OOM error, same code with same data volume worked fine previously\", \"x\": 10.338652610778809, \"y\": 2.4268476963043213}, {\"index\": 3716, \"title\": \"Need to opnion about optimization\", \"x\": 7.434854984283447, \"y\": 3.7054762840270996}, {\"index\": 3522, \"title\": \"Queries stuck in running state in sql endpoint\", \"x\": 8.943219184875488, \"y\": 6.718484878540039}, {\"index\": 1960, \"title\": \"Cluster autoscaling issue\", \"x\": 4.0191545486450195, \"y\": 1.3146623373031616}, {\"index\": 5426, \"title\": \"Unable to get Git Integration working\", \"x\": 9.918450355529785, \"y\": 6.660534381866455}, {\"index\": 3570, \"title\": \"CVE-2022-22965  Impact analysis\", \"x\": 6.735952854156494, \"y\": 3.1960926055908203}, {\"index\": 513, \"title\": \"Private Link Feature Request\", \"x\": 8.153547286987305, \"y\": 4.363897800445557}, {\"index\": 2307, \"title\": \"[ARR][Cluster terminated.Reason:Azure Resource Provider throttling]\", \"x\": 4.03181791305542, \"y\": 1.2581502199172974}, {\"index\": 3230, \"title\": \"workspace access - 2203310030000887\", \"x\": 10.092357635498047, \"y\": 6.040554046630859}, {\"index\": 2338, \"title\": \"2204200040005851  Not able to use Service Principal to create Databricks Azure Key Vault-backed secret scope\", \"x\": 4.404718399047852, \"y\": 0.9476327896118164}, {\"index\": 2985, \"title\": \"2204080030001823\", \"x\": 8.193395614624023, \"y\": 3.722465753555298}, {\"index\": 4845, \"title\": \"REQUEST_REJECTED(CLIENT_ERROR): databricks_error_message:Your workspace has been temporarily disabled. Please contact Databricks Support for more information.\", \"x\": 7.228854179382324, \"y\": 6.08431339263916}, {\"index\": 4853, \"title\": \"SQL endpoint query timeouts/connectionRefused errors\", \"x\": 8.507445335388184, \"y\": 4.938283920288086}, {\"index\": 1796, \"title\": \"Unable to connect to Azure SQL DB- Saved\", \"x\": 8.909162521362305, \"y\": 4.569681644439697}, {\"index\": 5092, \"title\": \"Some Tables are missing from one of our Production workspace\", \"x\": 8.907631874084473, \"y\": 3.880401372909546}, {\"index\": 1225, \"title\": \"Log out databricks.webapp functionality for the shard.yaml file\", \"x\": 8.0742826461792, \"y\": 6.51936149597168}, {\"index\": 62, \"title\": \"ARR Air Canada - Jobs randomly failing with module not found. \", \"x\": 9.242487907409668, \"y\": 4.967578887939453}, {\"index\": 3865, \"title\": \"Unable to run some functions from Rstudio to databricks\", \"x\": 7.311849117279053, \"y\": 2.3223118782043457}, {\"index\": 4760, \"title\": \"Terraform pipelines failing \", \"x\": 7.0039448738098145, \"y\": 0.7918753027915955}, {\"index\": 1117, \"title\": \"Please Engage Microsoft Azure Support - Databricks loading  JDBC SQL driver libraries  (Intermittent in prod & non-prod)\", \"x\": 5.6344523429870605, \"y\": 4.610621452331543}, {\"index\": 108, \"title\": \"recover deleted azure databricksm\", \"x\": 10.682847023010254, \"y\": 4.356415271759033}, {\"index\": 517, \"title\": \"Error in jobs: OutOfMemorySparkException \", \"x\": 10.654788970947266, \"y\": 3.499901533126831}, {\"index\": 2227, \"title\": \"6899 - g - ARR - anheuser busch - Table just created several commands earlier not found\", \"x\": 8.783275604248047, \"y\": 2.4549508094787598}, {\"index\": 4770, \"title\": \"Batch duration changes in DBR 9.1 version\", \"x\": 9.774420738220215, \"y\": 6.775694370269775}, {\"index\": 45, \"title\": \"MLflow protobuf Python version problem\", \"x\": 7.854630470275879, \"y\": 4.525274753570557}, {\"index\": 1918, \"title\": \"Databricks API Gateway Timeout\", \"x\": 10.13766860961914, \"y\": 2.615748643875122}, {\"index\": 1533, \"title\": \"[ARR] [Sev C] SR-2204110030000414 RCA  continue to 00141951  cannot evaluate expression current_timestamp() \", \"x\": 4.773125648498535, \"y\": 1.7618576288223267}, {\"index\": 2667, \"title\": \"problems with Databricks cluster\", \"x\": 7.653846263885498, \"y\": 5.736139297485352}, {\"index\": 4235, \"title\": \"Not able to access MYSQL database from SQL endpoint on Databricks cluster\", \"x\": 5.588225841522217, \"y\": 4.630790710449219}, {\"index\": 3926, \"title\": \"SSL cert error\", \"x\": 4.06494140625, \"y\": 1.375412940979004}, {\"index\": 3252, \"title\": \"SQL endpoint display wrong value when the length of number is more than 15 digit\", \"x\": 9.725236892700195, \"y\": 4.940791606903076}, {\"index\": 4585, \"title\": \"Streaming query Exception : exception thrown awaitResult\", \"x\": 4.022401332855225, \"y\": 1.3925797939300537}, {\"index\": 2578, \"title\": \"Prod job fails to start a cluster\", \"x\": 8.181017875671387, \"y\": 4.6417155265808105}, {\"index\": 3868, \"title\": \"Failed to start cluster due to error Azure VM Extension Failure\", \"x\": 8.045072555541992, \"y\": 4.313138484954834}, {\"index\": 1686, \"title\": \"Databricks CLI does not accept PAT to authenticate\", \"x\": 8.035958290100098, \"y\": 4.91888952255249}, {\"index\": 778, \"title\": \"Follow up case | 00143287\", \"x\": 7.2061448097229, \"y\": 3.943807363510132}, {\"index\": 2270, \"title\": \"Not able to install libraries tmap and epiR\", \"x\": 8.843097686767578, \"y\": 4.7211384773254395}, {\"index\": 1023, \"title\": \"Issue loading data to Cosmo DB\", \"x\": 7.767542839050293, \"y\": 3.4846177101135254}, {\"index\": 4943, \"title\": \"Folium stopped working\", \"x\": 7.533905506134033, \"y\": 2.07808780670166}, {\"index\": 3128, \"title\": \"Error starting the 'RStudio' cluster. \", \"x\": 4.7831010818481445, \"y\": 0.5199180841445923}, {\"index\": 348, \"title\": \"Databricks related queries\", \"x\": 7.495960235595703, \"y\": 2.5285701751708984}, {\"index\": 4248, \"title\": \"credential pass through authentication\", \"x\": 9.5974760055542, \"y\": 6.150805473327637}, {\"index\": 4314, \"title\": \"Feature Store package in Standard DBR\", \"x\": 4.782646656036377, \"y\": 0.5072051286697388}, {\"index\": 1885, \"title\": \"3337075389283370 - 6b64b59f-432d-401e-b552-d855f1e1d2e0 - 2204060040002674\", \"x\": 9.828832626342773, \"y\": 6.812028884887695}, {\"index\": 689, \"title\": \"Kafka Consumer Script Returns NULL Values\", \"x\": 9.135087013244629, \"y\": 2.134305238723755}, {\"index\": 389, \"title\": \"Delta Live Tables Cluster Init Scripts?\", \"x\": 8.621498107910156, \"y\": 5.658514976501465}, {\"index\": 3789, \"title\": \"Optimisation for PySpark Consumer\", \"x\": 7.672699451446533, \"y\": 5.932027816772461}, {\"index\": 839, \"title\": \"Misleading error message when github token is given but expired\", \"x\": 4.800481796264648, \"y\": 0.5584844350814819}, {\"index\": 3595, \"title\": \"Getting Error on SQL End point\", \"x\": 7.282820701599121, \"y\": 4.246459007263184}, {\"index\": 589, \"title\": \"HTML Injection Finding\", \"x\": 8.911954879760742, \"y\": 3.4876458644866943}, {\"index\": 5277, \"title\": \"Inconsistent ETL Job Behaviour after upgradation to 7.3 LTS\", \"x\": 6.213831424713135, \"y\": 1.810853123664856}, {\"index\": 1916, \"title\": \"ARR| HP Inc.| databricks-registry-webhooks |SR:2204270030001287 \", \"x\": 6.2483086585998535, \"y\": 5.066600322723389}, {\"index\": 1056, \"title\": \"Suspen Notification of Workspace\", \"x\": 9.9872407913208, \"y\": 4.7998738288879395}, {\"index\": 5239, \"title\": \"Query parsing errors\", \"x\": 9.711507797241211, \"y\": 4.456707954406738}, {\"index\": 2406, \"title\": \"Running a Python script in Databricks that currently is in local computer\", \"x\": 10.003320693969727, \"y\": 5.206140041351318}, {\"index\": 2696, \"title\": \"Cannot store / search MLFlow model by tag\", \"x\": 10.474504470825195, \"y\": 3.761648416519165}, {\"index\": 367, \"title\": \" Recover databricks workspace\", \"x\": 8.925477981567383, \"y\": 6.406708240509033}, {\"index\": 1139, \"title\": \"2205110030000282 |IHS Markit| POI module not working\", \"x\": 8.055293083190918, \"y\": 3.300391435623169}, {\"index\": 4494, \"title\": \"Performance issues\", \"x\": 6.85293436050415, \"y\": 4.301469326019287}, {\"index\": 3727, \"title\": \"Follow up case 00136537\", \"x\": 6.6564106941223145, \"y\": 3.983652114868164}, {\"index\": 5088, \"title\": \"S3 exceptions on HEAD request for bucket\", \"x\": 10.381377220153809, \"y\": 3.6276516914367676}, {\"index\": 4410, \"title\": \"The SQL Editor doesn't properly show the query status \", \"x\": 8.699949264526367, \"y\": 4.619643211364746}, {\"index\": 1102, \"title\": \"Autoloader streaming is not consistent\", \"x\": 8.941539764404297, \"y\": 6.399440288543701}, {\"index\": 957, \"title\": \"SSH Databricks cluster connection error Power BI\", \"x\": 10.22575569152832, \"y\": 5.068428039550781}, {\"index\": 3698, \"title\": \"Consequences of New Java Spring Core Vulnerability\", \"x\": 7.243354797363281, \"y\": 2.972877025604248}, {\"index\": 3811, \"title\": \"Spark Structured Streaming Input & Processing Graph stuck at 0 records / second for both input and processing\", \"x\": 5.5828938484191895, \"y\": 4.361565589904785}, {\"index\": 3668, \"title\": \"ARR 2203300050002445- Cannot see full SQL Statement\", \"x\": 6.015978813171387, \"y\": 1.4906578063964844}, {\"index\": 2719, \"title\": \"Databricks prd is down\", \"x\": 8.397072792053223, \"y\": 3.7682552337646484}, {\"index\": 2928, \"title\": \"10.4 and 9.1 Compatibility issues when using pyspark.sql.dataframe.filter\", \"x\": 5.826910495758057, \"y\": 3.5495893955230713}, {\"index\": 5583, \"title\": \"Databricks SQL query error\", \"x\": 9.801667213439941, \"y\": 6.896275997161865}, {\"index\": 1123, \"title\": \"Metadata Queries take hours in sql endpoints randomely\", \"x\": 9.95397663116455, \"y\": 3.178375005722046}, {\"index\": 1476, \"title\": \"Clusters failing with internal Failures\", \"x\": 7.355483531951904, \"y\": 1.2585476636886597}, {\"index\": 2687, \"title\": \"Network Configuration Update Not Working\", \"x\": 9.09745979309082, \"y\": 3.7111382484436035}, {\"index\": 2544, \"title\": \"Suddenly unable to log in to workspaces via SSO\", \"x\": 8.272336959838867, \"y\": 6.321181297302246}, {\"index\": 4642, \"title\": \"6973325950234402 - 6dfbe157-2219-4313-adfc-6df4829ab651 - 2203160010002617\", \"x\": 5.309699058532715, \"y\": 1.558002233505249}, {\"index\": 3528, \"title\": \"7464501611136417 - 7cca3be6-ea39-4394-9aab-242213bd98e5 - 2203310030003446\", \"x\": 4.033020973205566, \"y\": 1.2826719284057617}, {\"index\": 4067, \"title\": \"Inconsistent error across sql endpoints on different databricks e2 workspaces\", \"x\": 10.300968170166016, \"y\": 5.234792232513428}, {\"index\": 2018, \"title\": \"Enable secrets in DB-connect\", \"x\": 6.840964317321777, \"y\": 4.526979923248291}, {\"index\": 867, \"title\": \"enable table access control\", \"x\": 9.023674964904785, \"y\": 3.2853994369506836}, {\"index\": 4156, \"title\": \"when the cluster is getting restart, 1 node is getting skipped due to slow node\", \"x\": 8.451567649841309, \"y\": 6.2340288162231445}, {\"index\": 588, \"title\": \"http issues\", \"x\": 6.115316390991211, \"y\": 2.849391460418701}, {\"index\": 5306, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2202230040008781 \", \"x\": 4.099571704864502, \"y\": 1.4926656484603882}, {\"index\": 2861, \"title\": \"Admin users can't login\", \"x\": 5.442729949951172, \"y\": 1.4441288709640503}, {\"index\": 2584, \"title\": \"Clusters failing to start in production workspace - UNEXPECTED_LAUNCH_FAILURE - SERVICE_FAULT\", \"x\": 5.586299896240234, \"y\": 4.492611408233643}, {\"index\": 4018, \"title\": \"Receiving Error On Delta Table Write Operation, But Operation Completes\", \"x\": 4.938425540924072, \"y\": 0.6679877042770386}, {\"index\": 1370, \"title\": \"The VMs are not getting deallocated and costing to us\", \"x\": 7.8352460861206055, \"y\": 1.0389102697372437}, {\"index\": 555, \"title\": \"2205130040002413\", \"x\": 7.88906717300415, \"y\": 1.9975043535232544}, {\"index\": 3384, \"title\": \"ARR-[REG:2203300030003575]Issues in automating workspace setting of databricks\", \"x\": 9.988454818725586, \"y\": 3.7208898067474365}, {\"index\": 2319, \"title\": \"Parallel writes fail sporatically\", \"x\": 8.729142189025879, \"y\": 3.134377956390381}, {\"index\": 2796, \"title\": \"[ARR] [Sev B] SR-2204070010003095-How to install init script , python libraries and MQ Client\", \"x\": 6.233737468719482, \"y\": 2.9696640968322754}, {\"index\": 968, \"title\": \"2205110040001203\", \"x\": 8.840753555297852, \"y\": 4.121952056884766}, {\"index\": 940, \"title\": \"Databricks jobs getting failed due to AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE \", \"x\": 5.390853404998779, \"y\": 1.1562824249267578}, {\"index\": 4839, \"title\": \"Delta table optimize run 'MetadataFetchFailedExceptio'n failure\", \"x\": 6.553738594055176, \"y\": 3.015986680984497}, {\"index\": 996, \"title\": \"2205130060000046 \", \"x\": 3.9711811542510986, \"y\": 1.1471928358078003}, {\"index\": 3833, \"title\": \"test\", \"x\": 8.135437965393066, \"y\": 3.5445563793182373}, {\"index\": 2586, \"title\": \"Jobs are failing due to \\u2018Too many connections\\u2019 error\", \"x\": 6.720688343048096, \"y\": 3.345287322998047}, {\"index\": 4007, \"title\": \"Hanging Spark Job\", \"x\": 7.897215366363525, \"y\": 2.2077925205230713}, {\"index\": 1715, \"title\": \"Add IP to allow list\", \"x\": 8.645407676696777, \"y\": 2.865061044692993}, {\"index\": 941, \"title\": \"2205110050001274 - diagnostic logs and encryption of managed resouces of databricks\", \"x\": 6.485024929046631, \"y\": 5.106894016265869}, {\"index\": 2487, \"title\": \"getting error during configuration of log delivery\", \"x\": 10.580559730529785, \"y\": 4.397390365600586}, {\"index\": 432, \"title\": \"Job Failure\", \"x\": 7.038872718811035, \"y\": 5.707870006561279}, {\"index\": 5302, \"title\": \"Restrict admin login to only SAML. Do not allow local login\", \"x\": 10.650609016418457, \"y\": 4.347954750061035}, {\"index\": 1993, \"title\": \"Nodes Lost Communication\", \"x\": 10.277046203613281, \"y\": 2.608710527420044}, {\"index\": 349, \"title\": \"[ARR][Latency issue in Databricks workspace]\", \"x\": 5.389188766479492, \"y\": 1.1598091125488281}, {\"index\": 2881, \"title\": \"Job Cluster Notebooks are not visible to users\", \"x\": 4.835124969482422, \"y\": 0.5557606816291809}, {\"index\": 4593, \"title\": \"Need help from Databricks Team - 2203090030002266\", \"x\": 9.873370170593262, \"y\": 2.753020763397217}, {\"index\": 4301, \"title\": \"Photon ran out of memory error while switching from DBFS to ADLS\", \"x\": 7.893824100494385, \"y\": 2.755549430847168}, {\"index\": 2530, \"title\": \"2204190030001714 \", \"x\": 10.092703819274902, \"y\": 2.266629219055176}, {\"index\": 3507, \"title\": \"Follow up of SF ticket 00139230\", \"x\": 10.211350440979004, \"y\": 2.978139638900757}, {\"index\": 1396, \"title\": \"Issues Serving Pyspark Recommendations Models\", \"x\": 5.989810943603516, \"y\": 2.4781923294067383}, {\"index\": 434, \"title\": \"2205230010002783  Spark Task runs and does not rerturn results \", \"x\": 5.672142505645752, \"y\": 4.376946449279785}, {\"index\": 2890, \"title\": \"Cluster terminated.Reason:Azure Vm Extension Failure\", \"x\": 10.128511428833008, \"y\": 4.7587199211120605}, {\"index\": 1406, \"title\": \"Admin Account Console SSO\", \"x\": 7.00521993637085, \"y\": 5.324084758758545}, {\"index\": 2938, \"title\": \"Follow up of(00140747) Cluster Configuration changed with new Hardware Types.\", \"x\": 8.159463882446289, \"y\": 6.191511631011963}, {\"index\": 2931, \"title\": \"Control plane service endpoint configuration for the region`europe-west3`\", \"x\": 8.651941299438477, \"y\": 3.8963229656219482}, {\"index\": 4298, \"title\": \"How do I use the REST API to provide a service principal access to an instance profile\", \"x\": 9.646113395690918, \"y\": 5.484821796417236}, {\"index\": 5476, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2203020040009426\", \"x\": 7.62463903427124, \"y\": 5.555961608886719}, {\"index\": 1748, \"title\": \"2205020030000110  | Production jobs are failing with Argument not found error  |  AholdDelhaize.com\", \"x\": 7.080574989318848, \"y\": 0.42471879720687866}, {\"index\": 4977, \"title\": \"Data Loss while writing data to Azure SQL Database via Databricks\", \"x\": 8.574402809143066, \"y\": 2.125373363494873}, {\"index\": 1321, \"title\": \"Cluster creation failing with Instance Profile\", \"x\": 8.323480606079102, \"y\": 5.1858344078063965}, {\"index\": 4534, \"title\": \"Job terminated due to executor termination\", \"x\": 9.371006965637207, \"y\": 3.9551000595092773}, {\"index\": 4922, \"title\": \"ARR 2203110050001847 - Notebook being extracted through API\", \"x\": 10.324475288391113, \"y\": 5.444403648376465}, {\"index\": 2805, \"title\": \"Azure Databricks Cluster\", \"x\": 7.925871849060059, \"y\": 3.9501335620880127}, {\"index\": 4615, \"title\": \"Inquiry on why DBFS was unmounted\", \"x\": 6.1694488525390625, \"y\": 3.4935624599456787}, {\"index\": 2517, \"title\": \"Unable to spin up cluster from the workspace \", \"x\": 4.957891941070557, \"y\": 0.6723591685295105}, {\"index\": 1946, \"title\": \"7677 - g - ARR - jnj - cluster not starting in time\", \"x\": 10.538692474365234, \"y\": 4.340056419372559}, {\"index\": 600, \"title\": \"Unable to grant access to database\", \"x\": 10.301929473876953, \"y\": 2.9939496517181396}, {\"index\": 3234, \"title\": \"CSS ARR | 2203230030001649 - ADF pipeline job is failing in CART-ADF-Prod\", \"x\": 6.395347595214844, \"y\": 2.397914171218872}, {\"index\": 1924, \"title\": \"CSS-ARR-S500-SR#2204280030000558-Unable to read the data from offset for eventhub using spark jobs\", \"x\": 9.372575759887695, \"y\": 3.9531490802764893}, {\"index\": 248, \"title\": \" Databricks Support - SQL Workspace Issue\", \"x\": 8.378454208374023, \"y\": 1.0091865062713623}, {\"index\": 2185, \"title\": \"Cluster not starting\", \"x\": 9.787029266357422, \"y\": 2.465066909790039}, {\"index\": 4527, \"title\": \"Intermittent error:  Transport endpoint is not connected:\", \"x\": 8.990633964538574, \"y\": 5.929808139801025}, {\"index\": 4528, \"title\": \"SQL Analytics - Instance Profile Limitation\", \"x\": 9.721074104309082, \"y\": 3.7483229637145996}, {\"index\": 3881, \"title\": \"SQL queries\", \"x\": 6.1014180183410645, \"y\": 2.702408790588379}, {\"index\": 5436, \"title\": \"Failed to find data source\", \"x\": 6.3073954582214355, \"y\": 4.312999725341797}, {\"index\": 5516, \"title\": \"ARR 2203020050001288 - Customer needs to increase job performance\", \"x\": 8.445176124572754, \"y\": 4.152459621429443}, {\"index\": 1637, \"title\": \"Can not access https URL from databricks workspace (privatelink)\", \"x\": 7.897213459014893, \"y\": 2.7306132316589355}, {\"index\": 2854, \"title\": \"SHOW GRANT is not working\", \"x\": 6.455514430999756, \"y\": 4.305894374847412}, {\"index\": 4430, \"title\": \"Databricks: unresolved dependency: joda-time:joda-time:2.9.9: not found\", \"x\": 6.987950801849365, \"y\": 1.6628623008728027}, {\"index\": 4634, \"title\": \"JAR Upload Failing\", \"x\": 8.195256233215332, \"y\": 3.711198329925537}, {\"index\": 596, \"title\": \"Feature Store API in Docker\", \"x\": 8.100141525268555, \"y\": 2.7712767124176025}, {\"index\": 1125, \"title\": \"ARR - write txt file with headers in the datalake from databricks\", \"x\": 9.455887794494629, \"y\": 4.731478691101074}, {\"index\": 1026, \"title\": \"regular spark driver failures in job after upgrading 9.1LTS to 10.4LTS\", \"x\": 5.4261474609375, \"y\": 2.4251534938812256}, {\"index\": 3503, \"title\": \"All WebUI is down\", \"x\": 8.192191123962402, \"y\": 1.2195746898651123}, {\"index\": 3862, \"title\": \"Cannot delete Service Principals\", \"x\": 6.691472053527832, \"y\": 2.9156858921051025}, {\"index\": 1822, \"title\": \"unable to create Db and tables\", \"x\": 9.237350463867188, \"y\": 6.674368381500244}, {\"index\": 4479, \"title\": \"AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE and other errors\", \"x\": 8.558380126953125, \"y\": 4.311371326446533}, {\"index\": 3378, \"title\": \"Market specific SQL Endpoint creation\", \"x\": 8.540905952453613, \"y\": 3.563556432723999}, {\"index\": 344, \"title\": \"scheduled job did not get cluster assigned.\", \"x\": 4.831805229187012, \"y\": 0.5714954137802124}, {\"index\": 4995, \"title\": \"2203080040008835 | Job Failure\", \"x\": 8.014925956726074, \"y\": 3.9020843505859375}, {\"index\": 701, \"title\": \"ARR Customer Credit Swiss - Unable to Run Notebooks \", \"x\": 6.295962333679199, \"y\": 4.008299350738525}, {\"index\": 4139, \"title\": \"2203220050000608 S Databricks SQL unexpected behaviour when casting\", \"x\": 5.687309265136719, \"y\": 3.9439845085144043}, {\"index\": 4150, \"title\": \"unable to create repo \", \"x\": 7.032193183898926, \"y\": 4.82802677154541}, {\"index\": 4393, \"title\": \"2202280040007478\", \"x\": 7.535052299499512, \"y\": 1.4453550577163696}, {\"index\": 3925, \"title\": \"Testing: case number\", \"x\": 8.547811508178711, \"y\": 2.2306301593780518}, {\"index\": 5029, \"title\": \"Node communication lost errors and severe performance issues\", \"x\": 8.869086265563965, \"y\": 4.172455310821533}, {\"index\": 1852, \"title\": \"Error loading to Synapse\", \"x\": 7.6788411140441895, \"y\": 1.2995201349258423}, {\"index\": 3321, \"title\": \"Python shell exits with 143 exception and REPL gets restarted continously | ARR SR# 2204060030002257\", \"x\": 8.63059139251709, \"y\": 4.103927135467529}, {\"index\": 3026, \"title\": \"unusable since the driver is unhealthy\", \"x\": 6.88183069229126, \"y\": 2.3251819610595703}, {\"index\": 4986, \"title\": \"2202280010003137 | Job Issue\", \"x\": 9.688591957092285, \"y\": 2.155902147293091}, {\"index\": 5023, \"title\": \"Encounter Power BI refresh error\", \"x\": 4.812926769256592, \"y\": 0.5338762998580933}, {\"index\": 5035, \"title\": \"Cluster \\\"etl_Cluster\\\" was reboot at 2022-3-6 0:05:32 UTC\", \"x\": 6.609225749969482, \"y\": 0.6446060538291931}, {\"index\": 2434, \"title\": \"Enable Files in Repos option no available \", \"x\": 8.590055465698242, \"y\": 5.686746597290039}, {\"index\": 726, \"title\": \"Subscription is not registered for ADB private workspace\", \"x\": 7.729608058929443, \"y\": 2.422111988067627}, {\"index\": 2588, \"title\": \"Multiple Intermittent Cluster Failures in Production\", \"x\": 5.8210625648498535, \"y\": 4.218538761138916}, {\"index\": 4860, \"title\": \"Not able to mount storage account \", \"x\": 7.858767509460449, \"y\": 3.2717010974884033}, {\"index\": 1424, \"title\": \"Databricks execution failed with error state: InternalError, error message: Library installation failed for library due to infra fault. \", \"x\": 5.372280120849609, \"y\": 1.144943356513977}, {\"index\": 102, \"title\": \" Unable to access AzureDatabricks workspace\", \"x\": 9.99685001373291, \"y\": 5.154191017150879}, {\"index\": 416, \"title\": \"Conuming avro messages from kafka makes problem.\", \"x\": 8.855741500854492, \"y\": 4.163760185241699}, {\"index\": 4071, \"title\": \"CalledProcessError: Command 'pip install 'opencensus.ext.azure'' returned non-zero exit status 1.- \", \"x\": 4.9166483879089355, \"y\": 0.6724607944488525}, {\"index\": 3563, \"title\": \"Execution Error when accessing DBFS\", \"x\": 5.607470512390137, \"y\": 3.9514620304107666}, {\"index\": 1884, \"title\": \"Test case by Pablo\", \"x\": 10.241260528564453, \"y\": 5.046872615814209}, {\"index\": 5103, \"title\": \"failing job\", \"x\": 7.784653663635254, \"y\": 2.120122194290161}, {\"index\": 275, \"title\": \"Problema de conectividade DATABRICKS via firewall- Saved\", \"x\": 8.458863258361816, \"y\": 2.9377269744873047}, {\"index\": 2657, \"title\": \"Internet access issue through fortigate firewall. where we added the UDR on the subnet of databricks\", \"x\": 8.829319953918457, \"y\": 3.8860599994659424}, {\"index\": 3277, \"title\": \"Queries that used to work on 5.5LTS   fail in 10.4 LTS\", \"x\": 7.862800121307373, \"y\": 1.1332175731658936}, {\"index\": 1078, \"title\": \"ARR | 2205110030001559 | please help find instanceid/vmid\", \"x\": 9.571619987487793, \"y\": 2.7841684818267822}, {\"index\": 3244, \"title\": \"CSS-ARR-SFMC-2204070030000778-RU is increasing in GR from yesterday night in streming job\", \"x\": 10.441461563110352, \"y\": 3.816054582595825}, {\"index\": 2617, \"title\": \"Init script having rstudio server installation failing\", \"x\": 6.883841037750244, \"y\": 1.4937589168548584}, {\"index\": 4686, \"title\": \"Audit Log  configuration \", \"x\": 8.04307746887207, \"y\": 3.1381070613861084}, {\"index\": 3087, \"title\": \"SQL Endpoing Unity Catalog Error\", \"x\": 9.373736381530762, \"y\": 4.262465000152588}, {\"index\": 5409, \"title\": \"ARR - AmericanAirlines - 2203020010003431 - Job failure with metadata.HiveException\", \"x\": 8.791879653930664, \"y\": 6.440377235412598}, {\"index\": 329, \"title\": \"Error reading streaming state file of HDFSStateStoreProvider\", \"x\": 6.238844394683838, \"y\": 2.837956428527832}, {\"index\": 4284, \"title\": \"80809 - g - ARR - PepsiCo - Cannot access ADLS\", \"x\": 9.08271598815918, \"y\": 2.5418541431427}, {\"index\": 1634, \"title\": \"2203170040008511\", \"x\": 8.794662475585938, \"y\": 2.823598623275757}, {\"index\": 299, \"title\": \"gar-for-2205170050002951 \", \"x\": 8.258160591125488, \"y\": 6.166118621826172}, {\"index\": 3552, \"title\": \"Audit Logging API error\", \"x\": 5.5262627601623535, \"y\": 4.147731781005859}, {\"index\": 4633, \"title\": \"IP address of clusters blocked by Snowflake \", \"x\": 10.304115295410156, \"y\": 3.6651978492736816}, {\"index\": 3444, \"title\": \"notebook execution failure issue\", \"x\": 7.956705093383789, \"y\": 3.7841858863830566}, {\"index\": 5413, \"title\": \"Continuous Failures on Automated Job Runs\", \"x\": 9.365994453430176, \"y\": 2.4710593223571777}, {\"index\": 366, \"title\": \"Recover databricks workspace\", \"x\": 6.598118782043457, \"y\": 4.1751837730407715}, {\"index\": 5575, \"title\": \"Unable To Launch Cluster\", \"x\": 7.945714473724365, \"y\": 1.13164222240448}, {\"index\": 2104, \"title\": \"Cluster terminated.Reason:Container launch failure\", \"x\": 8.359271049499512, \"y\": 3.533322334289551}, {\"index\": 5367, \"title\": \"Hive table is empty\", \"x\": 4.825700759887695, \"y\": 0.5231214761734009}, {\"index\": 3257, \"title\": \"User getting error while running sql query in notepad\", \"x\": 9.383135795593262, \"y\": 4.531239032745361}, {\"index\": 5456, \"title\": \"ARR | MGM | Recovery delta tables for a recovered workspace | 2203020030001991\", \"x\": 10.38266372680664, \"y\": 3.6109206676483154}, {\"index\": 123, \"title\": \"ALTER TABLE statement failing with configurationInvalid fs.azure.account.key\", \"x\": 4.985774517059326, \"y\": 0.6992594003677368}, {\"index\": 2103, \"title\": \"Databricks OOM issue when trying to save data\", \"x\": 3.9899392127990723, \"y\": 1.2939730882644653}, {\"index\": 987, \"title\": \"pyspark JDBC Tuning\", \"x\": 6.554354667663574, \"y\": 4.002909183502197}, {\"index\": 1657, \"title\": \"Rename workspace \", \"x\": 6.292764663696289, \"y\": 3.6825499534606934}, {\"index\": 2366, \"title\": \"unable to create cluster\", \"x\": 9.73383617401123, \"y\": 5.399963855743408}, {\"index\": 3631, \"title\": \"SQL endpoint does not recognize Glue metastore\", \"x\": 9.084415435791016, \"y\": 6.427011013031006}, {\"index\": 4081, \"title\": \"2203240030002239 \", \"x\": 7.425303936004639, \"y\": 2.011453151702881}, {\"index\": 2712, \"title\": \"Spark 3.2 Unable to access Hive tables with underlying Snappy format\", \"x\": 10.399898529052734, \"y\": 3.4716238975524902}, {\"index\": 1907, \"title\": \"Notebook command shows 'Running command' but nothing seems to be happening\", \"x\": 6.9816813468933105, \"y\": 0.6460717916488647}, {\"index\": 2874, \"title\": \"\\\"Photon\\\" unit price and billing\", \"x\": 6.452568531036377, \"y\": 5.14799165725708}, {\"index\": 4222, \"title\": \"ARR | SQL endpoint failing to start | 2203180040002303 | JNJ\", \"x\": 9.901702880859375, \"y\": 3.3793160915374756}, {\"index\": 5518, \"title\": \"Cluster issue\", \"x\": 5.701443672180176, \"y\": 2.390141725540161}, {\"index\": 4461, \"title\": \"ARR | Repos Folder got accidentally deleted\", \"x\": 9.230659484863281, \"y\": 4.455678939819336}, {\"index\": 3232, \"title\": \"Jobs failing with library not being installed on clusters\", \"x\": 8.651663780212402, \"y\": 3.7804059982299805}, {\"index\": 552, \"title\": \"2 schema on top of each other\", \"x\": 5.637526035308838, \"y\": 1.2756767272949219}, {\"index\": 2748, \"title\": \"Issue with Unity Catalog data resource\", \"x\": 7.436271667480469, \"y\": 5.238281726837158}, {\"index\": 2090, \"title\": \"Delta schema merge issue\", \"x\": 9.0118989944458, \"y\": 6.035801410675049}, {\"index\": 5296, \"title\": \"Partner Connect for Fivetran does not work for existing Fivetran account\", \"x\": 4.844182968139648, \"y\": 0.5738322138786316}, {\"index\": 2213, \"title\": \"dynamic table sames in SQL\", \"x\": 7.138416767120361, \"y\": 5.252503871917725}, {\"index\": 3369, \"title\": \"2203240040006427\", \"x\": 8.987449645996094, \"y\": 6.191524028778076}, {\"index\": 3847, \"title\": \"Error Displaying HTML on Databricks Runtime 9.1 LTS ML\", \"x\": 10.3781156539917, \"y\": 5.030336856842041}, {\"index\": 2665, \"title\": \"SELECT * FROM delta doesn't work on Spark 3.2\", \"x\": 9.47926139831543, \"y\": 3.747269630432129}, {\"index\": 5599, \"title\": \"ARR | 2202280060001228 | RCA | Job that was triggered by Data Factory failed due to cluster inactivity  \", \"x\": 6.485970497131348, \"y\": 1.505052924156189}, {\"index\": 1216, \"title\": \"PAT throwing 403 errors with databricks apis\", \"x\": 8.353110313415527, \"y\": 1.066457748413086}, {\"index\": 4398, \"title\": \"VPC Flow Log - ETL - Optimized write takes more than 16 min\", \"x\": 7.679298400878906, \"y\": 5.963815689086914}, {\"index\": 1497, \"title\": \"Job failing with with 'spark cluster stopped unexpectedly' error\", \"x\": 10.097244262695312, \"y\": 2.388650894165039}, {\"index\": 2174, \"title\": \"Error while referring libraries on databricks APC\", \"x\": 7.476198196411133, \"y\": 3.2613155841827393}, {\"index\": 1857, \"title\": \"[ARR][Citrix][Guidance on PBI & SQL Analytics Endpoint Connection]\", \"x\": 8.858407020568848, \"y\": 6.753044605255127}, {\"index\": 610, \"title\": \"[ARR][2204260040005094 ][Geico]Unable to set log destiation to dbfs mnt\", \"x\": 9.001677513122559, \"y\": 6.270158767700195}, {\"index\": 3891, \"title\": \"Intermittent library installation error - need to check if there was any possible outage or issue\", \"x\": 6.741296768188477, \"y\": 4.1223225593566895}, {\"index\": 5427, \"title\": \"follow up for 00133505: delta table restore is causing problem\", \"x\": 8.960979461669922, \"y\": 5.322780609130859}, {\"index\": 560, \"title\": \"[ARR] [Sev B] SR-2205110030001940 continue to #00146312 Notebook is reverted back previous version automatically\", \"x\": 4.482390403747559, \"y\": 1.781956434249878}, {\"index\": 1626, \"title\": \"Error downloading dbc archive\", \"x\": 3.97330904006958, \"y\": 1.1381642818450928}, {\"index\": 4665, \"title\": \"NullPointerException in databricks job\", \"x\": 8.303329467773438, \"y\": 1.6302635669708252}, {\"index\": 3381, \"title\": \"Unable to access our legacy workspace\", \"x\": 7.45986795425415, \"y\": 5.427934169769287}, {\"index\": 2652, \"title\": \"Unable to log into  https://riotgames.cloud.databricks.com\", \"x\": 8.907509803771973, \"y\": 6.467873573303223}, {\"index\": 717, \"title\": \"Job failure due to driver disconnected\", \"x\": 5.183923244476318, \"y\": 2.0650453567504883}, {\"index\": 1743, \"title\": \" Unable to create empty delta tables in the ADLS stoarage\", \"x\": 6.308325290679932, \"y\": 3.443897008895874}, {\"index\": 4038, \"title\": \"Availability of gp3 volumes\", \"x\": 5.827089309692383, \"y\": 1.7011574506759644}, {\"index\": 4623, \"title\": \"\\u70ba\\u4f55\\u983b\\u7e41\\u7684up / down worker\", \"x\": 9.925943374633789, \"y\": 5.361395835876465}, {\"index\": 2106, \"title\": \"Cluster temporarily unavailable\", \"x\": 6.904479503631592, \"y\": 5.4302520751953125}, {\"index\": 1666, \"title\": \"Cluster lost at least one node. Reason: Communication lost\", \"x\": 5.245672702789307, \"y\": 0.9599210023880005}, {\"index\": 219, \"title\": \"ARR | SevA | 2205260040003611 | Backed service unavailable in eastus2 \", \"x\": 7.523589611053467, \"y\": 1.4900392293930054}, {\"index\": 5210, \"title\": \"2203080030000885\", \"x\": 5.523464679718018, \"y\": 4.010544776916504}, {\"index\": 4434, \"title\": \"ARR-[REG:2203140030001959]Unable to start any cluster\", \"x\": 6.440613746643066, \"y\": 2.6399927139282227}, {\"index\": 5158, \"title\": \"Queries on Databricks SQL pricing calculation\", \"x\": 7.624834060668945, \"y\": 1.2207025289535522}, {\"index\": 4363, \"title\": \"2203220060000118 \", \"x\": 6.532336235046387, \"y\": 2.4225082397460938}, {\"index\": 4088, \"title\": \"Dynamic partition insert overwrite Problem\", \"x\": 6.831467628479004, \"y\": 3.4318108558654785}, {\"index\": 4092, \"title\": \"not able to connect to external Hive metastore\", \"x\": 8.35453987121582, \"y\": 2.437709093093872}, {\"index\": 3902, \"title\": \" Facing issues in cluster mcbcnadevdbwengg01 \\u2013 library failing to load\", \"x\": 8.558002471923828, \"y\": 5.807526111602783}, {\"index\": 2635, \"title\": \"ARR - ATT : Failed to establish a new connection: [Errno 110] Connection timed out'))  : 2204110040003935 \", \"x\": 7.745657444000244, \"y\": 2.1698505878448486}, {\"index\": 931, \"title\": \" SQL endpoint cluster failing to start in PROD\", \"x\": 5.97245454788208, \"y\": 3.7971179485321045}, {\"index\": 4543, \"title\": \"connecting to minio AND dxdl via custom broker \", \"x\": 7.608565807342529, \"y\": 2.4188413619995117}, {\"index\": 4924, \"title\": \"2203110040001197 | Job failure\", \"x\": 9.989058494567871, \"y\": 2.2203216552734375}, {\"index\": 1247, \"title\": \"Driver is up but is not responsive errors in high concurrency clusters\", \"x\": 5.363314151763916, \"y\": 1.1012686491012573}, {\"index\": 5065, \"title\": \"ARR - Test Consent to False\", \"x\": 9.794469833374023, \"y\": 6.736371040344238}, {\"index\": 100, \"title\": \"Contact Details for Contingency Plan  Documentation\", \"x\": 6.622944355010986, \"y\": 2.7361063957214355}, {\"index\": 4985, \"title\": \"issue running vacuum on delta table\", \"x\": 5.759413719177246, \"y\": 4.590945243835449}, {\"index\": 4903, \"title\": \"ARR- pipeline is running slow- 2111100060003775- Followup of 00121792\", \"x\": 8.437561988830566, \"y\": 6.059328556060791}, {\"index\": 4596, \"title\": \"Unable to Mount QA Storage to Databricks\", \"x\": 7.896226406097412, \"y\": 1.0635069608688354}, {\"index\": 2105, \"title\": \"Dataframe column datatype is differenct for 2.4 & 3.0\", \"x\": 8.11233901977539, \"y\": 4.24727201461792}, {\"index\": 3313, \"title\": \"AND operator raising UnsupportedOperationException: dataType with LAG\", \"x\": 9.505475044250488, \"y\": 6.293920040130615}, {\"index\": 1784, \"title\": \"Lock Delta tables and give SELECT on DYNAMIC VIEWS\", \"x\": 8.75428581237793, \"y\": 3.631258249282837}, {\"index\": 2999, \"title\": \"Cannot enable audit logs for account\", \"x\": 5.961916923522949, \"y\": 5.028468608856201}, {\"index\": 3636, \"title\": \"Cluster not able to autoscale\", \"x\": 6.223154067993164, \"y\": 2.992435932159424}, {\"index\": 1793, \"title\": \"Unable to Debug Failing Model Inference on Large Dataset\", \"x\": 7.844138145446777, \"y\": 2.369523525238037}, {\"index\": 3817, \"title\": \"RCA required on the \\\"orphaned\\\" cluster node\", \"x\": 8.547357559204102, \"y\": 6.0597710609436035}, {\"index\": 1773, \"title\": \"How to connect to SFTP site from Azure Databricks\", \"x\": 7.5894389152526855, \"y\": 1.8186990022659302}, {\"index\": 4195, \"title\": \"Read taking way too long given data size, resulting in ExecutorLostException\", \"x\": 5.83600378036499, \"y\": 4.316578388214111}, {\"index\": 4909, \"title\": \"Users lost access to tables from Databricks SQL, they had access 2 days ago.\", \"x\": 5.96765661239624, \"y\": 4.371851444244385}, {\"index\": 1827, \"title\": \"Databricks managed table on mounted container not deleting files on DROP command\", \"x\": 10.33029842376709, \"y\": 5.036746501922607}, {\"index\": 5080, \"title\": \"ARR - Query Aggregate summation results in zeros in SQL Endpoint - 2203080010000161\", \"x\": 8.310906410217285, \"y\": 2.723428726196289}, {\"index\": 5132, \"title\": \"Unity Catalog: Quickstart Issues\", \"x\": 4.977357864379883, \"y\": 0.7299014925956726}, {\"index\": 3168, \"title\": \"[ARR] [Sev B] SR-2204070010003095-How to install init script ,  python libraries and MQ Client\", \"x\": 4.895129680633545, \"y\": 0.6708605289459229}, {\"index\": 3223, \"title\": \"Facing with an ArrayIndexOutOfBoundsException and NegativeArraySizeException errors\", \"x\": 7.343687057495117, \"y\": 5.695224761962891}, {\"index\": 5444, \"title\": \"Can't access Azure Databricks from AML\", \"x\": 6.013920783996582, \"y\": 3.232553243637085}, {\"index\": 3016, \"title\": \"ARR:SevA: Can't read xml from storage container:SR2204110040005543\", \"x\": 7.898935794830322, \"y\": 4.563533306121826}, {\"index\": 3929, \"title\": \"Trying to implement ip lockdown.\", \"x\": 5.410623550415039, \"y\": 1.2047815322875977}, {\"index\": 1246, \"title\": \"How to compute box plots charts on all rows \", \"x\": 8.761514663696289, \"y\": 3.927448034286499}, {\"index\": 5344, \"title\": \"not able to create cluster\", \"x\": 6.8116350173950195, \"y\": 2.5617563724517822}, {\"index\": 3260, \"title\": \"Delta table merge into command creates lots of files in storage | ARR SR# 2203110030000770001\", \"x\": 7.208114147186279, \"y\": 4.384073257446289}, {\"index\": 2097, \"title\": \"ACL permissions\", \"x\": 6.458797454833984, \"y\": 0.8509775996208191}, {\"index\": 3268, \"title\": \"Reg virtualenv          \", \"x\": 10.459378242492676, \"y\": 3.687833070755005}, {\"index\": 5469, \"title\": \"Unable to write to an external elasticsearch cluster\", \"x\": 7.918888568878174, \"y\": 6.37989616394043}, {\"index\": 2692, \"title\": \"Partition Pruning On Merge\", \"x\": 7.441534996032715, \"y\": 5.8299102783203125}, {\"index\": 5584, \"title\": \"Instance profile credentilas used outside of instance profile \", \"x\": 5.419552803039551, \"y\": 1.1115926504135132}, {\"index\": 4326, \"title\": \"Create private workspace failed\", \"x\": 7.465625762939453, \"y\": 1.425532579421997}, {\"index\": 372, \"title\": \"Platform Issues Faced \", \"x\": 6.485218524932861, \"y\": 4.141404151916504}, {\"index\": 3375, \"title\": \"2204050040008729 | Job Failure\", \"x\": 10.468072891235352, \"y\": 3.2169032096862793}, {\"index\": 5026, \"title\": \"This is a follow up ticket for 00136278 & 00135879\", \"x\": 7.625180244445801, \"y\": 4.600934028625488}, {\"index\": 2851, \"title\": \"GPU cluster disconnects while runnign hyperopt\", \"x\": 7.341691017150879, \"y\": 2.088310956954956}, {\"index\": 4389, \"title\": \"Databricks s3 cross-account access error\", \"x\": 9.913921356201172, \"y\": 5.390800476074219}, {\"index\": 2188, \"title\": \"Listing Queries are taking time \", \"x\": 10.372007369995117, \"y\": 3.1800477504730225}, {\"index\": 1669, \"title\": \"Requesting guidance on Token Rotation mechanism\", \"x\": 6.822078704833984, \"y\": 5.323262691497803}, {\"index\": 834, \"title\": \"Root bucket versioning\", \"x\": 8.269843101501465, \"y\": 6.377987861633301}, {\"index\": 5209, \"title\": \"Databricks job hung for several hours \", \"x\": 8.33469009399414, \"y\": 2.2835195064544678}, {\"index\": 3498, \"title\": \"Databricks-Connect write location \", \"x\": 10.329509735107422, \"y\": 3.173539638519287}, {\"index\": 1206, \"title\": \"Unable to connect ODBC Simba Spark Connector to Databricks SQL End Point\", \"x\": 9.702301025390625, \"y\": 4.644845485687256}, {\"index\": 2873, \"title\": \"CSS-ARR-SFMC-2203170040001201-ADB Cluster Hung\", \"x\": 4.923456192016602, \"y\": 0.7174651622772217}, {\"index\": 3510, \"title\": \"Follow up of the SF Ticket 00138870\", \"x\": 8.977448463439941, \"y\": 4.464838027954102}, {\"index\": 3921, \"title\": \"Cluster became unreachable during run\", \"x\": 8.253032684326172, \"y\": 5.1683430671691895}, {\"index\": 718, \"title\": \"MLFlow enable serving through code is not working\", \"x\": 7.553658962249756, \"y\": 3.169039249420166}, {\"index\": 3846, \"title\": \"SAML not working correctly\", \"x\": 6.491950988769531, \"y\": 0.7393392324447632}, {\"index\": 895, \"title\": \"https://adb-4xxx.7.azuredatabricks.net/?o=4xxxx#job/6xxxx/run/1xxxxx\", \"x\": 8.383280754089355, \"y\": 2.8549556732177734}, {\"index\": 2523, \"title\": \"9667 - g - ARR - Albertsons - Clusters not starting\", \"x\": 8.778831481933594, \"y\": 5.635090351104736}, {\"index\": 2336, \"title\": \"High Concurrency Cluster Not Terminating\", \"x\": 7.301208019256592, \"y\": 1.429534912109375}, {\"index\": 205, \"title\": \"[ARR] cannot see the deletion of clusters in Log Analytics\", \"x\": 6.67445182800293, \"y\": 3.8478262424468994}, {\"index\": 4617, \"title\": \"PySpark Tuning\", \"x\": 9.709257125854492, \"y\": 3.621716260910034}, {\"index\": 1766, \"title\": \"Issues while switching to 10.4 LTS Photon enabled cluster, inserting unidentified charachters in delta tables.\", \"x\": 7.4549784660339355, \"y\": 2.0135858058929443}, {\"index\": 4831, \"title\": \"Required metrics for Databricks Job Clusters\", \"x\": 10.576323509216309, \"y\": 3.582343578338623}, {\"index\": 1655, \"title\": \"databricks job running slower than expected\", \"x\": 10.829102516174316, \"y\": 3.3891348838806152}, {\"index\": 1922, \"title\": \"2204270030001493 \", \"x\": 5.198672294616699, \"y\": 0.9198432564735413}, {\"index\": 2116, \"title\": \"Cluster terminated - Bootstrap Timeout\", \"x\": 9.802902221679688, \"y\": 4.291634559631348}, {\"index\": 331, \"title\": \"runtime 10.4 duplicates columns where 9.1 does not \", \"x\": 6.134772777557373, \"y\": 3.0887105464935303}, {\"index\": 3023, \"title\": \"Table or View not found on Warehouse SQL Endpoint\", \"x\": 4.755616664886475, \"y\": 0.5219857692718506}, {\"index\": 3450, \"title\": \"Replacement of Account Admin\", \"x\": 7.552028656005859, \"y\": 5.238315582275391}, {\"index\": 1300, \"title\": \"Pyspark SQL Dataframe: Incorrect / Inconsistent Count Results\", \"x\": 6.948131561279297, \"y\": 0.41509246826171875}, {\"index\": 2839, \"title\": \"2204120030002479 | GIT access\", \"x\": 5.757550239562988, \"y\": 4.302321910858154}, {\"index\": 5338, \"title\": \"Cluster start issue throwing bootstrap timeout error\", \"x\": 9.088303565979004, \"y\": 4.391997337341309}, {\"index\": 5586, \"title\": \"job cluster config change does not take affect\", \"x\": 9.291825294494629, \"y\": 6.149385929107666}, {\"index\": 4989, \"title\": \"Need help in establishing connection between databricks and tableau\", \"x\": 8.719649314880371, \"y\": 6.340653419494629}, {\"index\": 244, \"title\": \"Databricks MLflow bug\", \"x\": 10.51212215423584, \"y\": 2.619053363800049}, {\"index\": 1825, \"title\": \"Delta live table interface broken after databrick's code applied to ec2 workspace \", \"x\": 9.475125312805176, \"y\": 4.090843677520752}, {\"index\": 2537, \"title\": \"2495 - g - ARR - Cummins - Failed job\", \"x\": 7.030178070068359, \"y\": 5.73743200302124}, {\"index\": 5265, \"title\": \"unable to access adlsfrom databricks\", \"x\": 7.708772659301758, \"y\": 5.939066410064697}, {\"index\": 3831, \"title\": \"how do we run instana from init script\", \"x\": 7.609334945678711, \"y\": 1.346826434135437}, {\"index\": 4086, \"title\": \"Code optimisation for slow jobs\", \"x\": 7.121056079864502, \"y\": 3.240863561630249}, {\"index\": 5172, \"title\": \"Help ingesting data from SQL Server using Windows authentication\", \"x\": 5.631772518157959, \"y\": 1.7696166038513184}, {\"index\": 2166, \"title\": \"Job cluster takes more time than interactive cluster\", \"x\": 10.104928970336914, \"y\": 4.454666614532471}, {\"index\": 2653, \"title\": \"API Failure for Library Installation \", \"x\": 9.095351219177246, \"y\": 4.332275390625}, {\"index\": 3600, \"title\": \"Getting RPCResponseTooLarge Issue\", \"x\": 8.972530364990234, \"y\": 3.145536422729492}, {\"index\": 1881, \"title\": \"Checkpoint files not being deleted using Foreachbatch\", \"x\": 6.835294723510742, \"y\": 2.6643242835998535}, {\"index\": 1373, \"title\": \"Failed to find data source: com.microsoft.sqlserver.jdbc.spark\", \"x\": 8.051275253295898, \"y\": 6.470351696014404}, {\"index\": 94, \"title\": \"Pipelines are failing with an error\", \"x\": 8.663267135620117, \"y\": 3.180764675140381}, {\"index\": 5557, \"title\": \"Performace issue with 'cssbi-platform-cluster' cluster\", \"x\": 10.675881385803223, \"y\": 4.368828296661377}, {\"index\": 2025, \"title\": \"[ARR][AIA][Storage Image Download Falure]\", \"x\": 8.151371955871582, \"y\": 2.285142421722412}, {\"index\": 57, \"title\": \"Follow up of 00142853\", \"x\": 9.141362190246582, \"y\": 3.407181978225708}, {\"index\": 4816, \"title\": \"ARR:Unable to use defaultvalue in the cluster policy definition as per doc:SR2203100040006979\", \"x\": 8.151103973388672, \"y\": 2.732828140258789}, {\"index\": 3021, \"title\": \"Retry Limit Error\", \"x\": 9.821104049682617, \"y\": 6.497628688812256}, {\"index\": 3077, \"title\": \"Customer is facing throtling issue and production jobs got halted\", \"x\": 6.635604381561279, \"y\": 2.2280964851379395}, {\"index\": 4836, \"title\": \"Databricks clusters terminated for streaming DAG's\", \"x\": 7.274819850921631, \"y\": 4.158424377441406}, {\"index\": 1448, \"title\": \"Caused by: org.rocksdb.RocksDBException: block checksum mismatch \", \"x\": 6.367283821105957, \"y\": 4.524008750915527}, {\"index\": 1132, \"title\": \"\\\"Unable to create session\\\" error\", \"x\": 5.2490997314453125, \"y\": 1.6922310590744019}, {\"index\": 3396, \"title\": \"autoloader failing with error Input marker does not start with input path\", \"x\": 9.366813659667969, \"y\": 4.620302677154541}, {\"index\": 646, \"title\": \"[ARR][Poor Performance of a Cluster]\", \"x\": 8.653098106384277, \"y\": 5.483773231506348}, {\"index\": 1028, \"title\": \"Delete orphan workspace\", \"x\": 9.757256507873535, \"y\": 4.21135139465332}, {\"index\": 5073, \"title\": \"2203090040006252 | Job Failure\", \"x\": 6.375094413757324, \"y\": 2.780942678451538}, {\"index\": 2132, \"title\": \"ARR- ATT: 2204250040008149 -  Databricks MLFlow registry cannot communicate with Storage Account\", \"x\": 9.845039367675781, \"y\": 3.376103401184082}, {\"index\": 1134, \"title\": \"2204130030000914 | ARR |  UnAuthorization error when listing databricks secret scopes\", \"x\": 5.684619426727295, \"y\": 4.519143581390381}, {\"index\": 2565, \"title\": \"Not able to provide database access in SQL endpoint\", \"x\": 7.9705586433410645, \"y\": 3.0720431804656982}, {\"index\": 1495, \"title\": \"Databricks SQL Endpoint Access/Permissions issue\", \"x\": 5.299847602844238, \"y\": 1.0253174304962158}, {\"index\": 4833, \"title\": \"Difficulty scaling Databricks permissions model\", \"x\": 4.09839391708374, \"y\": 1.4107742309570312}, {\"index\": 959, \"title\": \"Driver is not responsive\", \"x\": 8.411564826965332, \"y\": 4.467288494110107}, {\"index\": 4032, \"title\": \"How to enable comunication between third party system and AWS Databricks cluster from internet\", \"x\": 5.6959333419799805, \"y\": 2.8629207611083984}, {\"index\": 1162, \"title\": \"Not able to connect with Oracle in E2 Shard while it is working in E1\", \"x\": 7.0645527839660645, \"y\": 0.3419419229030609}, {\"index\": 2340, \"title\": \"2204210060007181 | Capacity issue\", \"x\": 5.318731784820557, \"y\": 1.0418671369552612}, {\"index\": 4095, \"title\": \"Getting v_CPU error while creating cluster\", \"x\": 9.141860961914062, \"y\": 2.3074536323547363}, {\"index\": 2992, \"title\": \"Gennie Access\", \"x\": 9.330917358398438, \"y\": 4.171291351318359}, {\"index\": 1652, \"title\": \"Issue connecting to Databricks non-prod environment via webUI\", \"x\": 5.595080852508545, \"y\": 3.8068017959594727}, {\"index\": 3466, \"title\": \"Latest  Databricks AMI not available\", \"x\": 10.09931755065918, \"y\": 3.1447412967681885}, {\"index\": 2739, \"title\": \"[ARR] SR-2204120030001695 Cluster policies not working as expected for the spark version\", \"x\": 10.091147422790527, \"y\": 5.716085433959961}, {\"index\": 1403, \"title\": \"2205060010001700 | Job Failure\", \"x\": 10.26791000366211, \"y\": 4.791394233703613}, {\"index\": 4566, \"title\": \"2203160050000289 Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1131)'))\", \"x\": 5.886717319488525, \"y\": 3.11517333984375}, {\"index\": 2142, \"title\": \"Error When Creating Cluster\", \"x\": 8.208147048950195, \"y\": 3.636359453201294}, {\"index\": 2079, \"title\": \"Access S3 using Spark\", \"x\": 8.02441120147705, \"y\": 6.043969631195068}, {\"index\": 203, \"title\": \"ARR | library loading failed | 2205260010001471 \", \"x\": 6.428090572357178, \"y\": 0.8207921385765076}, {\"index\": 2146, \"title\": \"The jobs fail when attempting to distribute this workflow into Worker Nodes\", \"x\": 9.988146781921387, \"y\": 3.3601162433624268}, {\"index\": 4327, \"title\": \"clusterAggressiveAutoscalingEnabled\", \"x\": 7.221161365509033, \"y\": 2.9079129695892334}, {\"index\": 3225, \"title\": \"CVE-2022-22965 vulnerability\", \"x\": 9.8897705078125, \"y\": 2.446643352508545}, {\"index\": 779, \"title\": \"Exception: could not open socket: ['tried to connect to ('127.0.0.1', 46229), but an error occured: [Errno 111] Connection refused']\", \"x\": 5.968760013580322, \"y\": 4.530683994293213}, {\"index\": 643, \"title\": \" Enable Delta Share ,Photon and Unity Catalog\", \"x\": 5.193817138671875, \"y\": 1.401108741760254}, {\"index\": 2402, \"title\": \"How to setup UTF-16 on Databricks Cluster ?\", \"x\": 9.676661491394043, \"y\": 2.6430811882019043}, {\"index\": 2257, \"title\": \"Lost admin access to workspace\", \"x\": 7.624251842498779, \"y\": 5.840050220489502}, {\"index\": 4213, \"title\": \"Enable audit logging in PVC\", \"x\": 7.111839771270752, \"y\": 5.274990558624268}, {\"index\": 5102, \"title\": \"Databricks issue\", \"x\": 4.060525894165039, \"y\": 1.3907309770584106}, {\"index\": 3431, \"title\": \"Folllowup on the SF ticket 00139349\", \"x\": 6.60250186920166, \"y\": 5.433851718902588}, {\"index\": 1310, \"title\": \"Opening Error\", \"x\": 9.521790504455566, \"y\": 3.581702709197998}, {\"index\": 4994, \"title\": \"permission api fails for get/put/patch\", \"x\": 5.535577297210693, \"y\": 4.319968223571777}, {\"index\": 1079, \"title\": \"Spark CSV reader is mis-interpreting date format\", \"x\": 8.768681526184082, \"y\": 4.016689300537109}, {\"index\": 4624, \"title\": \"Data bricks Mount storgate account is throwing 403 error\", \"x\": 5.399653911590576, \"y\": 1.118838906288147}, {\"index\": 2685, \"title\": \"Suggestion: Why don't you use ISO 8601 format on consoles?\", \"x\": 9.288591384887695, \"y\": 4.854002952575684}, {\"index\": 4432, \"title\": \"DBU usage logs are getting updated\", \"x\": 7.734404563903809, \"y\": 2.9054322242736816}, {\"index\": 5368, \"title\": \"2203040030000264\", \"x\": 5.892976760864258, \"y\": 4.204010963439941}, {\"index\": 1453, \"title\": \"Databricks Job failures and hanging with no metrics/logs\", \"x\": 9.552496910095215, \"y\": 2.771627187728882}, {\"index\": 5047, \"title\": \"Follow up ticket for SF 00136980\", \"x\": 7.401451110839844, \"y\": 3.90971040725708}, {\"index\": 2040, \"title\": \" 2204260040007251 Azure Grid\", \"x\": 8.2393159866333, \"y\": 2.906589984893799}, {\"index\": 5467, \"title\": \"PROD DBKS job running long and not succeeded\", \"x\": 7.303428649902344, \"y\": 1.915879487991333}, {\"index\": 1947, \"title\": \"Use of Unity Catalog External Locations\", \"x\": 8.471601486206055, \"y\": 5.07381534576416}, {\"index\": 181, \"title\": \"[ARR] [Sev B] SR- 2205260030000798 Notebook Execution taking more than the usual time\", \"x\": 7.049901485443115, \"y\": 2.037693738937378}, {\"index\": 958, \"title\": \"Ray on Databricks with error\", \"x\": 10.544928550720215, \"y\": 3.0416259765625}, {\"index\": 3248, \"title\": \"2204070030000272 \", \"x\": 7.592907428741455, \"y\": 2.8872172832489014}, {\"index\": 3837, \"title\": \"Clusters taking a long time to start/stop\", \"x\": 7.33636474609375, \"y\": 3.6554367542266846}, {\"index\": 3835, \"title\": \"High Concurrency cluster deprecated\", \"x\": 7.701189041137695, \"y\": 3.5893993377685547}, {\"index\": 2459, \"title\": \"3670587557554624 - 286d7b79-4849-4b63-a7ca-5dd839c53e86 - 2203180040004316\", \"x\": 5.279856204986572, \"y\": 0.9728171825408936}, {\"index\": 4895, \"title\": \"8185576136331840 - 66ed11d2-7a99-4f56-8bc6-20c797a75d74 - 2203080050002763\", \"x\": 7.706082820892334, \"y\": 1.7214716672897339}, {\"index\": 1625, \"title\": \"Clusters terminates on startup and init script execution because of missing libssl.so\", \"x\": 9.17037296295166, \"y\": 2.113314390182495}, {\"index\": 2318, \"title\": \"IM-ARM AKS (AKS-AUE-P-IM-01) jobs are failing\", \"x\": 8.245129585266113, \"y\": 4.939974308013916}, {\"index\": 3961, \"title\": \"2203260040000072 cluster takes 1 hour to start\", \"x\": 9.28215217590332, \"y\": 4.245116233825684}, {\"index\": 3787, \"title\": \"AutoML not working, failing with error \\\"ImportError: Numba needs NumPy 1.20 or less\\\"\", \"x\": 5.273070335388184, \"y\": 2.2803070545196533}, {\"index\": 5376, \"title\": \"Databricks jobs failing with INTERNAL Error\", \"x\": 9.162386894226074, \"y\": 4.6516313552856445}, {\"index\": 1537, \"title\": \"Case sensitivity difference in version 10.4 and 9.1\", \"x\": 9.464897155761719, \"y\": 6.648425102233887}, {\"index\": 573, \"title\": \"Executing code in Databricks interactive cluster is very slow.\", \"x\": 7.7571797370910645, \"y\": 2.2950212955474854}, {\"index\": 3680, \"title\": \"Multi task jobs with more than 100 tasks\", \"x\": 9.67733383178711, \"y\": 2.8462748527526855}, {\"index\": 3411, \"title\": \" Unable to mount the containers in Databricks\", \"x\": 10.302447319030762, \"y\": 5.545788764953613}, {\"index\": 761, \"title\": \"Databricks cluster Start Up Failure\", \"x\": 9.962321281433105, \"y\": 3.6584653854370117}, {\"index\": 2198, \"title\": \"2204220060000553\", \"x\": 6.2931809425354, \"y\": 3.6170284748077393}, {\"index\": 4714, \"title\": \"High Findings on PVC 3.60.4\", \"x\": 7.640110969543457, \"y\": 2.6964430809020996}, {\"index\": 1409, \"title\": \"2205060040003376\", \"x\": 7.4318013191223145, \"y\": 2.51822829246521}, {\"index\": 4319, \"title\": \"ARR 2203140050000737 - Unable to run job from ADF into cluster\", \"x\": 7.628377914428711, \"y\": 3.779848575592041}, {\"index\": 5370, \"title\": \"All jobs failed around 6pm MST\", \"x\": 4.740025520324707, \"y\": 0.5554702877998352}, {\"index\": 1929, \"title\": \"Databricks job failure - NPIP tunnel setup failure during launch\", \"x\": 10.059547424316406, \"y\": 3.2613472938537598}, {\"index\": 2101, \"title\": \"one node in a bad status\", \"x\": 8.19186019897461, \"y\": 6.105833530426025}, {\"index\": 190, \"title\": \"Cannot edit broken query in Databricks SQL\", \"x\": 5.448983192443848, \"y\": 2.597944736480713}, {\"index\": 4713, \"title\": \"Cannot Start Dash app in Databricks notebook\", \"x\": 9.774871826171875, \"y\": 6.8765716552734375}, {\"index\": 462, \"title\": \"driver memory consumption issue\", \"x\": 6.95053768157959, \"y\": 5.748507976531982}, {\"index\": 1413, \"title\": \"Py4JJavaError: An error occurred while calling o69119.collectToPython\", \"x\": 9.261279106140137, \"y\": 2.7972912788391113}, {\"index\": 1144, \"title\": \"Writing dataframe to delta table takes long time to complete| ARR SR# 2205110030001344\", \"x\": 9.762030601501465, \"y\": 6.864408016204834}, {\"index\": 484, \"title\": \"CSS-ARR-S500-2205180030002005-summary\\u201d column has more than 10MB of size data when we query or display in databricks we are getting following issue\", \"x\": 6.575229167938232, \"y\": 0.6695805191993713}, {\"index\": 3366, \"title\": \"Failed/successful job on two clusters\", \"x\": 7.922747611999512, \"y\": 4.56094217300415}, {\"index\": 1328, \"title\": \"Job failed with cluster termination\", \"x\": 7.100316047668457, \"y\": 5.713346004486084}, {\"index\": 101, \"title\": \"Please help to enable the Unity Catalog for customer\", \"x\": 9.042737007141113, \"y\": 4.991939544677734}, {\"index\": 1498, \"title\": \"Error while running the existing prod app on 10.4 cluster while its running fine with 6.4\", \"x\": 7.445851802825928, \"y\": 5.43243408203125}, {\"index\": 5133, \"title\": \"2203090030000311 \", \"x\": 8.75627613067627, \"y\": 3.463486433029175}, {\"index\": 3573, \"title\": \"[ARR] OSError: Errno 95 Operation not supported work around other than changing code in prod\", \"x\": 5.858997344970703, \"y\": 4.6366472244262695}, {\"index\": 4937, \"title\": \"CSS-ARR-S500-SR#2202240010000830-Arithmetic overflow error converting timestamp to data type DATE\", \"x\": 5.201074600219727, \"y\": 0.9701747894287109}, {\"index\": 2506, \"title\": \"Databricks environment highly unstable\", \"x\": 6.8061747550964355, \"y\": 0.4065246880054474}, {\"index\": 3408, \"title\": \"Cx job failing with \\\"com.databricks.NotebookExecutionException: TIMEDOUT\\\" | ARR SR# 2203290050000172\", \"x\": 5.309492588043213, \"y\": 1.6881266832351685}, {\"index\": 2472, \"title\": \"Alert emails not being sent\", \"x\": 6.626218795776367, \"y\": 3.6862850189208984}, {\"index\": 2247, \"title\": \"2204220060002803 | Azure storage\", \"x\": 10.173202514648438, \"y\": 2.1880886554718018}, {\"index\": 5070, \"title\": \"8155699648181868 - ba0678f9-6421-4c8e-9427-df0f92987854 - 2203090040005008\", \"x\": 6.427034854888916, \"y\": 0.7343297004699707}, {\"index\": 3871, \"title\": \"Jobs in Production and QA Databrciks are failing due to timeout error\", \"x\": 7.755091667175293, \"y\": 5.721714973449707}, {\"index\": 110, \"title\": \"[ARR][2205190040006377 ][Procter and Gamble]the spark driver has stopped unexpectedly and is restarting. your notebook will be automatically reattached.\", \"x\": 8.575221061706543, \"y\": 2.1199817657470703}, {\"index\": 2295, \"title\": \"2204220010000657\", \"x\": 7.908210277557373, \"y\": 1.1972239017486572}, {\"index\": 2349, \"title\": \"Okta SSO broke mysteriously\", \"x\": 6.574720859527588, \"y\": 4.88389778137207}, {\"index\": 3564, \"title\": \"Job outage on March 31\", \"x\": 7.683188438415527, \"y\": 3.6330173015594482}, {\"index\": 1463, \"title\": \"8254154577522778 - 6c05363f-2a16-4591-8cf7-5da1b8247e46 - 2205050040007471\", \"x\": 10.338191032409668, \"y\": 2.3672492504119873}, {\"index\": 1364, \"title\": \" Issue when filtering for boolean column in DBR 10.4\", \"x\": 5.925567150115967, \"y\": 4.6289472579956055}, {\"index\": 3203, \"title\": \"Issue with external table having integrity check fails when copied between prod to dev s3 buckets.\", \"x\": 7.949365615844727, \"y\": 3.7269139289855957}, {\"index\": 5569, \"title\": \"Databricks Job package install\", \"x\": 7.88565731048584, \"y\": 5.598628997802734}, {\"index\": 5087, \"title\": \"[ARR] Failing due to spark.rpc.message.maxSize has to be increased\", \"x\": 9.296487808227539, \"y\": 3.9302937984466553}, {\"index\": 232, \"title\": \"2205260030000947 GENIE\", \"x\": 9.66836166381836, \"y\": 2.7870748043060303}, {\"index\": 2256, \"title\": \" Databricks Notebook taking longer time moveing the data from ADLS to ADLS\", \"x\": 5.955721378326416, \"y\": 4.796274662017822}, {\"index\": 1651, \"title\": \"Can't create jobs with ACL\", \"x\": 8.778873443603516, \"y\": 6.5271196365356445}, {\"index\": 2089, \"title\": \"ARR - 2204220010001255 - AmericanAirlines - Job cancellation failing instead cancelling\", \"x\": 9.049481391906738, \"y\": 6.375828742980957}, {\"index\": 4838, \"title\": \"Job failing with \\\"The transaction log has failed integrity checks\\\"\", \"x\": 10.461542129516602, \"y\": 4.266743183135986}, {\"index\": 2065, \"title\": \"6649989745882060 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204260010002364\", \"x\": 8.336089134216309, \"y\": 3.9624452590942383}, {\"index\": 4258, \"title\": \"Job failure\", \"x\": 7.746631145477295, \"y\": 3.2216954231262207}, {\"index\": 3266, \"title\": \"JSON  file read issue\", \"x\": 5.688081741333008, \"y\": 4.586466312408447}, {\"index\": 3705, \"title\": \"Dataframe become empty when try to read a json\", \"x\": 8.195944786071777, \"y\": 5.770749568939209}, {\"index\": 933, \"title\": \"10.5 Regression - Exception when exceptAll with an empty DataFrame\", \"x\": 5.360630512237549, \"y\": 1.627357006072998}, {\"index\": 3731, \"title\": \"Sentry integration\", \"x\": 6.876003742218018, \"y\": 5.160988807678223}, {\"index\": 448, \"title\": \"Job Cluster Notebooks are not visible to users\", \"x\": 8.244973182678223, \"y\": 3.884188413619995}, {\"index\": 453, \"title\": \"Intermittent error \\\"Could not find ADLS Gen2 Token\\\" when running Jobs with credentials passthrough\", \"x\": 9.95168685913086, \"y\": 2.7614991664886475}, {\"index\": 1777, \"title\": \"ARR - Sev A - Databricks notebook is stuck \", \"x\": 7.68832540512085, \"y\": 3.7723660469055176}, {\"index\": 12, \"title\": \"Hitting rate limit of 100 nodes per minute.\", \"x\": 7.5197553634643555, \"y\": 4.3069000244140625}, {\"index\": 4128, \"title\": \"Account Console Invite Email Not Going Out\", \"x\": 7.001744747161865, \"y\": 0.3834269344806671}, {\"index\": 2242, \"title\": \"many jobs fail with PlacementLockTimeout: Could not acquire placement lock context\", \"x\": 8.168871879577637, \"y\": 4.440021991729736}, {\"index\": 4394, \"title\": \"Unable to use specific committers \", \"x\": 10.252795219421387, \"y\": 3.951012372970581}, {\"index\": 3092, \"title\": \"Create a VPC endpoint on the Databricks AWS account to enable GHE connectivity\", \"x\": 7.4711833000183105, \"y\": 2.523270606994629}, {\"index\": 4670, \"title\": \"Error while creating Database table in Azure Databricks\", \"x\": 9.535906791687012, \"y\": 2.4792473316192627}, {\"index\": 1988, \"title\": \"GPU cluster does not start at certain times of the day\", \"x\": 7.987046241760254, \"y\": 2.857417583465576}, {\"index\": 1021, \"title\": \"AT&T - 2205060040004996 - global-int script failed - RCA\", \"x\": 9.264107704162598, \"y\": 4.437594413757324}, {\"index\": 836, \"title\": \"Too Many Jobs \", \"x\": 8.78732967376709, \"y\": 2.7894670963287354}, {\"index\": 2779, \"title\": \"Customer cannot log MLFlow models when notebook is triggered from Azure Synapse\", \"x\": 4.934930801391602, \"y\": 0.7049681544303894}, {\"index\": 4115, \"title\": \"Performance\", \"x\": 9.596087455749512, \"y\": 4.208298683166504}, {\"index\": 5182, \"title\": \"Autoscaling Failure on Interactive Clusters\", \"x\": 8.232257843017578, \"y\": 4.25014066696167}, {\"index\": 1540, \"title\": \"Unable to connect to Azure SQL DB\", \"x\": 5.3848724365234375, \"y\": 1.1505911350250244}, {\"index\": 3249, \"title\": \"2204070050000281\", \"x\": 6.736148834228516, \"y\": 4.504853248596191}, {\"index\": 4724, \"title\": \"Cluster terminated: Slow Image Download &  Bootstrap Timeout\", \"x\": 6.989211082458496, \"y\": 2.7428736686706543}, {\"index\": 4804, \"title\": \"Private Link Enablement for new AWS Accounts\", \"x\": 7.850141525268555, \"y\": 4.359811305999756}, {\"index\": 863, \"title\": \" Unable to mount container in databricks script but are able to mount same container in databricks resource in separate resource group \", \"x\": 9.886652946472168, \"y\": 2.7134759426116943}, {\"index\": 104, \"title\": \"Cluster Terminating\", \"x\": 9.781628608703613, \"y\": 2.9100117683410645}, {\"index\": 2721, \"title\": \"Jobs API timeout\", \"x\": 8.903495788574219, \"y\": 3.8217837810516357}, {\"index\": 516, \"title\": \"JDBC Driver Documentation\", \"x\": 9.981595993041992, \"y\": 2.6355907917022705}, {\"index\": 2969, \"title\": \"Databricks cluster Nodes not getting released to pool on time.\", \"x\": 9.22485065460205, \"y\": 4.20250940322876}, {\"index\": 4825, \"title\": \"Need to get the mount point details\", \"x\": 5.7460551261901855, \"y\": 3.400890350341797}, {\"index\": 2112, \"title\": \"Not able to Install dplyr in the shard\", \"x\": 7.5676751136779785, \"y\": 1.583404541015625}, {\"index\": 5358, \"title\": \"Can't create new workspace using REST API\", \"x\": 6.591538906097412, \"y\": 2.124582290649414}, {\"index\": 3304, \"title\": \"compute resources bogging down\", \"x\": 6.9754157066345215, \"y\": 5.607390880584717}, {\"index\": 3057, \"title\": \"Installing python packages through pip fails on HC clusters with credential passthrough\", \"x\": 5.1393866539001465, \"y\": 2.00592041015625}, {\"index\": 159, \"title\": \" Connection failed to DataBricks\", \"x\": 8.797858238220215, \"y\": 4.58253288269043}, {\"index\": 3296, \"title\": \"Python code generates stdout overflow\", \"x\": 5.985889911651611, \"y\": 3.738696336746216}, {\"index\": 3806, \"title\": \"Jobs are failing with error \\\"java.lang.RuntimeException: Cannot find releaseKey from sparkVersions List\\\"\", \"x\": 8.805394172668457, \"y\": 5.000346660614014}, {\"index\": 4564, \"title\": \"Getting unauthorized error from CLI after turning on IP Access list\", \"x\": 7.9916534423828125, \"y\": 5.827699184417725}, {\"index\": 678, \"title\": \"ARR | | Databricks cluster not being created / :Azure Vm Extension Failure | 2205180050001306\", \"x\": 5.9048991203308105, \"y\": 4.355512619018555}, {\"index\": 1785, \"title\": \"After updating Cluster policy Spark environment variable, not able to update existing job\", \"x\": 8.78799819946289, \"y\": 2.9241585731506348}, {\"index\": 2977, \"title\": \"Databricks cluster is not starting. and failed\", \"x\": 8.302461624145508, \"y\": 6.071053981781006}, {\"index\": 165, \"title\": \"optimize the cost\", \"x\": 7.082280158996582, \"y\": 4.569790840148926}, {\"index\": 5184, \"title\": \"OOM issue\", \"x\": 9.09760570526123, \"y\": 2.234274387359619}, {\"index\": 4725, \"title\": \"Local Metastor in Databricks\", \"x\": 9.655637741088867, \"y\": 6.852987766265869}, {\"index\": 394, \"title\": \"Join takes seemingly forever and eventually aborts\", \"x\": 7.903359413146973, \"y\": 1.1243830919265747}, {\"index\": 2618, \"title\": \"write JSON data to tables using rest api \", \"x\": 7.972012996673584, \"y\": 3.2982394695281982}, {\"index\": 2156, \"title\": \"Error while running job\", \"x\": 5.6482462882995605, \"y\": 1.6360461711883545}, {\"index\": 3836, \"title\": \"Column showing incorrectly as empty during select \", \"x\": 7.857152462005615, \"y\": 1.8719805479049683}, {\"index\": 3451, \"title\": \"ARR - atabricks NoteBook Issues with Photon in Runtime 10.4 LTS\", \"x\": 5.720912456512451, \"y\": 4.559417247772217}, {\"index\": 4604, \"title\": \"ARR | 2203170030000772 | Databricks Subnet Exhausted Error | Safeway, Inc.\", \"x\": 8.318428039550781, \"y\": 3.8653318881988525}, {\"index\": 995, \"title\": \"Regarding the recent change in Databricks\", \"x\": 4.983931064605713, \"y\": 0.7134770750999451}, {\"index\": 1513, \"title\": \"Notebooks are taking more time to complete than average time\", \"x\": 9.380510330200195, \"y\": 4.269739627838135}, {\"index\": 5521, \"title\": \"Cx Oracle Library issue\", \"x\": 5.664981842041016, \"y\": 1.7429128885269165}, {\"index\": 2479, \"title\": \"Treating string as json\", \"x\": 8.524435043334961, \"y\": 1.8927724361419678}, {\"index\": 3074, \"title\": \"[ARR][Jobs are running for long]\", \"x\": 8.64511775970459, \"y\": 5.0680131912231445}, {\"index\": 2397, \"title\": \"Not able to store ML model\", \"x\": 8.785655975341797, \"y\": 3.494591474533081}, {\"index\": 1185, \"title\": \"MOVE dag is failing\", \"x\": 7.831948280334473, \"y\": 4.1959638595581055}, {\"index\": 3263, \"title\": \"Schema creation fails with exception 'CTLG_NAME'\", \"x\": 5.204989910125732, \"y\": 1.534437894821167}, {\"index\": 1197, \"title\": \"2205100030002026\", \"x\": 7.349641799926758, \"y\": 2.6964879035949707}, {\"index\": 5522, \"title\": \"cx is getting \\\"system error\\\" while saving to wav format\", \"x\": 5.342963695526123, \"y\": 1.2707761526107788}, {\"index\": 37, \"title\": \"File cannot inherit from folder when create table\", \"x\": 6.313002586364746, \"y\": 3.7471706867218018}, {\"index\": 5162, \"title\": \"2203070040007771\", \"x\": 5.233736038208008, \"y\": 0.9391182065010071}, {\"index\": 4097, \"title\": \"0116 - ARR - t-mobile - Unexpected results when using Cluster policies\", \"x\": 7.581709384918213, \"y\": 4.349066257476807}, {\"index\": 4111, \"title\": \"Having issues spinning up a cluster on a new E2 workspace\", \"x\": 8.82571792602539, \"y\": 2.0102527141571045}, {\"index\": 3118, \"title\": \"cannot create user: User already exists in another account\", \"x\": 6.045382022857666, \"y\": 3.524322032928467}, {\"index\": 1933, \"title\": \"Cluster terminated.Reason:Container launch failure\", \"x\": 5.796842575073242, \"y\": 2.832974910736084}, {\"index\": 4187, \"title\": \"Cluster sat at high utilization for several hours with light load\", \"x\": 9.622611045837402, \"y\": 6.635414123535156}, {\"index\": 3071, \"title\": \"SQL End point vs Cluster Performance for RShiny\", \"x\": 6.040107250213623, \"y\": 2.969902515411377}, {\"index\": 322, \"title\": \"Databricks execution taking a lot of time\", \"x\": 5.680835723876953, \"y\": 2.585012197494507}, {\"index\": 11, \"title\": \"Continuation of  00143942  | The Databricks job fails frequently - 2204250010002866\", \"x\": 4.275584697723389, \"y\": 1.387069821357727}, {\"index\": 17, \"title\": \"Upsert failing against destination table in Unity catalog\", \"x\": 8.22634506225586, \"y\": 2.450831890106201}, {\"index\": 2529, \"title\": \"databricks log check method\", \"x\": 5.749959945678711, \"y\": 2.599586009979248}, {\"index\": 4256, \"title\": \"CSS-ARR-S500-SR#2203210030000831-Need to migrate it to some other workspace\", \"x\": 7.692589282989502, \"y\": 2.9666099548339844}, {\"index\": 5446, \"title\": \"Databricks jobs not getting terminated\", \"x\": 10.585132598876953, \"y\": 3.8728675842285156}, {\"index\": 3183, \"title\": \"exception thrown initializating filesystem\", \"x\": 8.362020492553711, \"y\": 5.235833168029785}, {\"index\": 1600, \"title\": \"Issues with Delta Live APPLY CHANGES INTO\", \"x\": 4.870197772979736, \"y\": 0.6108019351959229}, {\"index\": 3786, \"title\": \"User already exists in another account\", \"x\": 9.710956573486328, \"y\": 4.140928745269775}, {\"index\": 1554, \"title\": \"Databricks cluster startup\", \"x\": 6.382742404937744, \"y\": 4.208252429962158}, {\"index\": 4539, \"title\": \"ATT: Ingest1\", \"x\": 8.912758827209473, \"y\": 6.036849021911621}, {\"index\": 579, \"title\": \"SQL query runs fine on DBR 7.3 LTS but exact same query is broken on DBR 10.4 LTS\", \"x\": 10.463085174560547, \"y\": 4.676672458648682}, {\"index\": 19, \"title\": \"Databricks job ran for hours without any progress\", \"x\": 4.810088634490967, \"y\": 0.5697025656700134}, {\"index\": 3593, \"title\": \"Unable to install package from cran repo in R \", \"x\": 8.559141159057617, \"y\": 4.481902122497559}, {\"index\": 3988, \"title\": \"[ARR] [Sev B] SR-2203250030001566 need to access File Shares from Azure Data Bricks without using SAS keys \", \"x\": 4.95427131652832, \"y\": 0.7137560844421387}, {\"index\": 2539, \"title\": \"Not getting SSO login page for some environments\", \"x\": 6.333012580871582, \"y\": 4.793852806091309}, {\"index\": 2776, \"title\": \"vulnerability issues on deere-edl-isg\", \"x\": 6.946698188781738, \"y\": 2.369123935699463}, {\"index\": 1447, \"title\": \"Unable to read table using spark sql\", \"x\": 10.267352104187012, \"y\": 5.463597774505615}, {\"index\": 4805, \"title\": \"Subscription Cancellation\", \"x\": 9.52071475982666, \"y\": 6.325222015380859}, {\"index\": 4228, \"title\": \"Suppress logging during start up of clusters\", \"x\": 8.608575820922852, \"y\": 5.755651950836182}, {\"index\": 5122, \"title\": \"Encounter error when accessing CLI in databricks notebook\", \"x\": 9.787074089050293, \"y\": 2.712130308151245}, {\"index\": 2750, \"title\": \"Not able to access files in Databricks from Azure Storage\", \"x\": 7.780055046081543, \"y\": 2.570084810256958}, {\"index\": 3051, \"title\": \"ARR - Databricks interal storageAccount and redundancy level-\", \"x\": 8.247286796569824, \"y\": 5.73119592666626}, {\"index\": 494, \"title\": \"Not able to create cluster pool with DBR Light 2.4 Extended\", \"x\": 10.423709869384766, \"y\": 3.0848443508148193}, {\"index\": 3414, \"title\": \"Job cluster unable to retrieve information of a table with data from dbfs location\", \"x\": 7.285566806793213, \"y\": 5.8865742683410645}, {\"index\": 1131, \"title\": \"Need help to implement Instana JVM Startup Agent\", \"x\": 9.971970558166504, \"y\": 2.448688268661499}, {\"index\": 2007, \"title\": \" Reading an  external configuration file (application.conf) from Azure databricks\", \"x\": 9.386839866638184, \"y\": 4.221203327178955}, {\"index\": 1504, \"title\": \"Cluster bootstrap timeout\", \"x\": 5.994216442108154, \"y\": 4.172858238220215}, {\"index\": 1377, \"title\": \"Prolonged execution time for PySpark SQL jobs in Azure databricks\", \"x\": 10.392970085144043, \"y\": 3.956643581390381}, {\"index\": 1518, \"title\": \"Cluster (10.4) shuts off randomly\", \"x\": 5.407555103302002, \"y\": 1.2039000988006592}, {\"index\": 4437, \"title\": \"The output of the notebook is too large\", \"x\": 9.523029327392578, \"y\": 6.6686224937438965}, {\"index\": 1685, \"title\": \"2204260030001439002 \", \"x\": 8.387121200561523, \"y\": 4.4969635009765625}, {\"index\": 2711, \"title\": \"Add IP address to allow list\", \"x\": 9.988068580627441, \"y\": 5.344904899597168}, {\"index\": 3807, \"title\": \"unable to start job cluster\", \"x\": 9.110010147094727, \"y\": 4.051815032958984}, {\"index\": 5313, \"title\": \"Admin permissions getting revoked\", \"x\": 7.932177543640137, \"y\": 1.0844082832336426}, {\"index\": 4098, \"title\": \"HIPAA exemption request for public preview feature: Secret paths in environment variables\", \"x\": 8.114652633666992, \"y\": 6.125697612762451}, {\"index\": 4315, \"title\": \"2201310040000447\", \"x\": 8.70305347442627, \"y\": 3.394554376602173}, {\"index\": 324, \"title\": \"Cluster terminated.Reason:Self Bootstrap Failure\", \"x\": 7.506838321685791, \"y\": 5.504509925842285}, {\"index\": 2481, \"title\": \"Use IAM instance profile in prod AWS account\", \"x\": 6.653329372406006, \"y\": 2.7188944816589355}, {\"index\": 86, \"title\": \" Error in SQL statement: SparkException\", \"x\": 8.0377197265625, \"y\": 4.782881736755371}, {\"index\": 2583, \"title\": \"Job Failures in Prod\", \"x\": 9.774679183959961, \"y\": 2.8964028358459473}, {\"index\": 90, \"title\": \"CSS-ARR-2205300030000871-Need to get the cluster log details for the given databricks workspace\", \"x\": 7.460136890411377, \"y\": 4.027711391448975}, {\"index\": 1691, \"title\": \"Migrate Instance profile from Databricks E1 to E2 in same AWS account\", \"x\": 5.970822811126709, \"y\": 1.7774202823638916}, {\"index\": 3216, \"title\": \"Jobs are not getting canceled\", \"x\": 10.187054634094238, \"y\": 3.2656564712524414}, {\"index\": 3239, \"title\": \"ACL Clusters TCP Ports Security Query\", \"x\": 5.575375556945801, \"y\": 4.6195197105407715}, {\"index\": 480, \"title\": \"Daily workflow suddently requiring more resources\", \"x\": 10.019205093383789, \"y\": 4.8212995529174805}, {\"index\": 3276, \"title\": \"Account Network Cloud configuration unable to delete\", \"x\": 10.490450859069824, \"y\": 2.9550118446350098}, {\"index\": 157, \"title\": \"GAR for 2205260030000949 \", \"x\": 7.473058223724365, \"y\": 2.009290933609009}, {\"index\": 1457, \"title\": \"4681 - ARR - American Airlines - Cluster getting bogged by GC\", \"x\": 10.632996559143066, \"y\": 4.22689962387085}, {\"index\": 3650, \"title\": \"We would need to know exactly what kind of data are transferred to the U.S.\", \"x\": 8.717464447021484, \"y\": 5.525060653686523}, {\"index\": 5395, \"title\": \"AnalysisException: `prod_silver`.`push_btc_player_data` is not a Delta table.\", \"x\": 6.797608852386475, \"y\": 5.571568965911865}, {\"index\": 4106, \"title\": \"Specific user receives error querying against a specific DB via SQL Endpoint\", \"x\": 8.806995391845703, \"y\": 3.740097761154175}, {\"index\": 3224, \"title\": \"Access dbfs default storage path is timing out | ARR SR# 2204070050000512\", \"x\": 8.125853538513184, \"y\": 4.660269737243652}, {\"index\": 3155, \"title\": \"Spark job failure\", \"x\": 5.833990573883057, \"y\": 4.3918609619140625}, {\"index\": 1044, \"title\": \"Querying data at an external location sometimes fails\", \"x\": 9.006999015808105, \"y\": 4.5196733474731445}, {\"index\": 1296, \"title\": \"Configuration HiveServerType is not available\", \"x\": 9.038538932800293, \"y\": 4.113134384155273}, {\"index\": 3695, \"title\": \"Support for OPTIMIZE SQL command on OSS Delta Lake\", \"x\": 9.12179183959961, \"y\": 4.915452003479004}, {\"index\": 1298, \"title\": \"errors running deployment script in databricks\", \"x\": 5.336676597595215, \"y\": 1.170025110244751}, {\"index\": 3889, \"title\": \"Databricks COPY INTO pattern regex - more info required\", \"x\": 7.973557949066162, \"y\": 3.8857779502868652}, {\"index\": 2228, \"title\": \"Spark job hang\", \"x\": 10.273987770080566, \"y\": 5.234426498413086}, {\"index\": 5283, \"title\": \"Databricks Upgrade and limiting the nodes to 2 in PVC\", \"x\": 7.591186046600342, \"y\": 3.8419718742370605}, {\"index\": 2968, \"title\": \"A retriable error occurred whilea ttempting to download a result file from the cloud store but the retry limi had been exceed\", \"x\": 9.164469718933105, \"y\": 3.2833354473114014}, {\"index\": 1897, \"title\": \"Audit log E1 account\", \"x\": 8.151368141174316, \"y\": 1.5557926893234253}, {\"index\": 1753, \"title\": \"ARR | 2204280030000731 - Delta lake ingestion from spark structured streaming is taking more time\", \"x\": 7.613548278808594, \"y\": 0.9361891746520996}, {\"index\": 3, \"title\": \"Unable to execute Notebook commands\", \"x\": 5.941588401794434, \"y\": 3.8327877521514893}, {\"index\": 230, \"title\": \"Notebook execution failure\", \"x\": 9.07448959350586, \"y\": 3.7263734340667725}, {\"index\": 2223, \"title\": \"6398507633548715 - e33df5d1-ae22-417d-b794-8d9b6f338409 - 2204230040000595\", \"x\": 9.915842056274414, \"y\": 3.6324005126953125}, {\"index\": 2611, \"title\": \"unable to find audit logs- overwatch\", \"x\": 7.55797004699707, \"y\": 3.0440001487731934}, {\"index\": 1004, \"title\": \"5641 - g - ARR - MolinaHealthCare - Search function not working in Databricks SQL for filter on data sources.\", \"x\": 5.809676647186279, \"y\": 4.6999664306640625}, {\"index\": 5012, \"title\": \"Convert to delta - error\", \"x\": 8.64044189453125, \"y\": 1.7632319927215576}, {\"index\": 3499, \"title\": \"Cluster Configuration changed with new Hardware Types\", \"x\": 5.894555568695068, \"y\": 2.53494930267334}, {\"index\": 965, \"title\": \"job cluster time out\", \"x\": 6.572858810424805, \"y\": 2.6735336780548096}, {\"index\": 1326, \"title\": \"Databricks AMI updates\", \"x\": 8.15584659576416, \"y\": 4.848965167999268}, {\"index\": 1113, \"title\": \"Unable to Drop Database because S3 bucket is no longer there\", \"x\": 6.279742240905762, \"y\": 3.557370662689209}, {\"index\": 3479, \"title\": \"Databricks PVC PROD upgrade to 3.60 support\", \"x\": 6.3288798332214355, \"y\": 4.191291809082031}, {\"index\": 4570, \"title\": \"ARR Customer: Cummins - Customer uses reserved Databricks DBU instances which are purchased ahead of time. He has tagged objects in his workspaces but the tags are not propagating properly when they run cost calculations. \", \"x\": 6.128138065338135, \"y\": 3.8980629444122314}, {\"index\": 5388, \"title\": \"Databricks scheduled job start to fail with internal error\", \"x\": 9.168822288513184, \"y\": 3.1192355155944824}, {\"index\": 5519, \"title\": \"Databricks Job taking longer time to execute than earlier\", \"x\": 6.596243381500244, \"y\": 1.7013869285583496}, {\"index\": 4185, \"title\": \"Scopes reach max limit\", \"x\": 7.00332498550415, \"y\": 5.783015251159668}, {\"index\": 2241, \"title\": \"OCR cluster taking too long to launch after initiaing the cluster--2204220040005758 \", \"x\": 9.939763069152832, \"y\": 4.779437065124512}, {\"index\": 5248, \"title\": \"Repos API Github Enterprise PAT not working\", \"x\": 8.558359146118164, \"y\": 3.054243803024292}, {\"index\": 3930, \"title\": \"ARR:Hit error (file not found) using High Concurrency cluster, but works fine with pass through cluster: SR 2203280030003944\", \"x\": 6.613675117492676, \"y\": 3.838878631591797}, {\"index\": 2300, \"title\": \"ARR - Failure to Launch Databricks fro ADF - SubscriptionRequestsThrottled exceeded the backend storage limit\", \"x\": 7.931830883026123, \"y\": 2.3045740127563477}, {\"index\": 4546, \"title\": \"Photon ran out of memory error while switching from DBFS to ADLS\", \"x\": 7.259299278259277, \"y\": 3.1734330654144287}, {\"index\": 4006, \"title\": \"The tables are not working with new cluster in databricks\", \"x\": 6.663501262664795, \"y\": 3.090839385986328}, {\"index\": 2358, \"title\": \"Issue trying to run Databricks Dash application on Databricks Cluster\", \"x\": 8.151796340942383, \"y\": 6.303364276885986}, {\"index\": 1039, \"title\": \"Job clusters REST API\", \"x\": 8.11905288696289, \"y\": 3.1937804222106934}, {\"index\": 4155, \"title\": \"[Very URGENT]Some Folder is deleted from Prod Folder Repo\", \"x\": 6.973395347595215, \"y\": 0.6863387823104858}, {\"index\": 2043, \"title\": \"[Feature request] want to use parameters and dynamic query with SQL in an IDE (Pycharm)\", \"x\": 4.9369120597839355, \"y\": 0.767338216304779}, {\"index\": 3812, \"title\": \"Metastore is not working, can't create a DB\", \"x\": 7.7588210105896, \"y\": 2.0649633407592773}, {\"index\": 2868, \"title\": \"User not receiving email\", \"x\": 10.42768383026123, \"y\": 2.420750141143799}, {\"index\": 393, \"title\": \"Problem reading XML files\", \"x\": 10.010344505310059, \"y\": 5.648159027099609}, {\"index\": 3204, \"title\": \"2204070030000778 | Streaming job failure\", \"x\": 6.193914413452148, \"y\": 2.157905101776123}, {\"index\": 3916, \"title\": \"databricks connect to kafka connection\", \"x\": 8.870680809020996, \"y\": 4.673042297363281}, {\"index\": 551, \"title\": \"2205190050002332 Token quota exceeded (600 tokens)\", \"x\": 6.543084621429443, \"y\": 1.0781259536743164}, {\"index\": 2706, \"title\": \"ARR | Databricks insertInto is behaving differently when run from our application vs notebook\", \"x\": 6.395999908447266, \"y\": 3.210850715637207}, {\"index\": 5440, \"title\": \" Cannot connect to Azure Dev Ops using databricks repos\", \"x\": 8.934274673461914, \"y\": 2.3654820919036865}, {\"index\": 3308, \"title\": \"2203110040001197 | Job Failure\", \"x\": 9.730295181274414, \"y\": 6.707757472991943}, {\"index\": 2855, \"title\": \"CLOUD_PROVIDER_LAUNCH_FAILURE\", \"x\": 7.181289196014404, \"y\": 4.748824596405029}, {\"index\": 1381, \"title\": \"Trino Delta Connector\", \"x\": 8.124428749084473, \"y\": 4.044585227966309}, {\"index\": 1750, \"title\": \"Spark Job not progressing.\", \"x\": 5.018875598907471, \"y\": 1.580419659614563}, {\"index\": 1441, \"title\": \"one node in a bad status\", \"x\": 9.20793628692627, \"y\": 3.7472610473632812}, {\"index\": 3689, \"title\": \"ARR 2203290050001662 - Cluster gets up and terminated immediately after\", \"x\": 8.384773254394531, \"y\": 2.4095513820648193}, {\"index\": 703, \"title\": \"Azure Databricks Billing Query.\", \"x\": 8.060025215148926, \"y\": 1.8375871181488037}, {\"index\": 823, \"title\": \"Intermittent issues when interacting with Storage Account\", \"x\": 9.720568656921387, \"y\": 6.668873310089111}, {\"index\": 4965, \"title\": \"[ARR][ADOBE][2203100010002839 ]Need increase TBL_NAME column size in databricks meta store\", \"x\": 7.573390007019043, \"y\": 1.6590861082077026}, {\"index\": 5562, \"title\": \"ARR | Did not get the first delta file version: 27247 to compute Snapshot\", \"x\": 7.126832008361816, \"y\": 0.886451244354248}, {\"index\": 1865, \"title\": \"occasionally get error 'at least one column must be specified for the table'\", \"x\": 9.905442237854004, \"y\": 2.2356228828430176}, {\"index\": 3851, \"title\": \"Issue where Databricks cluster gives error when saving data to container\", \"x\": 8.010721206665039, \"y\": 1.7880339622497559}, {\"index\": 1807, \"title\": \"Production process job that doesn't finish\", \"x\": 4.829586982727051, \"y\": 0.5747359395027161}, {\"index\": 901, \"title\": \"Not able to mount storage account\", \"x\": 9.046603202819824, \"y\": 4.965235710144043}, {\"index\": 4423, \"title\": \"Docker containers and Instance Profile\", \"x\": 8.765518188476562, \"y\": 3.860511064529419}, {\"index\": 3872, \"title\": \"DataBricks queries are failing\", \"x\": 8.78158950805664, \"y\": 5.050165176391602}, {\"index\": 4959, \"title\": \"Instance was not reachable\", \"x\": 8.930590629577637, \"y\": 2.7213518619537354}, {\"index\": 2897, \"title\": \"Query access issue in Databricks SQL\", \"x\": 5.688392162322998, \"y\": 4.158483982086182}, {\"index\": 659, \"title\": \"follow-up for 00142942 , 00145606\", \"x\": 9.709197044372559, \"y\": 2.668698787689209}, {\"index\": 4113, \"title\": \"Unable to delete an endpoint in DBSQL\", \"x\": 4.924863815307617, \"y\": 0.7088336944580078}, {\"index\": 3196, \"title\": \"Problem to create Service Principals\", \"x\": 6.084502696990967, \"y\": 4.187107086181641}, {\"index\": 994, \"title\": \"Required libraries were not installed when clusters started, causing problems in running jobs on clusters,- Saved\", \"x\": 9.797564506530762, \"y\": 4.303507328033447}, {\"index\": 4537, \"title\": \"129372011637026 - 42c0910c-ba8d-4218-96f2-e8bbcfdb8dc0 - 2203040040004381\", \"x\": 8.07186222076416, \"y\": 5.906268119812012}, {\"index\": 2689, \"title\": \"Permissions for DataBricks users\", \"x\": 8.527624130249023, \"y\": 4.320838451385498}, {\"index\": 4656, \"title\": \"ATT: Ingest3\", \"x\": 8.481895446777344, \"y\": 5.083961009979248}, {\"index\": 3611, \"title\": \"Databrick job failure , Unexpected failure while waiting for the cluster to ready\", \"x\": 8.339159965515137, \"y\": 5.308708190917969}, {\"index\": 2407, \"title\": \"2204200060001175_Databricks Jobs Failed\", \"x\": 7.679592132568359, \"y\": 6.2173051834106445}, {\"index\": 5118, \"title\": \"IP Address is out of capacity in Databricks - escalate to Databricks Team\", \"x\": 5.220580577850342, \"y\": 0.9531795382499695}, {\"index\": 1126, \"title\": \"Manage Accounts\", \"x\": 7.812389373779297, \"y\": 5.720523834228516}, {\"index\": 3579, \"title\": \"Could not reach driver of cluster 1118-054402-skid528 for 120 seconds.;\", \"x\": 4.893645763397217, \"y\": 1.3191218376159668}, {\"index\": 2490, \"title\": \"ARR |  job reported time longer than sum of individual cells | 2204190030002165\", \"x\": 8.19205093383789, \"y\": 5.998925685882568}, {\"index\": 218, \"title\": \"Recovery related issue.\", \"x\": 6.978896141052246, \"y\": 5.7600860595703125}, {\"index\": 4796, \"title\": \"[ARR] SR-#2202240030000132 Getting error while using databricks runtime version 9.1 LTS and 10.0\", \"x\": 7.955509662628174, \"y\": 5.618442058563232}, {\"index\": 1351, \"title\": \"Follow up of (00143088)\", \"x\": 7.558046817779541, \"y\": 1.7217987775802612}, {\"index\": 771, \"title\": \"CSS-ARR-S500-SR#2205110050000087-Databricks jobs don't start with error CLOUD_PROVIDER_LAUNCH_FAILURE\", \"x\": 5.1238250732421875, \"y\": 1.3562458753585815}, {\"index\": 13, \"title\": \"SQL Analytics Issue\", \"x\": 8.773580551147461, \"y\": 3.2826333045959473}, {\"index\": 5496, \"title\": \"Issues reading redshift view backed by external redshift table into spark\", \"x\": 7.195300102233887, \"y\": 5.318521022796631}, {\"index\": 1160, \"title\": \"2205110060000665 \", \"x\": 6.8492655754089355, \"y\": 5.63406229019165}, {\"index\": 4495, \"title\": \"'schema = clean_schema_creator(inputFilePath, inputFileExtension)' is not working with Standard or High concurrency clusters\", \"x\": 8.697248458862305, \"y\": 3.4123787879943848}, {\"index\": 3853, \"title\": \"2203290050002343 | Job Failure\", \"x\": 6.646025657653809, \"y\": 3.1005852222442627}, {\"index\": 4421, \"title\": \"Databricks S3 Usage\", \"x\": 6.595008373260498, \"y\": 4.073965072631836}, {\"index\": 133, \"title\": \"2205180040008873 | Job URL\", \"x\": 6.560934543609619, \"y\": 4.9496169090271}, {\"index\": 5037, \"title\": \"[ARR][2203100030000155][Cluster Driver shuts down in a middle of ADF job run]\", \"x\": 9.28841781616211, \"y\": 4.300193786621094}, {\"index\": 2305, \"title\": \"Follow up of (Cluster temporarily unavailabl) \", \"x\": 7.188291072845459, \"y\": 2.098877429962158}, {\"index\": 827, \"title\": \"Inconsistent type returned by describe and show table extended for BigInt columns\", \"x\": 8.931631088256836, \"y\": 3.560051441192627}, {\"index\": 3505, \"title\": \"Dynamic Partition Pruning triggering class cast exception for a unary node inserted by cusotm optimiser\", \"x\": 5.617461204528809, \"y\": 4.472466468811035}, {\"index\": 4304, \"title\": \"Databricks connect for DBR 10.4 LTS\", \"x\": 8.181493759155273, \"y\": 2.260850667953491}, {\"index\": 1673, \"title\": \"Permission error while accessing files in the REPO\", \"x\": 6.955219268798828, \"y\": 3.0623672008514404}, {\"index\": 2951, \"title\": \"Missing log entries from Azure Data Factory\", \"x\": 9.176342964172363, \"y\": 3.2346510887145996}, {\"index\": 4017, \"title\": \"Permissions Error On View\", \"x\": 8.354427337646484, \"y\": 1.0739035606384277}, {\"index\": 1768, \"title\": \"Databricks: OutOfMemory Error\", \"x\": 8.84768009185791, \"y\": 4.907094478607178}, {\"index\": 4269, \"title\": \"How to enable custom logging on worker to s3 bucket\", \"x\": 7.5944108963012695, \"y\": 4.420312404632568}, {\"index\": 5051, \"title\": \"Questions regarding cache in sql endpoints\", \"x\": 10.310559272766113, \"y\": 5.521996021270752}, {\"index\": 1909, \"title\": \"2204260050001740 Cx is getting multiple errors: \\\"ParquetLocalityManager received split keys that do not exist\\\"\", \"x\": 8.305929183959961, \"y\": 3.4289135932922363}, {\"index\": 3488, \"title\": \"error while connecting to Databricks SQL Endpoint from Power B\", \"x\": 9.39466667175293, \"y\": 3.1765401363372803}, {\"index\": 1094, \"title\": \"Workspace user login issues after AD username changes. \", \"x\": 9.26715087890625, \"y\": 4.1760430335998535}, {\"index\": 5303, \"title\": \"databricks activity failing\", \"x\": 5.854987621307373, \"y\": 1.4297869205474854}, {\"index\": 5520, \"title\": \"Cluster issue\", \"x\": 5.840620517730713, \"y\": 3.3648858070373535}, {\"index\": 622, \"title\": \"Cluster terminated.Reason:Self Bootstrap Failure\", \"x\": 6.494359016418457, \"y\": 3.526797294616699}, {\"index\": 2209, \"title\": \" Cluster terminated. Reason: Self Bootstrap Failure\", \"x\": 10.157912254333496, \"y\": 3.874326467514038}, {\"index\": 4201, \"title\": \"Libraries are not properly uninstalled from clusters on v9.x and 10.x\", \"x\": 9.441544532775879, \"y\": 2.2331840991973877}, {\"index\": 4864, \"title\": \"REPLACE TABLE preferred over DROP TABLE + CREATE TABLE\", \"x\": 7.267704486846924, \"y\": 4.717329502105713}, {\"index\": 804, \"title\": \"2205160030000169\", \"x\": 9.622588157653809, \"y\": 4.420901775360107}, {\"index\": 1030, \"title\": \"AWS Credentials not Loading on 1st run once a Cluster is Started\", \"x\": 8.793126106262207, \"y\": 2.8474156856536865}, {\"index\": 587, \"title\": \"Need deeper understanding of Delta Live Tables\", \"x\": 7.556577205657959, \"y\": 2.358393669128418}, {\"index\": 4346, \"title\": \"Getting an error on spark.read.format('delta') command in databricks with cfx-dataquality-1 and Scala: 2.11 and Spark: 2.4.5\", \"x\": 5.956839084625244, \"y\": 4.752755165100098}, {\"index\": 1850, \"title\": \"ARR | Serv A | Adobe |Job failed due to OOM | SR: 2204290030000582\", \"x\": 4.5779876708984375, \"y\": 1.8128015995025635}, {\"index\": 2707, \"title\": \"Unable to drop hive table \", \"x\": 5.636624336242676, \"y\": 2.4864277839660645}, {\"index\": 3347, \"title\": \"unable to create databricks storage account private endpoint - Due to deny Assignment\", \"x\": 6.4985032081604, \"y\": 1.4434417486190796}, {\"index\": 593, \"title\": \"Databricks SQL page with errors, not able to load saved queries nor the UI.\", \"x\": 7.742329120635986, \"y\": 3.7226877212524414}, {\"index\": 4514, \"title\": \"need suggestion for SSO connection of auzre db\", \"x\": 5.872054576873779, \"y\": 4.274453163146973}, {\"index\": 4102, \"title\": \"HIPAA exemption request for public preview feature: IAM Passthrough\", \"x\": 4.4412312507629395, \"y\": 2.1796576976776123}, {\"index\": 2603, \"title\": \"Delta Table merge functionality introducing duplicates\", \"x\": 5.179934501647949, \"y\": 0.8721707463264465}, {\"index\": 2378, \"title\": \"Cluster failure\", \"x\": 5.907382965087891, \"y\": 2.9831457138061523}, {\"index\": 756, \"title\": \"ARR Customer Cummins: Issue -  trying to use import pandas_profiling (pandas-profiling \\u00b7 PyPI) but it is failing \", \"x\": 6.3093438148498535, \"y\": 3.2494797706604004}, {\"index\": 757, \"title\": \"Saving dataframe as managed tables on s3\", \"x\": 8.131392478942871, \"y\": 4.811101913452148}, {\"index\": 3662, \"title\": \"test case 2 - please ignore\", \"x\": 6.101146221160889, \"y\": 1.8609766960144043}, {\"index\": 599, \"title\": \"Origin of Elastic IP in account\", \"x\": 9.313394546508789, \"y\": 2.897118091583252}, {\"index\": 4023, \"title\": \"Lost admin access in our Databricks instance\", \"x\": 9.045835494995117, \"y\": 2.174990177154541}, {\"index\": 3984, \"title\": \"databriks trial period extension \", \"x\": 7.999619483947754, \"y\": 3.8447070121765137}, {\"index\": 358, \"title\": \"Workspace Down\", \"x\": 6.8079142570495605, \"y\": 0.8825072646141052}, {\"index\": 998, \"title\": \"Long running production workflow performance issue\", \"x\": 7.379307746887207, \"y\": 4.889563083648682}, {\"index\": 4322, \"title\": \"Whitelist 44.198.103.215 in databricks\", \"x\": 5.988815784454346, \"y\": 4.977349281311035}, {\"index\": 4734, \"title\": \"Data bricks Mount storgate account is throwing 403 error\", \"x\": 6.305809497833252, \"y\": 3.4293901920318604}, {\"index\": 5531, \"title\": \"databricks dbx: issues with deploying job clusters and installing python wheel on it\", \"x\": 5.88232421875, \"y\": 3.87552547454834}, {\"index\": 5487, \"title\": \"databricks sql error: Job aborted due to stage failure\", \"x\": 7.483553886413574, \"y\": 3.4679715633392334}, {\"index\": 5191, \"title\": \"REQUEST_LIMIT_EXCEEDED in Azure Databricks\", \"x\": 6.589893341064453, \"y\": 5.439261436462402}, {\"index\": 5581, \"title\": \"Databricks best practices for naming conventions\", \"x\": 7.454621315002441, \"y\": 2.1691696643829346}, {\"index\": 4265, \"title\": \"2985  - g - ARR - providence - Unable to connect to synapse from Databricks\", \"x\": 7.297352313995361, \"y\": 5.860203742980957}, {\"index\": 191, \"title\": \"Java App Fails to Establish TLS handshake\", \"x\": 8.879189491271973, \"y\": 6.373800277709961}, {\"index\": 1919, \"title\": \"2204270030000561 |  Informatica | ARR\", \"x\": 8.56805419921875, \"y\": 2.866811990737915}, {\"index\": 1734, \"title\": \"Clarification on Databricks Managed Service Encryption & Access\", \"x\": 8.46751880645752, \"y\": 4.946633338928223}, {\"index\": 5039, \"title\": \"ADB dynamic floating IP's\", \"x\": 8.25180435180664, \"y\": 6.396276950836182}, {\"index\": 2372, \"title\": \"3962936883739740 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204210040011633\", \"x\": 6.490083694458008, \"y\": 1.9294933080673218}, {\"index\": 3214, \"title\": \"All Clusters in Dev Environment Suddenly Failing with Bootstrap Timeouts\", \"x\": 9.432689666748047, \"y\": 4.52123498916626}, {\"index\": 3612, \"title\": \"Follow-up case of 00124226: MLlib Persistence fails on RT 10.1 ML\", \"x\": 5.747293949127197, \"y\": 2.7697324752807617}, {\"index\": 197, \"title\": \"Informatica job is failing in Databricks\", \"x\": 8.306710243225098, \"y\": 2.0524134635925293}, {\"index\": 4009, \"title\": \"Explore options to reduce job run time\", \"x\": 6.786586284637451, \"y\": 4.27153205871582}, {\"index\": 3397, \"title\": \"test\", \"x\": 8.770564079284668, \"y\": 3.8372387886047363}, {\"index\": 2275, \"title\": \"Databricks alert emails for job deletions - where does the name come from?\", \"x\": 6.482304096221924, \"y\": 1.0914716720581055}, {\"index\": 3329, \"title\": \"Is it possible to connect Databricks SQL with AWS Redshift DB?\", \"x\": 9.829803466796875, \"y\": 3.0112695693969727}, {\"index\": 256, \"title\": \"2205250040009237 \", \"x\": 8.357766151428223, \"y\": 1.1305298805236816}, {\"index\": 4107, \"title\": \"error reading HoodieParquetInput files\", \"x\": 9.8371000289917, \"y\": 6.870126724243164}, {\"index\": 5075, \"title\": \"List all jobs return maximum 25 jobs\", \"x\": 8.571944236755371, \"y\": 5.875748634338379}, {\"index\": 2967, \"title\": \"2204120030001230\", \"x\": 7.754362106323242, \"y\": 1.3384912014007568}, {\"index\": 2527, \"title\": \"[reopened case about 00141031, 00141027\", \"x\": 8.992205619812012, \"y\": 2.113999128341675}, {\"index\": 3958, \"title\": \"Run result unavailable: job failed with error message Unexpected failure while waiting for the cluster\", \"x\": 8.12804126739502, \"y\": 6.168490409851074}, {\"index\": 3754, \"title\": \"How to understand fewer delta requests\", \"x\": 6.16657829284668, \"y\": 0.7039538025856018}, {\"index\": 3393, \"title\": \"[ARR] [Sev B] SR-2204040030003249- Getting Error on performing Test connection from Databricks to Storage account.\", \"x\": 9.259658813476562, \"y\": 2.087239980697632}, {\"index\": 246, \"title\": \"Databricks jobs failure with Internal Python error in the inspect module.\", \"x\": 10.087860107421875, \"y\": 5.003820419311523}, {\"index\": 1213, \"title\": \"Search all queries by created by or owner name\", \"x\": 7.69498348236084, \"y\": 5.771433353424072}, {\"index\": 1037, \"title\": \"column sorting in table on dashboard\", \"x\": 9.648882865905762, \"y\": 3.0396647453308105}, {\"index\": 471, \"title\": \"ARR | 2205180050001892 | Unable to access to history of the specific delta table \", \"x\": 9.033835411071777, \"y\": 2.61179256439209}, {\"index\": 2727, \"title\": \"Could not deploy and configure databricks workspace\", \"x\": 8.216120719909668, \"y\": 4.899362087249756}, {\"index\": 759, \"title\": \"Problems with Databricks SQL\", \"x\": 6.133394241333008, \"y\": 4.6441168785095215}, {\"index\": 4309, \"title\": \"2203090040006252 | Job Failure\", \"x\": 7.623485565185547, \"y\": 2.852834939956665}, {\"index\": 1042, \"title\": \"deleted VMs continue to write data to LogAnalytics\", \"x\": 6.689700603485107, \"y\": 4.288821697235107}, {\"index\": 4029, \"title\": \"Cannot install databricks-sql-connector library\", \"x\": 7.443661689758301, \"y\": 3.8792521953582764}, {\"index\": 2159, \"title\": \"jobs fail, but no error is reported\", \"x\": 7.991719722747803, \"y\": 4.877626419067383}, {\"index\": 2363, \"title\": \"Behavior change in %sh when Arbitrary Files in Repos feature is on\", \"x\": 7.2939982414245605, \"y\": 6.116022109985352}, {\"index\": 4240, \"title\": \"Insight Dev work space performance is very low\", \"x\": 8.805882453918457, \"y\": 6.691775798797607}, {\"index\": 891, \"title\": \"Databricks SQL: Clusters are failing to launch\", \"x\": 9.476202964782715, \"y\": 2.333873987197876}, {\"index\": 273, \"title\": \"Reverse CredentialsProvider setting\", \"x\": 7.720713138580322, \"y\": 5.6684889793396}, {\"index\": 1714, \"title\": \"Accounts API Create Workspace call fails every time in new OEM Databricks account\", \"x\": 6.936429977416992, \"y\": 4.721609592437744}, {\"index\": 1941, \"title\": \" Python packag issue module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'\", \"x\": 5.6275248527526855, \"y\": 4.141147613525391}, {\"index\": 4281, \"title\": \"increase job limit \", \"x\": 6.9530253410339355, \"y\": 5.7913994789123535}, {\"index\": 3496, \"title\": \"Password is visible in logs using key vault.\", \"x\": 8.137408256530762, \"y\": 5.44392728805542}, {\"index\": 99, \"title\": \"A piece of code in one of the notebooks has been behaving erratically\", \"x\": 6.265638828277588, \"y\": 2.936307191848755}, {\"index\": 3840, \"title\": \"Failed to resolve multi-task state, please try again\", \"x\": 9.239740371704102, \"y\": 3.966240882873535}, {\"index\": 3626, \"title\": \"SQL Endpoint 404 Error\", \"x\": 8.921306610107422, \"y\": 1.86299729347229}, {\"index\": 335, \"title\": \"Jobs API downtime ?\", \"x\": 8.154977798461914, \"y\": 3.723937749862671}, {\"index\": 5470, \"title\": \"ARR - AT&T - 2203020040009426 - Streaming query failing with \\\"Could not find ADLS Gen2 token\\\"\", \"x\": 8.02434253692627, \"y\": 3.783116340637207}, {\"index\": 1121, \"title\": \"I got stuck to an error in data bricks , the error is ' Command result size exceeds limit: Exceeded 20971520 bytes (current = 20972463)'\", \"x\": 7.143505096435547, \"y\": 0.4767690598964691}, {\"index\": 2229, \"title\": \"Cannot install Cuda\", \"x\": 6.471232891082764, \"y\": 2.9994442462921143}, {\"index\": 40, \"title\": \"Intermittent Access denied while write to Amazon S3 write encrypted storage \", \"x\": 8.227232933044434, \"y\": 2.9814646244049072}, {\"index\": 2754, \"title\": \"Unable to login to workspace SAML/SSO Error\", \"x\": 7.343409061431885, \"y\": 5.1132354736328125}, {\"index\": 1499, \"title\": \"pre-populated list is not available in Data Explorer\", \"x\": 9.043909072875977, \"y\": 4.985498428344727}, {\"index\": 2234, \"title\": \"6847458555757558 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2204220040003095\", \"x\": 9.678203582763672, \"y\": 6.714200496673584}, {\"index\": 4242, \"title\": \"Reset password option missing in admin console\", \"x\": 8.008359909057617, \"y\": 6.571276664733887}, {\"index\": 2285, \"title\": \"E2 USAGE/BILLABLE LOG\", \"x\": 6.317601203918457, \"y\": 3.9530856609344482}, {\"index\": 3682, \"title\": \"2203250050002139 decimal issue\", \"x\": 9.735688209533691, \"y\": 3.7406222820281982}, {\"index\": 4295, \"title\": \"Lost worker errors causing Databricks job to never finish\", \"x\": 9.294243812561035, \"y\": 2.473982810974121}, {\"index\": 4399, \"title\": \"ARR- 2203110040003582\", \"x\": 10.632669448852539, \"y\": 3.9059536457061768}, {\"index\": 1999, \"title\": \"ADB Job Getting Stuck\", \"x\": 7.565128803253174, \"y\": 4.080609321594238}, {\"index\": 1548, \"title\": \"Job failure due to exception \\\"IllegalArgumentException\\\"\", \"x\": 8.923318862915039, \"y\": 6.42304801940918}, {\"index\": 306, \"title\": \"Data loss with applyInPandas\", \"x\": 5.289732456207275, \"y\": 2.357163906097412}, {\"index\": 1249, \"title\": \"CSS-ARR-S500-SR#2204300030000321-Databricks Job are slow.\", \"x\": 8.429937362670898, \"y\": 2.721290111541748}, {\"index\": 2445, \"title\": \"Unable to run spark.read  for .csv file from storage in vnet/npip workspace\", \"x\": 8.706164360046387, \"y\": 2.6303467750549316}, {\"index\": 2458, \"title\": \"acessing databases using Mlflow projects and databricks\", \"x\": 9.805561065673828, \"y\": 6.88353967666626}, {\"index\": 1435, \"title\": \"Unable to read table using spark sql\", \"x\": 8.683208465576172, \"y\": 1.953535795211792}, {\"index\": 1620, \"title\": \"Databricks: issue with Simba JDBC connector\", \"x\": 9.68713665008545, \"y\": 4.389684677124023}, {\"index\": 4367, \"title\": \"ARR | 2203220040000035 | java.lang.OutOfMemoryError even though increasing the cluster size\", \"x\": 10.301133155822754, \"y\": 2.8659579753875732}, {\"index\": 553, \"title\": \"We are noticing Secure groups provisioned through SCIM are missing on databricks side although no change happened on SCIM side\", \"x\": 10.201194763183594, \"y\": 5.482292652130127}, {\"index\": 4541, \"title\": \"Dropping delta table does not clean up S3 bucket folder\", \"x\": 7.50990104675293, \"y\": 4.0012311935424805}, {\"index\": 2576, \"title\": \"compute services issue\", \"x\": 10.388679504394531, \"y\": 2.4844183921813965}, {\"index\": 1253, \"title\": \"Establishing FSX Connectivity from Databricks\", \"x\": 8.760129928588867, \"y\": 4.33502721786499}, {\"index\": 1041, \"title\": \"Error when running Job\", \"x\": 9.919327735900879, \"y\": 3.2587361335754395}, {\"index\": 188, \"title\": \"Not able to login from manage account\", \"x\": 5.862502574920654, \"y\": 2.8215172290802}, {\"index\": 2545, \"title\": \"Databricks SSO login issues to shards\", \"x\": 9.0375394821167, \"y\": 4.837705135345459}, {\"index\": 4141, \"title\": \"Unable to increase cluster configuration\", \"x\": 8.28415584564209, \"y\": 3.598416328430176}, {\"index\": 5398, \"title\": \"Issue running a databricks notebook\", \"x\": 8.846973419189453, \"y\": 3.1382718086242676}, {\"index\": 3953, \"title\": \"2202210040008521\", \"x\": 10.366896629333496, \"y\": 4.057347297668457}, {\"index\": 5490, \"title\": \"Dataset refresh in Power BI Service using Databricks connection to azuredatabricks.net end point fails\", \"x\": 5.936235427856445, \"y\": 3.487039089202881}, {\"index\": 685, \"title\": \"Job appears to be not running for 25+ minutes of execution\", \"x\": 6.343979358673096, \"y\": 2.879744529724121}, {\"index\": 2150, \"title\": \"we have requirement for a globally unique, monotonically increasing sequence number generator safe from multiprocessing\", \"x\": 8.807703018188477, \"y\": 4.860946178436279}, {\"index\": 4475, \"title\": \"Job failure due to stage failure: ShuffleMapStage 14\", \"x\": 9.283839225769043, \"y\": 2.31234073638916}, {\"index\": 789, \"title\": \"2205110030000348 |  Symantec Corporation | ARR\", \"x\": 8.33856201171875, \"y\": 4.597692489624023}, {\"index\": 3135, \"title\": \"Follow up 00132204\", \"x\": 5.302210330963135, \"y\": 2.393655776977539}, {\"index\": 2243, \"title\": \"Repo Notebook fails when using dbutils.notebook.run\", \"x\": 10.4984769821167, \"y\": 3.7206945419311523}, {\"index\": 1012, \"title\": \"test\", \"x\": 5.7371320724487305, \"y\": 1.7466028928756714}, {\"index\": 4785, \"title\": \"I get an error when doing division using DirectQuery to Azure Databricks\", \"x\": 5.289053440093994, \"y\": 1.9879673719406128}, {\"index\": 3140, \"title\": \"ARR | AIA | Databricks ssl cert expire date | SR: 2204070060000837 \", \"x\": 7.51127290725708, \"y\": 5.56751012802124}, {\"index\": 3857, \"title\": \"Can't create a cluster\", \"x\": 8.613783836364746, \"y\": 3.268834352493286}, {\"index\": 2353, \"title\": \"ARR - 2204210040011633 - delta table write to storage hanging for hours with no errors in logs\", \"x\": 8.803746223449707, \"y\": 6.4149274826049805}, {\"index\": 4640, \"title\": \"ARR |  Unable to access Sample Datasets on Databricks Control Plane from DTV Databricks workspace\", \"x\": 7.677837371826172, \"y\": 4.98229455947876}, {\"index\": 5206, \"title\": \"Facing erros while reading csv from databricks\", \"x\": 6.229248523712158, \"y\": 4.404829025268555}, {\"index\": 2403, \"title\": \"workspace access | 4729415788177088\", \"x\": 8.808900833129883, \"y\": 5.483265399932861}, {\"index\": 1630, \"title\": \"Long running jobs workspace request\", \"x\": 10.393414497375488, \"y\": 3.503383159637451}, {\"index\": 1683, \"title\": \"2204260030001439001\", \"x\": 8.342072486877441, \"y\": 1.8313227891921997}, {\"index\": 2804, \"title\": \"Not able to access Data from azure DataBricks (VGDATABricks) , getting authentication error\", \"x\": 8.256762504577637, \"y\": 5.440571308135986}, {\"index\": 316, \"title\": \"Enable Unity Catalog\", \"x\": 5.529951095581055, \"y\": 4.257549285888672}, {\"index\": 1355, \"title\": \" MOVE Dag is failing \", \"x\": 8.613499641418457, \"y\": 6.1840291023254395}, {\"index\": 5241, \"title\": \"Databricks Connect 9.1.10 throws ClassNotFoundException error\", \"x\": 8.052131652832031, \"y\": 6.214836120605469}, {\"index\": 3306, \"title\": \"RStudio init script fails\", \"x\": 4.299566268920898, \"y\": 1.302225947380066}, {\"index\": 1494, \"title\": \"Production ETL job failing due to Connection refused error\", \"x\": 6.872507095336914, \"y\": 2.5600032806396484}, {\"index\": 3210, \"title\": \"ARR Customer Altria: Databricks Repos sync issues from Azure Devops\", \"x\": 9.173770904541016, \"y\": 5.980413436889648}, {\"index\": 1177, \"title\": \"2205110030000113 \", \"x\": 10.31143569946289, \"y\": 2.461825370788574}, {\"index\": 2054, \"title\": \"Databricks job failed, malfunctioning instances\", \"x\": 9.357399940490723, \"y\": 6.73878812789917}, {\"index\": 3747, \"title\": \"Undetailed error when processing data in notebook using Python\", \"x\": 6.385081768035889, \"y\": 1.9894278049468994}, {\"index\": 5214, \"title\": \"unable to access adlsfrom databricks\", \"x\": 6.603107929229736, \"y\": 4.0731706619262695}, {\"index\": 330, \"title\": \"[ARR] [Sev C] 2205180030001625-Sometimes, we are facing an SSL connect issue when importing the data to PowerBI from Azure SQL Endpoint of ADB\", \"x\": 5.871455192565918, \"y\": 4.431302070617676}, {\"index\": 2399, \"title\": \"ds-etl-dev and rx-experiement-large-driver spark interactive clusters are not able to query the Data Lake Views\", \"x\": 5.548426628112793, \"y\": 4.260022163391113}, {\"index\": 2625, \"title\": \"2204180040002686\", \"x\": 8.540472984313965, \"y\": 4.792057991027832}, {\"index\": 32, \"title\": \"ARR - Not being able to open folder structures from storage\", \"x\": 8.678215026855469, \"y\": 3.234943389892578}, {\"index\": 1736, \"title\": \"Pfizer - Databricks overwatch Cost metrics\", \"x\": 5.668593406677246, \"y\": 1.7857643365859985}, {\"index\": 2442, \"title\": \"Case by Pablo\", \"x\": 7.534245014190674, \"y\": 1.8508009910583496}, {\"index\": 3341, \"title\": \"CSS-ARR-2204050010001186-Job aborted due to stage failure\", \"x\": 5.951077461242676, \"y\": 2.0544962882995605}, {\"index\": 2064, \"title\": \"Disable the public IP address on the cluster for our AWS workspaces\", \"x\": 7.168338775634766, \"y\": 1.725528359413147}, {\"index\": 4446, \"title\": \"Followup of SF ticket 00137735\", \"x\": 7.705530166625977, \"y\": 4.85041618347168}, {\"index\": 4127, \"title\": \"Error Creating External Table Via SQL Endpoint\", \"x\": 8.633731842041016, \"y\": 4.3576154708862305}, {\"index\": 2596, \"title\": \"Databricks PVC 3.55 configuration query\", \"x\": 6.93115758895874, \"y\": 1.2044873237609863}, {\"index\": 1398, \"title\": \"ARR | MLFlow project run failure, no logs available | 2203230050002013\", \"x\": 9.04959487915039, \"y\": 4.888490200042725}, {\"index\": 4646, \"title\": \"jobs api endpoint not working\", \"x\": 9.912581443786621, \"y\": 2.888925790786743}, {\"index\": 4905, \"title\": \"2203100010002864 - ARR - AmericanAirlines - CloudFiles returning less records than expected.\", \"x\": 7.421874046325684, \"y\": 3.2023022174835205}, {\"index\": 2331, \"title\": \"Stepup Dynamo External Source\", \"x\": 6.033960342407227, \"y\": 4.097714900970459}, {\"index\": 1010, \"title\": \"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2203020010003431\", \"x\": 6.727911949157715, \"y\": 2.826478958129883}, {\"index\": 1520, \"title\": \"Connectivity issue with AWS MSK cluster from AWS Databricks\", \"x\": 7.344461917877197, \"y\": 3.5796687602996826}, {\"index\": 221, \"title\": \"How to connect to aws quicksight from databricks\", \"x\": 7.246183395385742, \"y\": 5.417732238769531}, {\"index\": 1386, \"title\": \"Cluster startup errors\", \"x\": 7.56252908706665, \"y\": 1.6992191076278687}, {\"index\": 4435, \"title\": \"2203180030000847\", \"x\": 7.871633052825928, \"y\": 1.1736773252487183}, {\"index\": 1141, \"title\": \"Azure Databricks Vnet Connectivity\", \"x\": 9.431354522705078, \"y\": 5.753750324249268}, {\"index\": 1642, \"title\": \"Terraform High Concurrency Cluster with Table ACL's on\", \"x\": 8.61377239227295, \"y\": 4.373722553253174}, {\"index\": 2742, \"title\": \"Databricks sql endpoint performance low\", \"x\": 7.498874187469482, \"y\": 4.575678825378418}, {\"index\": 5400, \"title\": \"Issue with RDD filter\", \"x\": 10.174142837524414, \"y\": 3.8809664249420166}, {\"index\": 1764, \"title\": \"ARR |  T-Mobile USA | Job stuck at CREATE DF | SR: 2205020040000453 \", \"x\": 10.18934154510498, \"y\": 4.719326972961426}, {\"index\": 1534, \"title\": \"Databricks Issue\", \"x\": 8.162293434143066, \"y\": 4.2660417556762695}, {\"index\": 1380, \"title\": \"databricks is holding this process for a long time without marking to success\", \"x\": 6.40101432800293, \"y\": 4.079482078552246}, {\"index\": 2728, \"title\": \"ARR | Databricks spark ui show completed jobs as active jobs\", \"x\": 8.673710823059082, \"y\": 5.562586307525635}, {\"index\": 4179, \"title\": \"please help to recovery the folder under shared\", \"x\": 7.093115329742432, \"y\": 3.5507161617279053}, {\"index\": 1925, \"title\": \"2204270030000561\", \"x\": 5.855769634246826, \"y\": 2.2259299755096436}, {\"index\": 2045, \"title\": \"Job failing with an error \", \"x\": 10.169272422790527, \"y\": 2.3693227767944336}, {\"index\": 932, \"title\": \"spark dataframe is not able to read column that contains data as #VALUE\", \"x\": 9.620455741882324, \"y\": 4.632620811462402}, {\"index\": 4730, \"title\": \"Termination of the cluster not responsive\", \"x\": 10.44245719909668, \"y\": 2.881395101547241}, {\"index\": 3218, \"title\": \"Cannot add column to delta table on DBR 9.1 LTS\", \"x\": 8.255742073059082, \"y\": 5.175823211669922}, {\"index\": 4928, \"title\": \"ARR - databricks job failing with Task failure\", \"x\": 8.45141887664795, \"y\": 4.832760810852051}, {\"index\": 3701, \"title\": \"streaming job active but not  picking up and processing new files\", \"x\": 9.666596412658691, \"y\": 3.92995023727417}, {\"index\": 3604, \"title\": \"Different configurations for same Databricks Runtime version\", \"x\": 7.622649669647217, \"y\": 2.765347957611084}, {\"index\": 3243, \"title\": \"[ARR][2204070030000183][Databricks job not able to complete as the cluster config has reached the max capacity]\", \"x\": 6.825584888458252, \"y\": 3.38718843460083}, {\"index\": 4682, \"title\": \"REPO: Allow List broken \", \"x\": 6.014292240142822, \"y\": 4.702597141265869}, {\"index\": 1140, \"title\": \"Issue triggering notebook from Airflow\", \"x\": 8.439728736877441, \"y\": 3.3493216037750244}, {\"index\": 5150, \"title\": \"POC environment is failing while running terraform code.\", \"x\": 9.799361228942871, \"y\": 6.831156253814697}, {\"index\": 2287, \"title\": \"CSS-ARR-S500-SR#2204220060001892-Metastore initialization and clsuter not attached\", \"x\": 8.926727294921875, \"y\": 4.988497734069824}, {\"index\": 710, \"title\": \"Unexpected behavior in a Notebook \", \"x\": 6.473136901855469, \"y\": 4.111630916595459}, {\"index\": 3383, \"title\": \"Azure Container Subscription Clean up\", \"x\": 9.808847427368164, \"y\": 5.448317527770996}, {\"index\": 3282, \"title\": \"Databricks Long running job stuck in pending state not able to acquire resources\", \"x\": 7.011236190795898, \"y\": 5.22066593170166}, {\"index\": 3125, \"title\": \"CSS-ARR-S500-SR#2204080060001080-Needs to recover deleted folder in databricks workspace\", \"x\": 8.489238739013672, \"y\": 5.670193672180176}, {\"index\": 3458, \"title\": \"Need to figure out job cluster and its corresponding EC2 instances id  \", \"x\": 9.570171356201172, \"y\": 3.076178550720215}, {\"index\": 2902, \"title\": \"Submitting sql notebook using Databricks api\", \"x\": 10.124368667602539, \"y\": 3.1331329345703125}, {\"index\": 3504, \"title\": \"Cluster problems\", \"x\": 8.751627922058105, \"y\": 6.309844970703125}, {\"index\": 772, \"title\": \"gar for 1958239611621368\", \"x\": 9.20925235748291, \"y\": 2.6449990272521973}, {\"index\": 3710, \"title\": \"Workspace audit logs - unexpected behaviour\", \"x\": 10.339802742004395, \"y\": 3.8338234424591064}, {\"index\": 3401, \"title\": \" advanced diagnostic information\", \"x\": 8.804262161254883, \"y\": 4.535672664642334}, {\"index\": 4728, \"title\": \"Unable to use NC compute on databricks\", \"x\": 9.073230743408203, \"y\": 3.5177412033081055}, {\"index\": 854, \"title\": \"Ganglia UI on customized clusters\", \"x\": 10.59453010559082, \"y\": 4.362975597381592}, {\"index\": 3445, \"title\": \"Manage DBFS\", \"x\": 8.328289031982422, \"y\": 6.0809783935546875}, {\"index\": 2621, \"title\": \"Not able to start cluster server\", \"x\": 5.91354513168335, \"y\": 2.9139883518218994}, {\"index\": 1392, \"title\": \"table access\", \"x\": 9.224677085876465, \"y\": 6.722092151641846}, {\"index\": 5566, \"title\": \"CPU usage metrics collected in datadog is above 100 percent\", \"x\": 6.81672477722168, \"y\": 1.9028089046478271}, {\"index\": 874, \"title\": \"Email Alerts to group email\", \"x\": 10.712404251098633, \"y\": 4.253798007965088}, {\"index\": 2778, \"title\": \"Cluster Configuration Limitations\", \"x\": 7.356263160705566, \"y\": 5.790842056274414}, {\"index\": 4037, \"title\": \"Databricks Cluster needs to be restored\", \"x\": 7.613610744476318, \"y\": 3.2501635551452637}, {\"index\": 4571, \"title\": \"Streaming query Exception : exception thrown awaitResult\", \"x\": 10.015756607055664, \"y\": 5.406331539154053}, {\"index\": 749, \"title\": \"Try detaching and re-attaching the notebook\", \"x\": 8.963955879211426, \"y\": 4.579404354095459}, {\"index\": 4259, \"title\": \"weird Spark behaviour\", \"x\": 6.123420238494873, \"y\": 4.103912353515625}, {\"index\": 3712, \"title\": \"Communication lost with driver\", \"x\": 9.736194610595703, \"y\": 2.554704427719116}, {\"index\": 83, \"title\": \"UnityCatalog\\u30de\\u30cd\\u30fc\\u30b8\\u30c9\\u30c6\\u30fc\\u30d6\\u30eb\\u306b\\u5229\\u7528\\u3059\\u308bS3\\u306e\\u8a2d\\u5b9a\\u306b\\u3064\\u3044\\u3066\", \"x\": 7.747001647949219, \"y\": 2.3180394172668457}, {\"index\": 4695, \"title\": \"Performance issues\", \"x\": 9.890469551086426, \"y\": 6.628281593322754}, {\"index\": 128, \"title\": \"guidance about security vulnerability scanning\", \"x\": 10.34631061553955, \"y\": 4.483031749725342}, {\"index\": 1576, \"title\": \"Identity Column is generating Duplicates\", \"x\": 6.97968053817749, \"y\": 4.275604724884033}, {\"index\": 2878, \"title\": \"Jobs Preview Requests\", \"x\": 7.931225776672363, \"y\": 3.380371332168579}, {\"index\": 885, \"title\": \"Modifying Subnet of Databricks Workspace\", \"x\": 6.809638500213623, \"y\": 5.432802200317383}, {\"index\": 1431, \"title\": \"Jobs failing with \\\"Unexpected failure while waiting for the cluster\\\"\", \"x\": 7.133308410644531, \"y\": 5.580867767333984}, {\"index\": 134, \"title\": \"Pip in Delta Live Tables\", \"x\": 6.00571346282959, \"y\": 3.7702369689941406}, {\"index\": 3095, \"title\": \" Storage access inconsistent\", \"x\": 8.135622024536133, \"y\": 3.438274621963501}, {\"index\": 2513, \"title\": \"2204190040001460 \", \"x\": 9.131561279296875, \"y\": 4.122624397277832}, {\"index\": 2908, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2202230040008781\", \"x\": 8.332904815673828, \"y\": 5.695531368255615}, {\"index\": 3493, \"title\": \"Rapids on single node\", \"x\": 5.757933616638184, \"y\": 2.6572766304016113}, {\"index\": 2379, \"title\": \"Random users added to Account\", \"x\": 6.784889221191406, \"y\": 5.628480911254883}, {\"index\": 3228, \"title\": \"Delete older data from S3\", \"x\": 7.957390785217285, \"y\": 6.58497953414917}, {\"index\": 522, \"title\": \"Error while trying to modify table\", \"x\": 6.285511016845703, \"y\": 3.1940627098083496}, {\"index\": 2078, \"title\": \"Collabratrive Engagement with Databricks Direct: Assistance with DLT PipeLine from our Databricks Workspace.\", \"x\": 9.256133079528809, \"y\": 3.899078845977783}, {\"index\": 2075, \"title\": \"AWS account 089805728736 cannot find endpoint\", \"x\": 10.14123249053955, \"y\": 5.691967010498047}, {\"index\": 4563, \"title\": \"ARR | PAT token permission | 2203170050001235\", \"x\": 8.470580101013184, \"y\": 5.706173896789551}, {\"index\": 1508, \"title\": \"330372711545732 access\", \"x\": 8.22161865234375, \"y\": 4.757445812225342}, {\"index\": 2098, \"title\": \"'Metastore is Down' issues with external metastore\", \"x\": 7.276433944702148, \"y\": 3.9565417766571045}, {\"index\": 111, \"title\": \"7763010224164137 - b8080fe8-25bc-47b2-85e7-fd02dd3aee1e - 2205280040000814\", \"x\": 8.663021087646484, \"y\": 3.223257303237915}, {\"index\": 3646, \"title\": \"Job failing with Caused by: java.io.IOException: No space left on device\", \"x\": 6.982470989227295, \"y\": 4.952322006225586}, {\"index\": 2619, \"title\": \"1.Databricks-Log contains no information about causing column in case of type mismatch during Synapse-Write\", \"x\": 7.9955339431762695, \"y\": 3.504213809967041}, {\"index\": 2823, \"title\": \" Help investigate a failing job\", \"x\": 9.502676963806152, \"y\": 2.473958969116211}, {\"index\": 3093, \"title\": \"Databricks\", \"x\": 6.273984432220459, \"y\": 3.3654937744140625}, {\"index\": 3516, \"title\": \"All the job clusters are not starting and failing with INIT_SCRIPT_FAILURE\", \"x\": 10.370152473449707, \"y\": 2.9991955757141113}, {\"index\": 1791, \"title\": \"Trigger.AvailableNow causing significant job delay compared to Trigger.Once\", \"x\": 8.224900245666504, \"y\": 5.724365234375}, {\"index\": 2039, \"title\": \"Follow-up of  SF - 00142662\", \"x\": 7.362107276916504, \"y\": 1.698959469795227}, {\"index\": 1545, \"title\": \"cant see clone folder option\", \"x\": 6.731624603271484, \"y\": 3.68864369392395}, {\"index\": 2192, \"title\": \"v\", \"x\": 5.378645896911621, \"y\": 1.9116580486297607}, {\"index\": 3178, \"title\": \"ARR| RCA for intermittent errors in production environment - 2203210010002555\", \"x\": 7.624270439147949, \"y\": 1.62770414352417}, {\"index\": 1273, \"title\": \"Connecting from fivetran to databricks  throwing error Check version compatibility: Failed to read databricks cluster version. Error : {\\\"error_code\\\":\\\"403\\\",\\\"message\\\":\\\"Unauthorized access to Org: 3438608682691136\\\"}\", \"x\": 9.135590553283691, \"y\": 5.332040309906006}, {\"index\": 2030, \"title\": \"need Databricks clarification of the Spark conguration of \\\"spark.hadoop.fs.abfss.impl.disable.cache\\\" and the suggestions of any other configurations to optimize the performance of accessing the \", \"x\": 8.281351089477539, \"y\": 3.613900661468506}, {\"index\": 3469, \"title\": \"RE : VPC CIDR clock for https://asurion-prod-edp.cloud.databricks.com/\", \"x\": 6.385178565979004, \"y\": 0.8356684446334839}, {\"index\": 1814, \"title\": \"Problem with a unresponsive Delta Live Tables UI when using 6 notebooks in a DLT-pipeline.\", \"x\": 8.304533004760742, \"y\": 1.7780616283416748}, {\"index\": 1098, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2205110040011261\", \"x\": 10.580342292785645, \"y\": 2.8976733684539795}, {\"index\": 5015, \"title\": \"Job run with javax.net.ssl.SSLException: Tag mismatch message\", \"x\": 9.111519813537598, \"y\": 6.337687015533447}, {\"index\": 3826, \"title\": \"Clarification on where common table expressions are supported\", \"x\": 5.109574794769287, \"y\": 0.8400864005088806}, {\"index\": 3951, \"title\": \"Grant any permission on Repos but Deleting\", \"x\": 9.685827255249023, \"y\": 6.086539268493652}, {\"index\": 3508, \"title\": \"Getting java.lang.AssertionError after upgrading DBR version to 10.0\", \"x\": 5.604398250579834, \"y\": 4.404772758483887}, {\"index\": 1147, \"title\": \"unity catalog private preview feature\", \"x\": 5.807306289672852, \"y\": 2.7580626010894775}, {\"index\": 1432, \"title\": \"2205050010000980_notebook is taking longer time\", \"x\": 7.491305828094482, \"y\": 2.9676036834716797}, {\"index\": 748, \"title\": \"Issue while creating DB workspace\", \"x\": 8.26942253112793, \"y\": 4.7856597900390625}, {\"index\": 1938, \"title\": \"Request to increase total number of workspace count\", \"x\": 6.611046314239502, \"y\": 4.12156867980957}, {\"index\": 2068, \"title\": \"Configure Self-Managed VPC\", \"x\": 9.780029296875, \"y\": 4.323982238769531}, {\"index\": 910, \"title\": \"unable to install great_expectations python module\", \"x\": 9.487383842468262, \"y\": 4.037478923797607}, {\"index\": 3162, \"title\": \"ARR | 2204070010002917 | Does NotebookTask in Job API support ADLS paths?\", \"x\": 10.514273643493652, \"y\": 3.660949468612671}, {\"index\": 304, \"title\": \"worker is lost during jobs execution\", \"x\": 7.788449764251709, \"y\": 4.353903293609619}, {\"index\": 1958, \"title\": \"E2 Migration\", \"x\": 9.808305740356445, \"y\": 3.893820285797119}, {\"index\": 4230, \"title\": \"Download files from S3 bucket in Dev\", \"x\": 8.94118881225586, \"y\": 2.358215808868408}, {\"index\": 2346, \"title\": \"Stardog Metastore refresh is slow in databricks cluster\", \"x\": 9.882753372192383, \"y\": 4.531250476837158}, {\"index\": 4221, \"title\": \"Databricks internal dbfs tables migration from one workspace to another workspace in different subscription\", \"x\": 10.478519439697266, \"y\": 3.7191667556762695}, {\"index\": 2246, \"title\": \"2650234915343663 - c450f3d1-583c-495f-b5d3-0b38b99e70c0 - 2204220010001255\", \"x\": 7.499589920043945, \"y\": 1.5013962984085083}, {\"index\": 3152, \"title\": \"2204070030001149\", \"x\": 4.39408016204834, \"y\": 1.7256450653076172}, {\"index\": 3601, \"title\": \"2203290030000614\", \"x\": 5.87864351272583, \"y\": 2.927086353302002}, {\"index\": 1577, \"title\": \"ARR - SQL using a CTE that runs fine in Databricks Runtime 9.1 LTS Photon, but gives an error in 10.4 LTS Photon\", \"x\": 6.780264854431152, \"y\": 2.688089370727539}, {\"index\": 3816, \"title\": \"Databricks repos should support symlinks\", \"x\": 6.608912944793701, \"y\": 2.588165044784546}, {\"index\": 3753, \"title\": \"Grant access to private workspace preview feature\", \"x\": 9.009804725646973, \"y\": 3.3289170265197754}, {\"index\": 3525, \"title\": \"4031723673524963 - f42342d1-2d01-49b5-beee-364183ed8af5 - 2204020040000225\", \"x\": 8.160321235656738, \"y\": 1.9631398916244507}, {\"index\": 107, \"title\": \"Job email notification alert not sending to specific email address\", \"x\": 9.642650604248047, \"y\": 4.782470703125}, {\"index\": 2733, \"title\": \"faced error when running 3-table join Spark SQL\", \"x\": 6.630142688751221, \"y\": 1.736755609512329}, {\"index\": 3808, \"title\": \"Job clusters fail to start\", \"x\": 7.263922214508057, \"y\": 3.2728357315063477}, {\"index\": 120, \"title\": \"Unable to access Jobs API 2.1 - 401 Access Denied\", \"x\": 7.932129383087158, \"y\": 6.674214839935303}, {\"index\": 4193, \"title\": \"ARR - Canadian Tire - 2203040040004381 - Connectivity to AzureSQL failing with cert error\", \"x\": 6.534297943115234, \"y\": 2.0118186473846436}, {\"index\": 5117, \"title\": \"Databricks to ECS connectivity issue\", \"x\": 9.195313453674316, \"y\": 6.217807292938232}, {\"index\": 5285, \"title\": \"Unable to remove an admin user from account.databricks.com Portal\", \"x\": 9.150535583496094, \"y\": 3.421858549118042}, {\"index\": 364, \"title\": \"SAML error while setting up\", \"x\": 8.844304084777832, \"y\": 4.537426948547363}, {\"index\": 4383, \"title\": \"ARR | Nuance |  2203180040005297 | Databricks Jobs taking too long post Databricks Runtime version upgrade from 7.3 to 10.2\", \"x\": 9.826716423034668, \"y\": 5.3347697257995605}, {\"index\": 3987, \"title\": \"Metastore is down\", \"x\": 9.584534645080566, \"y\": 3.2159488201141357}, {\"index\": 1156, \"title\": \"SQL query works in DBR 9.1 but fails in DBR 10.4\", \"x\": 10.22161865234375, \"y\": 4.745697975158691}, {\"index\": 4091, \"title\": \"DBX service outage on ae-hopper, nike-metcon and ae-turing production workspaces.\", \"x\": 7.005286693572998, \"y\": 5.3275675773620605}, {\"index\": 1277, \"title\": \"Debug cluster init script\", \"x\": 5.921077728271484, \"y\": 3.2686092853546143}, {\"index\": 4988, \"title\": \"AWS secret Redactor class name (com.databricks.spark.util.RegexBasedAWSSecretKeyRedactor) could not be instantiated\", \"x\": 8.572488784790039, \"y\": 4.769497394561768}, {\"index\": 872, \"title\": \"Unity Catalog Enablment on Azure Databricks\", \"x\": 5.6492462158203125, \"y\": 3.4574472904205322}, {\"index\": 898, \"title\": \"dbutils.fs.mount error\", \"x\": 7.859349727630615, \"y\": 1.1032787561416626}, {\"index\": 4884, \"title\": \"DirectQuery query folding to Databricks producing invalid SQL\", \"x\": 7.778815269470215, \"y\": 3.285447120666504}, {\"index\": 3830, \"title\": \"Issue with databricks-connect spark-3.1.1 SNAPSHOT jars\", \"x\": 6.913857936859131, \"y\": 5.644712924957275}, {\"index\": 3675, \"title\": \"Failed to find data source\", \"x\": 7.310882091522217, \"y\": 1.3995811939239502}, {\"index\": 5211, \"title\": \"Have access to both DBFS in S3 and Glue Catalog at the same time\", \"x\": 6.537741661071777, \"y\": 2.6366100311279297}, {\"index\": 3935, \"title\": \"Error when running query on AWS PVC\", \"x\": 9.141223907470703, \"y\": 6.724155426025391}, {\"index\": 3470, \"title\": \"Jobs failing due to unavailability of DBFS service\", \"x\": 9.503473281860352, \"y\": 3.342984199523926}, {\"index\": 2773, \"title\": \"Ganglia metrics per day basis\", \"x\": 7.1802802085876465, \"y\": 0.8656445145606995}, {\"index\": 4532, \"title\": \"Under 9.1 LTS \\\"sqlContext.sql()\\\" causes scheduled jobs to be timed out\", \"x\": 9.127453804016113, \"y\": 4.785877704620361}, {\"index\": 3814, \"title\": \"fetching result slow while using databricks-sql-connector via sql endpoint\", \"x\": 7.599147796630859, \"y\": 3.204944372177124}, {\"index\": 1165, \"title\": \"CSS-ARR-2205050030001596001-Restore Databricks Workspace \", \"x\": 7.027352333068848, \"y\": 2.9434008598327637}, {\"index\": 5161, \"title\": \"Our notebook jobs training a pytorch model is getting stuck\", \"x\": 6.977617263793945, \"y\": 2.3304009437561035}, {\"index\": 4069, \"title\": \"test\", \"x\": 9.079713821411133, \"y\": 2.616283655166626}, {\"index\": 791, \"title\": \" Notebooks are taking more time to complete than average time\", \"x\": 9.089664459228516, \"y\": 4.431333541870117}, {\"index\": 2901, \"title\": \"[ARR] SR#2204120010000787 Databricks communication between Java/Scala and Python\", \"x\": 10.393388748168945, \"y\": 2.4940686225891113}, {\"index\": 4337, \"title\": \"Failed to update InstanceProfile while job cluster creating\", \"x\": 7.050350189208984, \"y\": 4.192492961883545}, {\"index\": 700, \"title\": \"2205180050000638 problems with UI, cannot load pages\", \"x\": 7.240197658538818, \"y\": 4.662735462188721}, {\"index\": 4957, \"title\": \"request to update  a  doc\", \"x\": 7.121176719665527, \"y\": 4.5558671951293945}, {\"index\": 3512, \"title\": \"tagging jobs\", \"x\": 9.87469482421875, \"y\": 2.343567371368408}, {\"index\": 5247, \"title\": \"Databricks job is running slow and intermittently failing\", \"x\": 6.639829158782959, \"y\": 2.3409082889556885}, {\"index\": 3632, \"title\": \"Delta Live Tables pip install local package\", \"x\": 9.248855590820312, \"y\": 2.597813844680786}, {\"index\": 1524, \"title\": \"The training process is idle \", \"x\": 8.454201698303223, \"y\": 3.1491076946258545}, {\"index\": 4834, \"title\": \"SQLA Failed When CIS Team Added the additional SSM Permissions to the  SQLA Role \", \"x\": 8.48595905303955, \"y\": 5.319668292999268}, {\"index\": 2377, \"title\": \"2204200040006556 | Job performance\", \"x\": 5.661739349365234, \"y\": 1.680905818939209}, {\"index\": 3759, \"title\": \"ExecutorLostFailure, Reason: Remote RPC client disassociated\", \"x\": 8.78905200958252, \"y\": 4.286629676818848}, {\"index\": 2249, \"title\": \"While using Redshift endpoints run button in DBSQL is greyed out\", \"x\": 9.938443183898926, \"y\": 3.283918619155884}, {\"index\": 5227, \"title\": \"manifest function on azure\", \"x\": 5.5601911544799805, \"y\": 4.079934597015381}, {\"index\": 1749, \"title\": \"load data from infludb in delta : looking for examples/advices\", \"x\": 5.58856201171875, \"y\": 2.6299002170562744}, {\"index\": 2670, \"title\": \"CSS ARR | 2204080030000440 - we are not able to get the logs on databricks\", \"x\": 9.436332702636719, \"y\": 2.9253265857696533}, {\"index\": 770, \"title\": \"column name with % gives an error\", \"x\": 7.080329895019531, \"y\": 5.331372261047363}, {\"index\": 5105, \"title\": \"2201250060001153 \", \"x\": 7.9330339431762695, \"y\": 5.007042407989502}, {\"index\": 2942, \"title\": \"User was not able to enter Databricks workspace\", \"x\": 7.1321539878845215, \"y\": 3.816786289215088}, {\"index\": 1587, \"title\": \"Follow up of Sf ticket 00143272\", \"x\": 9.391265869140625, \"y\": 2.7168452739715576}, {\"index\": 55, \"title\": \"job failed to retry\", \"x\": 10.462403297424316, \"y\": 4.947801113128662}, {\"index\": 115, \"title\": \"Is there any connector available for writing  to dynamodb from databricks pyspark/scala spark ? \", \"x\": 7.8383588790893555, \"y\": 5.999388694763184}, {\"index\": 3014, \"title\": \"strange UI in job list\", \"x\": 8.227845191955566, \"y\": 4.901564598083496}, {\"index\": 3247, \"title\": \"unable to run job cluster when attaching a notebook to it.\", \"x\": 10.101887702941895, \"y\": 5.727214336395264}, {\"index\": 2720, \"title\": \"follow-up case..00140794\", \"x\": 7.438731670379639, \"y\": 1.5995714664459229}, {\"index\": 2197, \"title\": \"[ARR] Cannot access external config in artifacts path with dbx\", \"x\": 10.306075096130371, \"y\": 5.181983470916748}, {\"index\": 2316, \"title\": \"Spark job was stuck for 8.9hrs\", \"x\": 9.892382621765137, \"y\": 3.073003053665161}, {\"index\": 2253, \"title\": \"256496764439556 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2204220040003095\", \"x\": 7.900702476501465, \"y\": 1.1006944179534912}, {\"index\": 4356, \"title\": \"9084 - ARR - Inspire Brands - .cache() not working as expected\", \"x\": 10.404180526733398, \"y\": 3.280977964401245}, {\"index\": 5523, \"title\": \"Delta Error while appending data to delta table\", \"x\": 6.242311477661133, \"y\": 3.8429291248321533}, {\"index\": 1729, \"title\": \"Databricks Upgrade v3.55\", \"x\": 6.274240493774414, \"y\": 3.326404571533203}, {\"index\": 1550, \"title\": \"Azure deployment in violation of the Microsoft Online Services Acceptable Use Policy\", \"x\": 10.236306190490723, \"y\": 3.2228891849517822}, {\"index\": 4983, \"title\": \"cluster provisioning never finish\", \"x\": 6.569833755493164, \"y\": 3.054492235183716}, {\"index\": 482, \"title\": \"On some occasions the count file is having more numbers than the data file\", \"x\": 8.929360389709473, \"y\": 4.666409492492676}, {\"index\": 3463, \"title\": \"Unable to read from/ write to s3 bucket\", \"x\": 8.647488594055176, \"y\": 6.212560653686523}, {\"index\": 2236, \"title\": \"Spark Job Hangs Indefinitely when encountering java.nio.file.AccessDeniedException during `insertInto()` call\", \"x\": 9.008892059326172, \"y\": 5.403494358062744}, {\"index\": 5401, \"title\": \"8573 - ARR - Ashley Furniture - Request for more info on security risk and possibly enable dbutils.secrets.get\", \"x\": 6.7220940589904785, \"y\": 0.7797386646270752}, {\"index\": 1578, \"title\": \"ARR - Ganglia metrics not loading in Linux machine\", \"x\": 7.884477138519287, \"y\": 3.6104848384857178}, {\"index\": 5351, \"title\": \"ARR | different results when running streaming and batch jobs | 2202250050001792 \", \"x\": 6.552278518676758, \"y\": 3.626753568649292}, {\"index\": 1613, \"title\": \"changing default timestamp for hive tables and databricks workspace to CET\", \"x\": 7.914559364318848, \"y\": 4.461533546447754}, {\"index\": 921, \"title\": \"Job failure\", \"x\": 8.332642555236816, \"y\": 6.250931262969971}, {\"index\": 1427, \"title\": \"Unable to start cluster\", \"x\": 8.04713249206543, \"y\": 4.898157596588135}, {\"index\": 1913, \"title\": \"2204270030000882 Databricks Blob Storage Cannot be reached lead to failure to clusters\", \"x\": 5.598262786865234, \"y\": 1.3709031343460083}, {\"index\": 5036, \"title\": \" Having multiple issues in Databricks\", \"x\": 10.829389572143555, \"y\": 3.7293825149536133}, {\"index\": 2117, \"title\": \" Databricks jobs are failing to trigger the automation account Runbooks.\", \"x\": 5.9983601570129395, \"y\": 3.0872299671173096}, {\"index\": 5350, \"title\": \"Need help on optimising of spark job with range join\", \"x\": 9.004090309143066, \"y\": 2.189364433288574}, {\"index\": 4351, \"title\": \"Follow up of   00131426\", \"x\": 7.51760196685791, \"y\": 5.588520526885986}, {\"index\": 187, \"title\": \"Ability to specify different VM instance types based by schedule\", \"x\": 10.428513526916504, \"y\": 2.922959566116333}, {\"index\": 1783, \"title\": \"[ARR][Molina Healthcare][2204270040005587 ] In SQL endpoint  table created with distinct  and group by are not resulting into records\", \"x\": 9.154019355773926, \"y\": 3.3444466590881348}, {\"index\": 3536, \"title\": \"erewewr\", \"x\": 8.885710716247559, \"y\": 3.8698267936706543}, {\"index\": 4531, \"title\": \"Spark write parquet operation is producing duplicate files \", \"x\": 7.060762882232666, \"y\": 3.8861782550811768}, {\"index\": 4999, \"title\": \"Shell| MCDR Data loads| Slow Running Job Runs\", \"x\": 8.308952331542969, \"y\": 1.7694077491760254}, {\"index\": 3315, \"title\": \"Does this set up will work on spark kafka connector?\", \"x\": 4.448319435119629, \"y\": 1.6828633546829224}, {\"index\": 1549, \"title\": \"Vulnerability report shows a few issues\", \"x\": 7.186557292938232, \"y\": 0.5079827308654785}, {\"index\": 4797, \"title\": \"Follow Up with ticket 00137350\", \"x\": 8.058856010437012, \"y\": 4.200119972229004}, {\"index\": 1963, \"title\": \"AttributeError: module 'lib' has no attribute 'x509_V_flag_cd_issuer_check\", \"x\": 6.444052219390869, \"y\": 4.6192240715026855}, {\"index\": 3500, \"title\": \"CSS ARR | Follow up case | 2203210030000403\", \"x\": 8.043764114379883, \"y\": 2.8975024223327637}, {\"index\": 2027, \"title\": \"[ARR] SR-2204260030001581 Need to restores am01-fra-dbw02 in resource group ch-corp-devtest-am01-fra-rg01\", \"x\": 8.485128402709961, \"y\": 3.587395429611206}, {\"index\": 3432, \"title\": \"[TrackingID#2203310010000891 DsPredictionV3 messages are lost while publishing to TOPIC\", \"x\": 8.797272682189941, \"y\": 6.144620895385742}, {\"index\": 1710, \"title\": \"All-Purpose Cluster Is taking very long to Terminate\", \"x\": 10.388843536376953, \"y\": 4.042469024658203}, {\"index\": 1512, \"title\": \"ARR | 2204260030001644 | Grant access control to mount point\", \"x\": 7.940388202667236, \"y\": 2.595088005065918}, {\"index\": 5257, \"title\": \"Databricks and Kepler GL\", \"x\": 7.002564430236816, \"y\": 3.1919450759887695}, {\"index\": 2644, \"title\": \"Cluster terminating with Inactivity while command in notebook is still running\", \"x\": 9.450200080871582, \"y\": 4.188575744628906}, {\"index\": 2147, \"title\": \"2204250010000575 | SPN secret\", \"x\": 7.986481666564941, \"y\": 1.8770127296447754}, {\"index\": 3832, \"title\": \"2576137007542488 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2203290040006483\", \"x\": 6.62092924118042, \"y\": 3.449124336242676}, {\"index\": 2320, \"title\": \"training notebook on single node cluster crashes after random intervals of time\", \"x\": 6.093810081481934, \"y\": 2.345414400100708}, {\"index\": 4511, \"title\": \"Connect databrick work space with azure SQL managed Instance through JDBC.\", \"x\": 7.90706729888916, \"y\": 5.592884063720703}, {\"index\": 3001, \"title\": \"Job stuck on simple read from mounted data lake\", \"x\": 6.836677074432373, \"y\": 3.638990879058838}, {\"index\": 3289, \"title\": \"ARR: Sev A:Streaming Jobs are stuck:SR 2204060040007641 \", \"x\": 10.20152759552002, \"y\": 3.883573293685913}, {\"index\": 236, \"title\": \"Data loss with applyInPandas\", \"x\": 7.079351902008057, \"y\": 4.552826404571533}, {\"index\": 2010, \"title\": \"Downtown for changing cluster node type\", \"x\": 8.272872924804688, \"y\": 4.830551624298096}, {\"index\": 1859, \"title\": \"[ARR]TrackingID#2204280040006568]Unable to Start a newly created Cluster\", \"x\": 9.790962219238281, \"y\": 3.5351665019989014}, {\"index\": 311, \"title\": \"2205230030001614\", \"x\": 7.799354553222656, \"y\": 2.66218638420105}, {\"index\": 2453, \"title\": \"AnalysisException: Undefined function: count Spark Sql\", \"x\": 9.138420104980469, \"y\": 3.924032211303711}, {\"index\": 4916, \"title\": \"ARR:Unable to use defaultvalue in the cluster policy definition as per doc:SR2203100040006979 \", \"x\": 6.604518413543701, \"y\": 4.133184909820557}, {\"index\": 3866, \"title\": \"Pipeline failed with error 'Path not found' and await time crossed\", \"x\": 6.581521034240723, \"y\": 1.6707794666290283}, {\"index\": 2497, \"title\": \"transaction control when writing to SQL DB\", \"x\": 7.850104331970215, \"y\": 2.5351409912109375}, {\"index\": 2382, \"title\": \"2204130040009267 | Cluster startup\", \"x\": 5.233668327331543, \"y\": 0.9940169453620911}, {\"index\": 3255, \"title\": \"[Airflow Task] Data of certain conditions are loaded twice on a 'specific date' delta table (insert into)\", \"x\": 6.950425624847412, \"y\": 2.8307785987854004}, {\"index\": 4136, \"title\": \"Error while using Databricks rest api with Azure ad instead of Databricks PAT\", \"x\": 7.753083229064941, \"y\": 5.8113226890563965}, {\"index\": 2688, \"title\": \"Backward compatibility issues\", \"x\": 9.609048843383789, \"y\": 6.646300792694092}, {\"index\": 3144, \"title\": \"2204080030000474\", \"x\": 8.770328521728516, \"y\": 4.1724982261657715}, {\"index\": 5131, \"title\": \"2203090060000399 \", \"x\": 6.081824779510498, \"y\": 2.779766082763672}, {\"index\": 1608, \"title\": \"[ARR][AirCanada]\", \"x\": 8.30878734588623, \"y\": 4.75464391708374}, {\"index\": 607, \"title\": \"secret scope has INVALID_STATE: Databricks could not access keyvault:\", \"x\": 8.454670906066895, \"y\": 2.4291586875915527}, {\"index\": 1444, \"title\": \"aws instances running non stop\", \"x\": 9.901371955871582, \"y\": 4.282410621643066}, {\"index\": 846, \"title\": \"Databricks memory issues\", \"x\": 9.08363151550293, \"y\": 4.587152004241943}, {\"index\": 4865, \"title\": \"htmlwidgets doesn't since March3th\", \"x\": 5.794155120849609, \"y\": 1.5958278179168701}, {\"index\": 3876, \"title\": \" R libraries are not installed (timeout)\", \"x\": 5.881592750549316, \"y\": 3.078118324279785}, {\"index\": 4668, \"title\": \" SQL endpoint query timeouts/connectionRefused errors\", \"x\": 7.920733451843262, \"y\": 1.7804051637649536}, {\"index\": 3355, \"title\": \"All Purpose cluster Slow on GCP\", \"x\": 8.467676162719727, \"y\": 6.155760288238525}, {\"index\": 1971, \"title\": \"AttributeError: module \\u2018lib\\u2019 has no attribute \\u2018X509_V_FLAG_CB_ISSUER_CHECK\\u2019\", \"x\": 8.218711853027344, \"y\": 3.1794817447662354}, {\"index\": 2389, \"title\": \"ConnectException: Connection refused  DataMessages$GetMountsV2 since peer BoundRPCClient[http://10.139.0.6:7070 with NoProxy] is DOWN\", \"x\": 5.684427738189697, \"y\": 3.716794013977051}, {\"index\": 4697, \"title\": \"Performance issues with databricks\", \"x\": 7.238887310028076, \"y\": 1.9674265384674072}, {\"index\": 5154, \"title\": \"azure databricks cluster configuration not working\", \"x\": 8.97544002532959, \"y\": 2.1569337844848633}, {\"index\": 3443, \"title\": \"Delta Lake stream fails to recover on restart\", \"x\": 10.339051246643066, \"y\": 3.7874462604522705}, {\"index\": 1304, \"title\": \"Intermittent errors while upsizing the nodes.\", \"x\": 6.813841342926025, \"y\": 1.4650535583496094}, {\"index\": 563, \"title\": \"Are GCM cipher suites enabled by default now?_Azure China_2205190010000432\", \"x\": 8.72670841217041, \"y\": 4.143476486206055}, {\"index\": 418, \"title\": \"ARR | Blackrock-OneAladdin| Restapi using access token+management token not working for workspace enabled private endpoint| SR:2205190030000177  \", \"x\": 9.57314682006836, \"y\": 4.258577346801758}, {\"index\": 3900, \"title\": \"spark job failure\", \"x\": 7.216163635253906, \"y\": 3.1138083934783936}, {\"index\": 5223, \"title\": \"0239 - g - ARR - RCI - SQL results not expected\", \"x\": 6.928678035736084, \"y\": 0.33962664008140564}, {\"index\": 2273, \"title\": \"Demonstration on using Data Bricks Rest API\", \"x\": 9.629951477050781, \"y\": 3.3067946434020996}, {\"index\": 3309, \"title\": \"Cluster did not timeout after being unable to aqquire executors\", \"x\": 8.238362312316895, \"y\": 5.929438591003418}, {\"index\": 3361, \"title\": \"RserveException: eval failed-memory issues\", \"x\": 5.136709213256836, \"y\": 0.8678101301193237}, {\"index\": 4847, \"title\": \"Cluster Taking longer time to start\", \"x\": 10.0092191696167, \"y\": 2.867284059524536}, {\"index\": 1702, \"title\": \"Failed to import git on cluster with docker image and ML Flow\", \"x\": 10.463004112243652, \"y\": 4.031954288482666}, {\"index\": 437, \"title\": \"failed integrity checks\", \"x\": 6.592548370361328, \"y\": 3.414952278137207}, {\"index\": 518, \"title\": \"Advana Databricks PVC v3.68 Upgrade Issues\", \"x\": 6.802978038787842, \"y\": 3.847770929336548}, {\"index\": 3541, \"title\": \"Dashboard and query return 500\", \"x\": 8.459625244140625, \"y\": 5.838358402252197}, {\"index\": 4621, \"title\": \"Follow-up case #00137146\", \"x\": 6.915987968444824, \"y\": 4.015564441680908}, {\"index\": 1020, \"title\": \"3337075389283370 - 6b64b59f-432d-401e-b552-d855f1e1d2e0 - 2204060040002674\", \"x\": 5.847097873687744, \"y\": 3.6968092918395996}, {\"index\": 5093, \"title\": \"2203090050001300 - RowMatrix code being stuck on DBR 9.1 but works on 7.3\", \"x\": 10.394119262695312, \"y\": 2.855397939682007}, {\"index\": 993, \"title\": \"[ARR][2203310040005788 ][Pepsico]unable to reach Azure load balancer from databricks\", \"x\": 9.986551284790039, \"y\": 2.284172773361206}, {\"index\": 2581, \"title\": \"Jobs failing and cluster is being terminated\", \"x\": 5.891925811767578, \"y\": 3.069061279296875}, {\"index\": 4930, \"title\": \"Databricks SQL api\", \"x\": 6.289124965667725, \"y\": 4.5426926612854}, {\"index\": 3012, \"title\": \"any command line or api to replace jar file name in the job?\", \"x\": 8.719518661499023, \"y\": 4.119634628295898}, {\"index\": 4464, \"title\": \"Unable to launch cluster\", \"x\": 8.403901100158691, \"y\": 4.742152214050293}, {\"index\": 3770, \"title\": \"VACUUM fails with OutOfMemory\", \"x\": 7.661098003387451, \"y\": 6.127797603607178}, {\"index\": 1414, \"title\": \"Follow up of  (00141972)Significant performance degradation when upgrating from 9.1 ML LTS to 10.4 ML LTS. 'Process Test Performance\", \"x\": 9.868764877319336, \"y\": 2.2570221424102783}, {\"index\": 3227, \"title\": \"2204070030001149\", \"x\": 9.70389175415039, \"y\": 6.812186241149902}, {\"index\": 5406, \"title\": \"ARR | 2202090040005368 | Follow up to 00134388\", \"x\": 7.296720027923584, \"y\": 2.1264328956604004}, {\"index\": 4733, \"title\": \"intermittent error while running sales denom aggregation query\", \"x\": 10.318618774414062, \"y\": 3.558276891708374}, {\"index\": 2541, \"title\": \"Not able to login using SSO\", \"x\": 8.879549026489258, \"y\": 4.038143157958984}, {\"index\": 5237, \"title\": \"Assistance connecting VS Code to Databricks using DB Connect\", \"x\": 6.313693523406982, \"y\": 1.6301238536834717}, {\"index\": 2457, \"title\": \"Airflow DAGs not working when VPN is connected\", \"x\": 8.962096214294434, \"y\": 1.8717095851898193}, {\"index\": 534, \"title\": \"Need to run DBInsight forjobs however it always fails\", \"x\": 6.150347709655762, \"y\": 3.2333312034606934}, {\"index\": 821, \"title\": \"DLT import custom python module\", \"x\": 6.3293256759643555, \"y\": 3.4266912937164307}, {\"index\": 1571, \"title\": \"describe schema EXTENDED shows root as owner after changing the owner\", \"x\": 9.544880867004395, \"y\": 2.778831958770752}, {\"index\": 4331, \"title\": \"No SAS process attached. SAS process has terminated unexpectedly\", \"x\": 9.268704414367676, \"y\": 6.1215291023254395}, {\"index\": 4685, \"title\": \"Duplicates in output delta table after 'ignoreMissingFiles' set to true\", \"x\": 8.175424575805664, \"y\": 3.767469644546509}, {\"index\": 5408, \"title\": \"2202280010003221 - not able to create repo top level folder\", \"x\": 8.702771186828613, \"y\": 4.8055291175842285}, {\"index\": 1928, \"title\": \"Consistent error on job \", \"x\": 8.169557571411133, \"y\": 4.505411624908447}, {\"index\": 1477, \"title\": \"4310827714638474 - 934970a2-fedc-42dd-9240-1793428f4211 - 2205050040006329\", \"x\": 9.07064151763916, \"y\": 3.068925142288208}, {\"index\": 3883, \"title\": \"How to ensure job clusters scale as pool resources become available\", \"x\": 7.835847854614258, \"y\": 2.630798578262329}, {\"index\": 4583, \"title\": \"Degraded performance of SQL vs Pyspark\", \"x\": 6.890168190002441, \"y\": 3.9569015502929688}, {\"index\": 5144, \"title\": \"I can't start clusters\", \"x\": 6.426785945892334, \"y\": 0.9278680682182312}, {\"index\": 3343, \"title\": \"2204060030001043 \", \"x\": 9.559694290161133, \"y\": 3.854837417602539}, {\"index\": 1563, \"title\": \"Workspace Job Limit\", \"x\": 8.627355575561523, \"y\": 1.9891937971115112}, {\"index\": 175, \"title\": \"Need to check why the model training for so long and why it's stuck shown from log4j logs\", \"x\": 6.555628299713135, \"y\": 4.351357936859131}, {\"index\": 3158, \"title\": \"Follow up of- 00131007\", \"x\": 7.988124370574951, \"y\": 6.525306224822998}, {\"index\": 2520, \"title\": \"Python lib install failed in databricks cluster- Saved\", \"x\": 5.640326023101807, \"y\": 1.3849871158599854}, {\"index\": 3661, \"title\": \"One of the ETL continiously failing with broadcast join\", \"x\": 5.112861633300781, \"y\": 0.8799928426742554}, {\"index\": 4868, \"title\": \"2202170030000764\", \"x\": 9.673977851867676, \"y\": 2.2994167804718018}, {\"index\": 4206, \"title\": \"ARR: Question for Canada Region: SR 2203230040005538 \", \"x\": 10.476343154907227, \"y\": 2.6863181591033936}, {\"index\": 5079, \"title\": \"Azure DevOps Integration is failing since 3rd March\", \"x\": 5.8734259605407715, \"y\": 3.7609078884124756}, {\"index\": 946, \"title\": \"cannot start job\", \"x\": 9.200818061828613, \"y\": 3.2764203548431396}, {\"index\": 3721, \"title\": \"2203300030000589\", \"x\": 5.995522499084473, \"y\": 3.3529696464538574}, {\"index\": 3599, \"title\": \"Clusters list api not returning all clusters\", \"x\": 8.383252143859863, \"y\": 2.883992910385132}, {\"index\": 4536, \"title\": \"ARR | 2203170040005277 | Bootstrap timeout in EastUS2 in 2 subscriptions of AT&T\", \"x\": 6.598334789276123, \"y\": 1.6700661182403564}, {\"index\": 5066, \"title\": \"Follow Up - 00135252 || Notebook getting a transient, intermittent error updating DeltaTable log with last check point\", \"x\": 8.966757774353027, \"y\": 3.6272389888763428}, {\"index\": 1542, \"title\": \"User or User Group Access Granted to All Scopes, All Jobs, and All Notebooks\", \"x\": 4.363482475280762, \"y\": 1.7380149364471436}, {\"index\": 5169, \"title\": \"unable to instantiate HiveMetastoreclient\", \"x\": 10.463201522827148, \"y\": 4.024902820587158}, {\"index\": 1433, \"title\": \"Random issue running a query in Databricks\", \"x\": 5.865718364715576, \"y\": 4.758449077606201}, {\"index\": 616, \"title\": \"[ARR][2204260040005094 ][Geico]Unable to set log destiation to dbfs mnt\", \"x\": 10.19260025024414, \"y\": 3.9393458366394043}, {\"index\": 3455, \"title\": \"Log4j Libraries still coming up with the latest PVC 3.60.7\", \"x\": 6.121082305908203, \"y\": 3.093740224838257}, {\"index\": 3176, \"title\": \"delta table can't be modified\", \"x\": 9.13461685180664, \"y\": 4.468482971191406}, {\"index\": 1646, \"title\": \"Cluster starting with an deregistered instance profile\", \"x\": 5.641894340515137, \"y\": 3.8550853729248047}, {\"index\": 1395, \"title\": \"Long running time of pymc3 sampling\", \"x\": 9.255826950073242, \"y\": 4.319104194641113}, {\"index\": 630, \"title\": \"2205130040002413 databricks's autoloader can not consume message from the queue storage\", \"x\": 9.896390914916992, \"y\": 2.5590364933013916}, {\"index\": 955, \"title\": \"Redirect URIs\", \"x\": 9.687956809997559, \"y\": 3.6497390270233154}, {\"index\": 194, \"title\": \"2205260040007389 | Job Failure\", \"x\": 10.173067092895508, \"y\": 2.465920925140381}, {\"index\": 4800, \"title\": \"Cluster Slowness\", \"x\": 4.518218994140625, \"y\": 1.7545459270477295}, {\"index\": 909, \"title\": \"7797454808119526 - f8197865-f5c7-4719-a848-6301201fe96a - 2205140060000097\", \"x\": 10.198131561279297, \"y\": 3.0857784748077393}, {\"index\": 2911, \"title\": \"EU-West-2 and EU-Central-1 down\", \"x\": 6.8787994384765625, \"y\": 3.8863978385925293}, {\"index\": 1006, \"title\": \"ARR-Getting time out issues with storage account\", \"x\": 7.8239970207214355, \"y\": 4.771477699279785}, {\"index\": 5108, \"title\": \"deleted cluster\", \"x\": 5.950268268585205, \"y\": 4.036731719970703}, {\"index\": 3671, \"title\": \"Invalid call to qualifier on unresolved object, tree:\", \"x\": 6.951967716217041, \"y\": 2.420931577682495}, {\"index\": 2827, \"title\": \"Python packages pip install takes too long\", \"x\": 8.164334297180176, \"y\": 5.452325820922852}, {\"index\": 4572, \"title\": \"Streaming query Exception : exception thrown awaitResult\", \"x\": 10.0204439163208, \"y\": 3.0979197025299072}, {\"index\": 368, \"title\": \"Increased AWS data transfer regional costs on clusters\", \"x\": 8.024871826171875, \"y\": 2.8309855461120605}, {\"index\": 2351, \"title\": \"Admin access lost\", \"x\": 6.149492263793945, \"y\": 2.594947338104248}, {\"index\": 4719, \"title\": \"Unable to write to S3 using same config\", \"x\": 10.457235336303711, \"y\": 3.507561445236206}, {\"index\": 1828, \"title\": \"is_member function in python\", \"x\": 5.809459686279297, \"y\": 3.5146167278289795}, {\"index\": 3448, \"title\": \"ARR| 2204010050001066 | Job Failure OSError: [Errno 107] Transport endpoint is not connected\", \"x\": 8.228610038757324, \"y\": 3.86018705368042}, {\"index\": 5269, \"title\": \"htmlwidgets doesn't since March3th\", \"x\": 7.094972610473633, \"y\": 3.2121036052703857}, {\"index\": 2361, \"title\": \"Unable to start cluster\", \"x\": 6.646070957183838, \"y\": 5.489840507507324}, {\"index\": 2577, \"title\": \"unable to stat any cliusters\", \"x\": 7.418583869934082, \"y\": 5.634020805358887}, {\"index\": 619, \"title\": \"Python version/library issue - hundreds of jobs are failing starting today\", \"x\": 7.538148403167725, \"y\": 2.501107692718506}, {\"index\": 4673, \"title\": \"cluster created have spark.master as local[*] by default, preventing worker utilization\", \"x\": 9.6889009475708, \"y\": 6.7291483879089355}, {\"index\": 4605, \"title\": \"IMDSv2 enforcement\", \"x\": 9.780440330505371, \"y\": 3.0565736293792725}, {\"index\": 3963, \"title\": \"add user failure\", \"x\": 7.395214557647705, \"y\": 6.2318243980407715}, {\"index\": 4419, \"title\": \"cmd get stuck in UI\", \"x\": 10.219893455505371, \"y\": 5.166396141052246}, {\"index\": 216, \"title\": \"Notebook execution failure\", \"x\": 9.281695365905762, \"y\": 3.1503937244415283}, {\"index\": 4589, \"title\": \"Issue while parsing csv in databricks\", \"x\": 8.326155662536621, \"y\": 1.5329302549362183}, {\"index\": 2954, \"title\": \"2204120030001057 |  ARR | problem with  DBR 10.4 LTS while flattening the json\", \"x\": 8.101832389831543, \"y\": 2.6274611949920654}, {\"index\": 4808, \"title\": \"Status on long term solution for Databricks - ES-52683\", \"x\": 7.673888683319092, \"y\": 5.496679782867432}, {\"index\": 4590, \"title\": \"Availability of Graviton R6g instances\", \"x\": 7.68192720413208, \"y\": 5.450974941253662}, {\"index\": 4161, \"title\": \"Elevated job execution time on E2 workspace\", \"x\": 6.220286846160889, \"y\": 4.65549373626709}, {\"index\": 1055, \"title\": \"ACL permissions\", \"x\": 5.1790666580200195, \"y\": 0.918807327747345}, {\"index\": 5390, \"title\": \"all jobs failed with error of Cannot find releaseKey from sparkVersions List(SparkVersion(10.4.x-aarch64-photon-scala2.12\", \"x\": 8.052518844604492, \"y\": 2.131042957305908}, {\"index\": 4384, \"title\": \"1866 - ARR - Safeway - Failed job\", \"x\": 9.539033889770508, \"y\": 6.732717990875244}, {\"index\": 1810, \"title\": \"Data access from Databricks SQL endpoint works incorrecly\", \"x\": 10.19551944732666, \"y\": 2.467498540878296}, {\"index\": 4824, \"title\": \"Unable to load AWS credentials\", \"x\": 6.502185821533203, \"y\": 5.3361430168151855}, {\"index\": 4885, \"title\": \"Crypto Mining notification email received from Microsoft for the tenant 'Creative EC'-\", \"x\": 8.815352439880371, \"y\": 5.54922342300415}, {\"index\": 4841, \"title\": \"Delta table schema rewrite failures\", \"x\": 8.797935485839844, \"y\": 4.793659210205078}, {\"index\": 2678, \"title\": \"PySpark testing in Databricks\", \"x\": 9.344067573547363, \"y\": 4.121143341064453}, {\"index\": 1416, \"title\": \"Data Lake multiple Hourly Job Failures due to Invalid access token error\", \"x\": 8.002579689025879, \"y\": 3.3862574100494385}, {\"index\": 2608, \"title\": \"Specify Auto Loader checkpoint location with Delta Live Table\", \"x\": 5.281444549560547, \"y\": 2.352858543395996}, {\"index\": 1724, \"title\": \"Possible bug in JDBC driver\", \"x\": 7.01411247253418, \"y\": 3.463865041732788}, {\"index\": 460, \"title\": \"Access Azure Synapse from Azure Databricks with SQLDW spark connector\", \"x\": 7.597162246704102, \"y\": 5.500887393951416}, {\"index\": 4184, \"title\": \"Job in E2 is taking much longer compared to run in PVC\", \"x\": 7.9491987228393555, \"y\": 2.1080780029296875}, {\"index\": 2450, \"title\": \"Databricks repos - Seeing changed/modified files in the repos tab\", \"x\": 7.9047112464904785, \"y\": 1.1036133766174316}, {\"index\": 3080, \"title\": \"Correct Course to Learn Databricks\", \"x\": 9.373725891113281, \"y\": 4.570446968078613}, {\"index\": 1135, \"title\": \"Unity Catalog Data Explorer stopped working\", \"x\": 7.595142364501953, \"y\": 3.7252278327941895}, {\"index\": 267, \"title\": \"Databricks Operational Outage\", \"x\": 10.628647804260254, \"y\": 4.279773712158203}, {\"index\": 3565, \"title\": \"dbutils.fs.cp failed with s3 mnt target\", \"x\": 10.449193954467773, \"y\": 4.590731620788574}, {\"index\": 3179, \"title\": \"2204070040008171 | Init script\", \"x\": 6.504763126373291, \"y\": 0.8415856957435608}, {\"index\": 3941, \"title\": \"Longrunning Jobs with Databricks\", \"x\": 7.106104373931885, \"y\": 3.2598495483398438}, {\"index\": 2752, \"title\": \"unable to launch clusters\", \"x\": 8.399155616760254, \"y\": 1.8600564002990723}, {\"index\": 1521, \"title\": \"many jobs on the Scheduled Jobs cluster failing at the very beginning with 'Canceled' messaged\", \"x\": 8.235575675964355, \"y\": 5.03731632232666}, {\"index\": 4137, \"title\": \"Unregistered User flagged as Registered In Databricks Academy\", \"x\": 9.787323951721191, \"y\": 2.5961694717407227}, {\"index\": 4610, \"title\": \"location is not accessble in Privesera cluster\", \"x\": 8.2433443069458, \"y\": 4.211506366729736}, {\"index\": 817, \"title\": \"Inconsistent File Loading Error\", \"x\": 7.269577503204346, \"y\": 5.108401775360107}, {\"index\": 769, \"title\": \"Cannot add user to Databricks\", \"x\": 9.462997436523438, \"y\": 5.115767002105713}, {\"index\": 3597, \"title\": \"Max retries exceeded with url , Failed to establish a new connection when connecting translator resource from databricks\", \"x\": 7.375460624694824, \"y\": 1.787187099456787}, {\"index\": 4917, \"title\": \"Dependency management in Databricks\", \"x\": 9.477910995483398, \"y\": 2.5864527225494385}, {\"index\": 2206, \"title\": \"Unable to launch AWS databricks jobs - AmazonS3Exception on databricks bucket\", \"x\": 5.703537464141846, \"y\": 2.2507214546203613}, {\"index\": 4614, \"title\": \"ARR- Azure Databricks Production Cluster terminated.Reason:Bootstrap Timeout- 2203170040000405\", \"x\": 6.2966413497924805, \"y\": 3.2958106994628906}, {\"index\": 2857, \"title\": \"wrong EBS volume attached to clsuters\", \"x\": 8.463566780090332, \"y\": 1.7663935422897339}, {\"index\": 234, \"title\": \"AuditLog\\u8a2d\\u5b9a\\u6642\\u306es3:DeleteObject\\u306f\\u5fc5\\u9808\\u6a29\\u9650\\u3068\\u306a\\u308b\\u304b\\u3002\", \"x\": 7.661472320556641, \"y\": 6.043008327484131}, {\"index\": 4214, \"title\": \"UI doesn't allow Auth Flows\", \"x\": 5.361008644104004, \"y\": 1.4519270658493042}, {\"index\": 4466, \"title\": \"Follow up for 00138574 | ARR | Serv A \", \"x\": 8.644855499267578, \"y\": 5.556878566741943}, {\"index\": 1291, \"title\": \"\\\"No such file or directory\\\" error when using databricks impor_dir cli\", \"x\": 8.790220260620117, \"y\": 3.9499924182891846}, {\"index\": 1299, \"title\": \"errors running deployment script in databricks\", \"x\": 7.816776275634766, \"y\": 5.496603012084961}, {\"index\": 1, \"title\": \"Union with Null\", \"x\": 9.7839937210083, \"y\": 6.822251319885254}, {\"index\": 3015, \"title\": \"2204040050001391\", \"x\": 9.986710548400879, \"y\": 2.247093677520752}, {\"index\": 2162, \"title\": \"Cannot register AWS KMS customer-managed encryption key\", \"x\": 10.313720703125, \"y\": 2.546130895614624}, {\"index\": 2354, \"title\": \"Access issues for S3 mounts\", \"x\": 8.148345947265625, \"y\": 5.539950370788574}, {\"index\": 2772, \"title\": \"cluster stage looks stuck\", \"x\": 6.5426554679870605, \"y\": 3.6314802169799805}, {\"index\": 1752, \"title\": \"Cluster startup takes more than 10mins\", \"x\": 8.025017738342285, \"y\": 3.512874126434326}, {\"index\": 3180, \"title\": \"Cannot make user account\", \"x\": 4.387484073638916, \"y\": 1.7051411867141724}, {\"index\": 3737, \"title\": \"[AT&T][ARR][2203290040006816 ]do not see the download button\", \"x\": 7.905869960784912, \"y\": 2.539111852645874}, {\"index\": 4835, \"title\": \"True billing\", \"x\": 6.842044830322266, \"y\": 3.9302713871002197}, {\"index\": 210, \"title\": \"GENIE ACCESS REQUEST\", \"x\": 10.150665283203125, \"y\": 2.951042413711548}, {\"index\": 671, \"title\": \"Streaming Aggregation not working with Delta Source\", \"x\": 5.246840953826904, \"y\": 1.4363256692886353}, {\"index\": 2717, \"title\": \"Need separate workspace for prod environment\", \"x\": 10.342642784118652, \"y\": 3.2445130348205566}, {\"index\": 5312, \"title\": \"Databricks write optimization\", \"x\": 7.989189624786377, \"y\": 4.201605319976807}, {\"index\": 2605, \"title\": \"Not able to access file in Azure Blob Storage\", \"x\": 7.690901756286621, \"y\": 4.360283851623535}, {\"index\": 438, \"title\": \" 2205230040006464  Problem passing a timestamp argument\", \"x\": 5.788792133331299, \"y\": 3.3878400325775146}, {\"index\": 3474, \"title\": \"update zone_id on instance pools\", \"x\": 9.377211570739746, \"y\": 2.4695258140563965}, {\"index\": 1475, \"title\": \"Cluster not getting terminated even when job is getting completed\", \"x\": 7.297972202301025, \"y\": 1.8391143083572388}, {\"index\": 1103, \"title\": \"Connection pool shut down\", \"x\": 5.549261569976807, \"y\": 1.5882776975631714}, {\"index\": 2019, \"title\": \"[ARR][AIA][Storage Artifact Download Failure]\", \"x\": 9.746011734008789, \"y\": 6.766215801239014}, {\"index\": 3946, \"title\": \"ML flow model migration\", \"x\": 8.867454528808594, \"y\": 5.594696044921875}, {\"index\": 1419, \"title\": \"Unable to delete Private Endpoint connection in AzureDatabricksworkspaces- 220429003000055\", \"x\": 7.6345624923706055, \"y\": 5.357649803161621}, {\"index\": 2435, \"title\": \"can't read mounted files from DBFS from sh commands\", \"x\": 6.6235127449035645, \"y\": 2.548379898071289}, {\"index\": 880, \"title\": \"Unable to mount container in databricks script but are able to mount same container in databricks resource in separate resource group\", \"x\": 7.7039899826049805, \"y\": 2.6523094177246094}, {\"index\": 2640, \"title\": \"Too many connections error\", \"x\": 6.989194869995117, \"y\": 5.636860370635986}, {\"index\": 711, \"title\": \"ARR | 2205160050001904 | Cluster tasks were cancelled\", \"x\": 8.872404098510742, \"y\": 3.2561850547790527}, {\"index\": 1335, \"title\": \"Is there a download API for databricks billing?\", \"x\": 6.071954250335693, \"y\": 4.231569766998291}, {\"index\": 1787, \"title\": \"Azure_185313d2-52ed-4afe-9699-a52268c0db4c\\u2019 cluster are running longer than expected--2204290030001366 \", \"x\": 9.84489917755127, \"y\": 3.0239779949188232}, {\"index\": 3751, \"title\": \"Restrict Shared folder\", \"x\": 8.046595573425293, \"y\": 4.411907196044922}, {\"index\": 4712, \"title\": \"Query returning wrong results\", \"x\": 8.579291343688965, \"y\": 4.873666763305664}, {\"index\": 1008, \"title\": \"please whitelist  3.216.161.215\", \"x\": 9.62270736694336, \"y\": 3.3855905532836914}, {\"index\": 4496, \"title\": \"6148774466112347 | Job performance\", \"x\": 6.224255084991455, \"y\": 2.373307228088379}, {\"index\": 1561, \"title\": \"Queries running for a long time after adding spark config\", \"x\": 7.3085713386535645, \"y\": 3.8863024711608887}, {\"index\": 2872, \"title\": \"scheduled job failed with error message Error in SQL statement: AzureCredentialNotFoundException: Could not find ADLS Gen2 Token\", \"x\": 10.349637031555176, \"y\": 3.173915147781372}, {\"index\": 2694, \"title\": \"Sorted Bucketing does not eliminate sorting from plan\", \"x\": 8.912312507629395, \"y\": 2.9658422470092773}, {\"index\": 4329, \"title\": \"2203180040002303\", \"x\": 6.681820869445801, \"y\": 3.7856879234313965}, {\"index\": 1882, \"title\": \"ARR | Exception: A master URL must be set in your configuration | 2204280030001723 \", \"x\": 4.663535118103027, \"y\": 1.71829092502594}, {\"index\": 5320, \"title\": \"AccessDeniedException: s3a://uat-databricks-trade-blotter/\", \"x\": 9.690732955932617, \"y\": 2.109175443649292}, {\"index\": 5221, \"title\": \"2203080060000023\", \"x\": 8.41879940032959, \"y\": 5.2773308753967285}, {\"index\": 2290, \"title\": \"Prometheus jobs-observability metrics\", \"x\": 7.863773345947266, \"y\": 1.1018236875534058}, {\"index\": 1426, \"title\": \"2205050030000380  When the same notebook runs with job cluster, it is not able to parse a particular xml block and returns NULL instead\", \"x\": 6.05122184753418, \"y\": 1.6240586042404175}, {\"index\": 1120, \"title\": \"ARR Customer Uniparks: Issue - Databricks to storage/Synapse connection issue\", \"x\": 6.861027717590332, \"y\": 5.232108116149902}, {\"index\": 321, \"title\": \"ARR | Production issue : Unable to read delta tables | GEP | 2205250030000679 \", \"x\": 9.367032051086426, \"y\": 2.167433738708496}, {\"index\": 385, \"title\": \"Unable to start clusters\", \"x\": 7.443863391876221, \"y\": 5.449033260345459}, {\"index\": 2217, \"title\": \"cluster failed to launch\", \"x\": 8.165006637573242, \"y\": 3.662198781967163}, {\"index\": 2347, \"title\": \"2204210060007310 | Cluster scaling issue\", \"x\": 7.982585430145264, \"y\": 4.9300127029418945}, {\"index\": 4467, \"title\": \"8573 - ARR - AshleyFurniture - Please enable dbutils.secrets.get for new workspace\", \"x\": 7.983841896057129, \"y\": 5.0352702140808105}, {\"index\": 5181, \"title\": \"Unreliable JDBC connection credentials\", \"x\": 5.712625980377197, \"y\": 1.6072028875350952}, {\"index\": 3078, \"title\": \"Follow-up for 00138795\", \"x\": 4.474968433380127, \"y\": 1.7966219186782837}, {\"index\": 4832, \"title\": \"Databricks Upgrade - Japan\", \"x\": 7.935978889465332, \"y\": 6.597072601318359}, {\"index\": 2705, \"title\": \"Delta and AWS intelligent tiering\", \"x\": 8.5861177444458, \"y\": 4.485689163208008}, {\"index\": 4492, \"title\": \"job that usually takes minutes to run got stuck for more than one day\", \"x\": 7.904871940612793, \"y\": 3.5429129600524902}, {\"index\": 1301, \"title\": \"Cluster not starting up\", \"x\": 4.610087871551514, \"y\": 1.767508864402771}, {\"index\": 4148, \"title\": \"custom_tags \\\"unlimited\\\" - Job clusters policy\", \"x\": 7.610486030578613, \"y\": 1.7604498863220215}, {\"index\": 4462, \"title\": \"Query Failing with Null Pointer Exception\", \"x\": 7.918259620666504, \"y\": 5.69793701171875}, {\"index\": 1330, \"title\": \"CLOUD_PROVIDER_LAUNCH_FAILURE(CLOUD_FAILURE):\", \"x\": 9.076669692993164, \"y\": 6.284490585327148}, {\"index\": 4323, \"title\": \"When trying to create a workspace using terraform I get the error message...\", \"x\": 6.39994478225708, \"y\": 2.145561933517456}, {\"index\": 1399, \"title\": \"5829038454671229 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2205060040004996\", \"x\": 7.259408473968506, \"y\": 2.0086846351623535}, {\"index\": 5391, \"title\": \"Can't start a cluster from Databricks console or thru API\", \"x\": 5.7373223304748535, \"y\": 1.6540353298187256}, {\"index\": 4489, \"title\": \"Spark questions\", \"x\": 10.604084014892578, \"y\": 4.312709808349609}, {\"index\": 3729, \"title\": \"QUOTA_EXCEEDED error in production\", \"x\": 9.598668098449707, \"y\": 3.296328067779541}, {\"index\": 5226, \"title\": \"3121 - g - ARR - GAP - Job not finishing after all tasks complete\", \"x\": 8.22205924987793, \"y\": 6.434441089630127}, {\"index\": 3773, \"title\": \"Workspace creation fails with \\\"registering credentials\\\" error\", \"x\": 9.030730247497559, \"y\": 4.758916854858398}, {\"index\": 1420, \"title\": \"UI is limited for DBFS\", \"x\": 9.24449348449707, \"y\": 2.5904793739318848}, {\"index\": 2274, \"title\": \"delta live table pipeline freezes the databricks UI\", \"x\": 8.993268013000488, \"y\": 4.600072383880615}, {\"index\": 1893, \"title\": \"Written pandas data can not be read outside Databricks\", \"x\": 5.382815837860107, \"y\": 1.357944130897522}, {\"index\": 2781, \"title\": \"ARR Customer MyFedEx - I am encountering an error during cluster launch or creation\", \"x\": 5.702463150024414, \"y\": 1.6352442502975464}, {\"index\": 2975, \"title\": \"Schema Browser not Wworking\", \"x\": 9.452662467956543, \"y\": 3.1422135829925537}, {\"index\": 3767, \"title\": \"Cluster isssue\", \"x\": 9.058349609375, \"y\": 3.531865358352661}, {\"index\": 1721, \"title\": \"User accounts went missing - now are corrupt?\", \"x\": 7.867976665496826, \"y\": 6.68220853805542}, {\"index\": 2961, \"title\": \"[ARR] Enable dbutils.secrets.get on WS 3385660146604390\", \"x\": 6.9008049964904785, \"y\": 5.37238073348999}, {\"index\": 3755, \"title\": \"Custom Image issue\", \"x\": 7.543350696563721, \"y\": 4.4300713539123535}, {\"index\": 1231, \"title\": \"ClassNotFoundException: org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory on 10.4 LTS runtime\", \"x\": 6.5816874504089355, \"y\": 1.5818731784820557}, {\"index\": 3514, \"title\": \"CSS-ARR-2204010030001002-Run_result_unavailable\", \"x\": 5.606154918670654, \"y\": 1.572175145149231}, {\"index\": 3305, \"title\": \"Job Execution Poor Performance\", \"x\": 7.075486660003662, \"y\": 4.7247467041015625}, {\"index\": 4908, \"title\": \"How to provision S3 from databricks?\", \"x\": 7.995420932769775, \"y\": 6.164797782897949}, {\"index\": 396, \"title\": \"Security vulnerabilities\", \"x\": 9.521970748901367, \"y\": 4.074319839477539}, {\"index\": 5603, \"title\": \"Cluster crash weekly\", \"x\": 7.483842372894287, \"y\": 4.285887718200684}, {\"index\": 4422, \"title\": \"Cannot do time travel to older version of delta\", \"x\": 8.271785736083984, \"y\": 4.353005409240723}, {\"index\": 4964, \"title\": \"Jobs failing in our production enviornment due to cluster issues\", \"x\": 7.146424770355225, \"y\": 0.45474573969841003}, {\"index\": 4164, \"title\": \"A problem against Databrick service\", \"x\": 9.197694778442383, \"y\": 6.685790061950684}, {\"index\": 3172, \"title\": \"new failure in automated databricks pipeline\", \"x\": 8.960806846618652, \"y\": 4.986460208892822}, {\"index\": 1546, \"title\": \"Patient 360 - Production Job failure due to exception \\\"IllegalArgumentException\\\"\", \"x\": 7.805088996887207, \"y\": 2.88378643989563}, {\"index\": 3079, \"title\": \"Assistance in tracking a spike in VPC network cost\", \"x\": 9.844795227050781, \"y\": 2.6652908325195312}, {\"index\": 1535, \"title\": \"FOLLOUP | ARR | 2205040030000589 | Metastore connection failure\", \"x\": 7.705836296081543, \"y\": 1.6414868831634521}, {\"index\": 4580, \"title\": \"Databricks Job\", \"x\": 5.260683059692383, \"y\": 2.013698101043701}, {\"index\": 2730, \"title\": \"ARR | 2204130030001643 | The job is failing to run multiple notebooks in the same job\", \"x\": 6.753453731536865, \"y\": 3.399740219116211}, {\"index\": 5330, \"title\": \"AT&T - SparkGeo Library not working on DBR 9.1 LTS with GPU\", \"x\": 8.572779655456543, \"y\": 3.1054799556732178}, {\"index\": 2088, \"title\": \"Jobs API 2.1 \\\"/reset\\\" endpoint doesn't pick up \\\"runtime_engine\\\" PHOTON specification\", \"x\": 9.621414184570312, \"y\": 2.296757221221924}, {\"index\": 2612, \"title\": \"[P1]sql endpoint didn't working at all\", \"x\": 8.291326522827148, \"y\": 2.3094606399536133}, {\"index\": 613, \"title\": \"Unable to launch a large cluster\", \"x\": 6.309579849243164, \"y\": 3.8707947731018066}, {\"index\": 290, \"title\": \"ARR | Databricks job deleted automatically | 2205250030001931 \", \"x\": 7.377037525177002, \"y\": 4.152822017669678}, {\"index\": 3100, \"title\": \"Hi there, when we try to run packrat::init('/databricks') in the databricks notebook, it gives us an error\", \"x\": 5.177600860595703, \"y\": 2.109611749649048}, {\"index\": 186, \"title\": \"runtime 6.4 support ending by June'22. What is the best way to upgrade with least amount of changes and regression testing? Extension?\", \"x\": 5.463140487670898, \"y\": 1.1950905323028564}, {\"index\": 990, \"title\": \"[ARR] [Sev B] SR-2205120030001522 Disable notebook schedule option\", \"x\": 4.461041450500488, \"y\": 1.7472440004348755}, {\"index\": 2038, \"title\": \"Follow up of 00141209.\", \"x\": 8.245719909667969, \"y\": 1.8439569473266602}, {\"index\": 5282, \"title\": \"What cluster manager types on Spark on Databricks\", \"x\": 8.127368927001953, \"y\": 2.1220853328704834}, {\"index\": 3794, \"title\": \"Overwatch job is failing \", \"x\": 9.338648796081543, \"y\": 4.2052154541015625}, {\"index\": 4637, \"title\": \"Query using odbc is getting stuck\", \"x\": 6.732885360717773, \"y\": 1.4910025596618652}, {\"index\": 4692, \"title\": \"2203070030000668\", \"x\": 5.965798377990723, \"y\": 4.524946212768555}, {\"index\": 4126, \"title\": \"Query has been timed out due to inactivity. on sql endpoint\", \"x\": 9.898716926574707, \"y\": 2.485142230987549}, {\"index\": 2263, \"title\": \"Error when mounting file system and trying to read file on spark\", \"x\": 5.656128406524658, \"y\": 1.586028814315796}, {\"index\": 2233, \"title\": \"Error in a table\", \"x\": 6.997476100921631, \"y\": 1.650571346282959}, {\"index\": 5359, \"title\": \"2203020050002002 - JDBC Connection from Tableau\", \"x\": 8.022597312927246, \"y\": 2.0981240272521973}, {\"index\": 239, \"title\": \"How to assess performance of notebook included with %run magic\", \"x\": 6.291750431060791, \"y\": 3.3160743713378906}, {\"index\": 1352, \"title\": \"Follow up 00143996\", \"x\": 10.375091552734375, \"y\": 2.5773425102233887}, {\"index\": 650, \"title\": \"CSS-ARR-S500-SR#2205130050000700-OSError: [Errno 95] Operation not supported\", \"x\": 8.65177059173584, \"y\": 5.446527481079102}, {\"index\": 1501, \"title\": \"2205050050000703 Cx cannot clone and move a folder in workspace using UI\", \"x\": 6.028652667999268, \"y\": 2.9473912715911865}, {\"index\": 1559, \"title\": \"ARR Customer: Deloitte - Unable to Access Databricks Cluster \", \"x\": 7.613278865814209, \"y\": 3.9145684242248535}, {\"index\": 1331, \"title\": \"Business Impact field change testing.Ignore\", \"x\": 8.394661903381348, \"y\": 2.358285665512085}, {\"index\": 4649, \"title\": \"2202240040007195SF || OSError: [Errno 5] Input/output error - 'ImportHookFinder' object has no attribute 'find_spec'\", \"x\": 8.868071556091309, \"y\": 4.932351112365723}, {\"index\": 487, \"title\": \"REST API connectivity Issues from ADB Notebook\", \"x\": 5.8884196281433105, \"y\": 4.211630821228027}, {\"index\": 4850, \"title\": \"Azure databrikcs isn't able to fetch the Hive metastore 3.1.0 JARs from Maven\", \"x\": 9.472719192504883, \"y\": 2.688404083251953}, {\"index\": 4947, \"title\": \"Databricks issue\", \"x\": 10.319809913635254, \"y\": 3.8218438625335693}, {\"index\": 4157, \"title\": \"Issue in table creation from Rstudio using saveAsTable through databricks connect\", \"x\": 5.109745502471924, \"y\": 0.8621317744255066}, {\"index\": 1772, \"title\": \"Jobs time out (follow up 00138978)\", \"x\": 7.6049394607543945, \"y\": 1.9386569261550903}, {\"index\": 730, \"title\": \"[ARR] [Sev C] SR-2205080030000417-Slow performance\", \"x\": 9.034833908081055, \"y\": 4.340385913848877}, {\"index\": 3793, \"title\": \"Cannot find releaseKey from sparkVersions List(SparkVersion(12.x-snapshot-aarch64-photon-scala2.12,12.x Snaps...\", \"x\": 8.00853157043457, \"y\": 1.8534221649169922}, {\"index\": 1678, \"title\": \"Workspace network configuration Warning\", \"x\": 7.905830383300781, \"y\": 4.671439170837402}, {\"index\": 2478, \"title\": \"Saving DataFrame to db table results in inserting duplicates\", \"x\": 10.127432823181152, \"y\": 5.876887798309326}, {\"index\": 345, \"title\": \"some inconsistent stats in the few feature 'Data Profile'\", \"x\": 9.030839920043945, \"y\": 4.7161993980407715}, {\"index\": 1313, \"title\": \"Cluster issues\", \"x\": 8.85093879699707, \"y\": 4.79166841506958}, {\"index\": 3104, \"title\": \"Delta write failures to S3\", \"x\": 9.605572700500488, \"y\": 3.8090057373046875}, {\"index\": 5431, \"title\": \"Query about event logs and init scripts \", \"x\": 7.1621294021606445, \"y\": 2.932673931121826}, {\"index\": 5453, \"title\": \"Cluster terminated by init script failure\", \"x\": 7.06147575378418, \"y\": 5.480893135070801}, {\"index\": 3928, \"title\": \"ARR:Hit error (file not found) using High Concurrency cluster, but works fine with pass through cluster:SR 2203280030003944\", \"x\": 6.622166156768799, \"y\": 3.473507881164551}, {\"index\": 4096, \"title\": \"Need attention on SF # 00139224\", \"x\": 7.669069290161133, \"y\": 2.2214725017547607}, {\"index\": 5063, \"title\": \"2203080040007597SF || Databricks Autoloader\", \"x\": 7.544533729553223, \"y\": 3.0819857120513916}, {\"index\": 3112, \"title\": \"nvirginia.cloud.databricks.com:443 failed to respond\", \"x\": 7.68992280960083, \"y\": 3.9910972118377686}, {\"index\": 92, \"title\": \"DBInsight 2205270040003899 \", \"x\": 9.750076293945312, \"y\": 6.69854211807251}, {\"index\": 1333, \"title\": \"Not able to start any cluster due to timeout issue\", \"x\": 7.010903835296631, \"y\": 0.31570321321487427}, {\"index\": 5381, \"title\": \"Unable to login to databricks current production E1\", \"x\": 10.001120567321777, \"y\": 3.308537006378174}, {\"index\": 3534, \"title\": \" Job run longer and manually terminated the cluster\", \"x\": 6.050585746765137, \"y\": 2.4578733444213867}, {\"index\": 3485, \"title\": \"Does databricks support folded query in powerBI?\", \"x\": 10.210260391235352, \"y\": 4.8258209228515625}, {\"index\": 913, \"title\": \"ARR | Continuation of 00138590 |Cluster failed due to driver unhealthy error - 2203150040007744\", \"x\": 8.018105506896973, \"y\": 2.0125532150268555}, {\"index\": 3202, \"title\": \"Unusual Exit Status\", \"x\": 6.307155132293701, \"y\": 4.602329730987549}, {\"index\": 794, \"title\": \"Error in installing Geopandas package\", \"x\": 7.668874263763428, \"y\": 1.4747931957244873}, {\"index\": 960, \"title\": \"Billing in AWS\", \"x\": 10.107399940490723, \"y\": 5.426371097564697}, {\"index\": 2060, \"title\": \"databricks job failed, DRIVER_UNRESPONSIVE\", \"x\": 4.62064266204834, \"y\": 1.7139437198638916}, {\"index\": 392, \"title\": \"ARR |  | 2205130040004443\", \"x\": 8.106311798095703, \"y\": 5.976558685302734}, {\"index\": 5297, \"title\": \"Issues reading gzip csv file from S3\", \"x\": 7.667856216430664, \"y\": 3.001546621322632}, {\"index\": 268, \"title\": \"ARR Customer Chicago Trading Company: Issue - Databricks Clusters are Inaccessible\", \"x\": 8.186468124389648, \"y\": 3.497544527053833}, {\"index\": 4880, \"title\": \"2203140030000467 \", \"x\": 8.914868354797363, \"y\": 5.066593647003174}, {\"index\": 342, \"title\": \"Spark version change issue\", \"x\": 7.2599711418151855, \"y\": 1.7200695276260376}, {\"index\": 2196, \"title\": \"Vulnerability Issue in nodes of databricks cluster\", \"x\": 7.3125224113464355, \"y\": 3.21000075340271}, {\"index\": 1989, \"title\": \"Prod jobs failing with library issues\", \"x\": 8.30406379699707, \"y\": 3.3777413368225098}, {\"index\": 4592, \"title\": \"Databricks Job Hanging Tasks\", \"x\": 8.215335845947266, \"y\": 3.5791265964508057}, {\"index\": 4897, \"title\": \"[ARR] SR-2203080050002763  Multiple failures on multiple cosmos db instances java.net.SocketTimeoutException: Read timed out\", \"x\": 9.380125999450684, \"y\": 2.5662708282470703}, {\"index\": 3762, \"title\": \"Databricks job got stuck for 7 Days\", \"x\": 7.1248459815979, \"y\": 4.392613410949707}, {\"index\": 702, \"title\": \"Networking rules, firewalls on the private and public sub-net\", \"x\": 8.8268404006958, \"y\": 2.7628073692321777}, {\"index\": 1774, \"title\": \"ARR | 2204220060001879 | \", \"x\": 9.775136947631836, \"y\": 2.3661909103393555}, {\"index\": 2181, \"title\": \"Not able to mount storage account\", \"x\": 9.655933380126953, \"y\": 2.6602461338043213}, {\"index\": 1077, \"title\": \"Remote RPC client disassociated\", \"x\": 7.895306587219238, \"y\": 4.3990678787231445}, {\"index\": 2916, \"title\": \"How to collect Spark worker logs?\", \"x\": 7.856785297393799, \"y\": 3.1822829246520996}, {\"index\": 2107, \"title\": \"Databricks Mount point using terraform failed\", \"x\": 8.174793243408203, \"y\": 5.907373428344727}, {\"index\": 4647, \"title\": \"Git pull with a service principal\", \"x\": 7.0734639167785645, \"y\": 4.072296619415283}, {\"index\": 893, \"title\": \"2205130030000118\", \"x\": 6.57944393157959, \"y\": 3.5231218338012695}, {\"index\": 695, \"title\": \"ARR| continuation of 00143529 | Issue using Databricks Repo API\", \"x\": 9.283446311950684, \"y\": 2.600792646408081}, {\"index\": 2126, \"title\": \"Observing a lot of delay and intermittent failures when trying to start a Spark Cluster in Databricks\", \"x\": 6.666018009185791, \"y\": 3.064082145690918}, {\"index\": 4084, \"title\": \"IllegalStateException reading checkpoint\", \"x\": 8.498814582824707, \"y\": 4.440997123718262}, {\"index\": 2885, \"title\": \"Azure Databricks | Technical clarity on usage\", \"x\": 8.030107498168945, \"y\": 2.0943634510040283}, {\"index\": 2971, \"title\": \"2203300030002167 customer wants to use a custom java package in databricks with python\", \"x\": 7.561498165130615, \"y\": 1.9800344705581665}, {\"index\": 2279, \"title\": \"Databricks doesn't use external metastore.\", \"x\": 7.153842449188232, \"y\": 2.032686948776245}, {\"index\": 4518, \"title\": \"Databricks Notebook activity failing in pipeline- Saved\", \"x\": 7.598665237426758, \"y\": 3.8384897708892822}, {\"index\": 3752, \"title\": \"AWS s3 connection timeout\", \"x\": 8.7755765914917, \"y\": 2.392542839050293}, {\"index\": 3120, \"title\": \"Change in data bricks worker type and driver type\", \"x\": 8.245260238647461, \"y\": 4.647005558013916}, {\"index\": 1525, \"title\": \"[ARR] [Sev B] SR-2205040030001061 DataframeReadError (Reason: worker lost)\", \"x\": 8.097084999084473, \"y\": 5.765219211578369}, {\"index\": 443, \"title\": \"vacuum not working\", \"x\": 5.703783988952637, \"y\": 1.4722795486450195}, {\"index\": 1650, \"title\": \"extension on trial\", \"x\": 8.197521209716797, \"y\": 2.238213062286377}, {\"index\": 3269, \"title\": \"Unable to stream data from google pub sub using spark 3\", \"x\": 5.761727809906006, \"y\": 1.733988881111145}, {\"index\": 2037, \"title\": \"ARR | 2204250060001035 | Error with union command saying object of type 'NoneType' has no len() only in DBR  9.1 LTS\", \"x\": 9.32630729675293, \"y\": 6.754134178161621}, {\"index\": 2512, \"title\": \"Facing slowness while executing queries in Databricks cluster\", \"x\": 4.8467278480529785, \"y\": 1.699507474899292}, {\"index\": 2473, \"title\": \"3337075389283370 - 6b64b59f-432d-401e-b552-d855f1e1d2e0 - 2204060040002674\", \"x\": 6.676473617553711, \"y\": 1.155992865562439}, {\"index\": 2634, \"title\": \"User not authenticated error - Workspace access re\", \"x\": 10.739911079406738, \"y\": 3.530683755874634}, {\"index\": 5553, \"title\": \"2203010030002237SF || Recover the Scripts and Notebooks\", \"x\": 9.68289852142334, \"y\": 5.54785680770874}, {\"index\": 3619, \"title\": \"[ARR] [Sev B] SR-2203300030002890 - Need to enable table ACL on standard clusters\", \"x\": 10.403129577636719, \"y\": 4.081228256225586}, {\"index\": 2812, \"title\": \"2204130040011430 | ARR | Error running Job\", \"x\": 7.504997253417969, \"y\": 2.9708898067474365}, {\"index\": 5393, \"title\": \"[ARR] [Sev A] SR-2202230030000040 continue to #00136276Job fails after hive metastore migration\", \"x\": 7.982421398162842, \"y\": 6.422776222229004}, {\"index\": 1653, \"title\": \"Issue with FeatureStoreClient Python API when working with a string type Primary Key\", \"x\": 8.991018295288086, \"y\": 2.8945088386535645}, {\"index\": 430, \"title\": \"Reliable Cluster Configurations\", \"x\": 6.3222527503967285, \"y\": 2.176701068878174}, {\"index\": 1195, \"title\": \"2205 - ARR - GAP - ACL table ownership\", \"x\": 5.535295486450195, \"y\": 3.887730360031128}, {\"index\": 3954, \"title\": \"Create a friendly workspace url\", \"x\": 8.828263282775879, \"y\": 3.550198793411255}, {\"index\": 2743, \"title\": \"Databricks console is down since 6:05 PM PT\", \"x\": 6.597957611083984, \"y\": 4.196605205535889}, {\"index\": 495, \"title\": \"CSS-ARR-2205200050001067-Jobs are failing with error 'Caused by: com.databricks.NotebookExecutionException: FAILED'\", \"x\": 5.725521564483643, \"y\": 4.5522637367248535}, {\"index\": 1437, \"title\": \"[ARR] [Sev B] SR-2204180030001121  query is running since 6 hours\", \"x\": 7.747714996337891, \"y\": 3.9658362865448}, {\"index\": 1684, \"title\": \"ARR 2204110050002238  - Not able to connect to Databricks CLI using AAD token\", \"x\": 9.222914695739746, \"y\": 6.59632682800293}, {\"index\": 5592, \"title\": \"2202240030000948\", \"x\": 8.97425651550293, \"y\": 2.1318142414093018}, {\"index\": 4250, \"title\": \"Did API 2.1 support cluster policy id?\", \"x\": 6.237948417663574, \"y\": 1.5559996366500854}, {\"index\": 5260, \"title\": \"AWS Glue/athena table based on json files is not available in Databricks\", \"x\": 8.525278091430664, \"y\": 2.047163724899292}, {\"index\": 4991, \"title\": \"Unable to connect to MLflow from databricks notebook\", \"x\": 7.019977569580078, \"y\": 0.3020869791507721}, {\"index\": 3663, \"title\": \"Accessing Databricks clusters api via Azure service principal giving error\", \"x\": 7.075382232666016, \"y\": 4.1937336921691895}, {\"index\": 2015, \"title\": \": java.lang.SecurityException: To access secrets using Databricks Connect, please contact support to enable this feature on your workspace.\", \"x\": 6.549694061279297, \"y\": 0.824061393737793}, {\"index\": 3426, \"title\": \"ARR-SFMC-2204040030002172-Quota request for Compute-VM (cores-vCPUs) \", \"x\": 7.56360387802124, \"y\": 1.8392380475997925}, {\"index\": 4815, \"title\": \"Encountered many AWS_REQUEST_LIMIT_EXCEEDED errors\", \"x\": 5.044435501098633, \"y\": 1.5485522747039795}, {\"index\": 1891, \"title\": \"Photon Enabled Instances does not support IST timezone\", \"x\": 9.681632041931152, \"y\": 6.820197582244873}, {\"index\": 3531, \"title\": \"Spark Submit job never completes\", \"x\": 10.229654312133789, \"y\": 3.879696846008301}, {\"index\": 1268, \"title\": \"Scheduled Job - Pass start date as parameter to notebook\", \"x\": 9.629498481750488, \"y\": 2.4115211963653564}, {\"index\": 4477, \"title\": \"EULA expired\", \"x\": 7.220599174499512, \"y\": 0.8883755207061768}, {\"index\": 5318, \"title\": \"Model Training vs Serving Library Versions\", \"x\": 10.127273559570312, \"y\": 4.908791542053223}, {\"index\": 1739, \"title\": \"Information Security - AWS Account 414351767826\", \"x\": 10.052773475646973, \"y\": 2.5458197593688965}, {\"index\": 1128, \"title\": \"Management of Notebook\", \"x\": 7.351017951965332, \"y\": 1.3625513315200806}, {\"index\": 54, \"title\": \"On some occasions the count file is having more numbers than the data file\", \"x\": 7.583129405975342, \"y\": 5.6992669105529785}, {\"index\": 2573, \"title\": \"Unable to start a cluster\", \"x\": 8.07254409790039, \"y\": 2.152099132537842}, {\"index\": 5193, \"title\": \"There is a pipeline on ADF which has databricks notebooks that used to run for 6 hours now for even a \", \"x\": 7.5376105308532715, \"y\": 1.093747854232788}, {\"index\": 2809, \"title\": \"2204110030000463\", \"x\": 6.453120708465576, \"y\": 3.7341134548187256}, {\"index\": 2963, \"title\": \"Libraries are not properly uninstalled from clusters\", \"x\": 8.57715892791748, \"y\": 2.4063844680786133}, {\"index\": 2252, \"title\": \"java.io.IOException Class class com.amazonaws.auth.InstanceProfileCredentialsProvider does not implement AWSCredentialsProvider\", \"x\": 7.836589813232422, \"y\": 1.9202944040298462}, {\"index\": 5242, \"title\": \"Remote RPC client disassociated\", \"x\": 7.590624809265137, \"y\": 3.5648558139801025}, {\"index\": 4498, \"title\": \"Databricks library conflicts\", \"x\": 8.709831237792969, \"y\": 2.4663074016571045}, {\"index\": 5337, \"title\": \"Skip rows for read csv\", \"x\": 7.9270172119140625, \"y\": 1.7735975980758667}, {\"index\": 632, \"title\": \"2205130040002413\", \"x\": 7.6132588386535645, \"y\": 3.2511823177337646}, {\"index\": 577, \"title\": \"Need to enable secret access in Databricks from Databricks-connect\", \"x\": 5.174635410308838, \"y\": 1.9974467754364014}, {\"index\": 4124, \"title\": \"2860334649817398 - ae0b13d9-e09c-4e54-b1e6-8e777d8fbe14 - 2203230010002982\", \"x\": 7.390327453613281, \"y\": 1.9714733362197876}, {\"index\": 5042, \"title\": \"s3commit exception\", \"x\": 9.78891372680664, \"y\": 4.0988311767578125}, {\"index\": 2265, \"title\": \"Job API 2.0 Retirement Date\", \"x\": 6.666996002197266, \"y\": 2.1997265815734863}, {\"index\": 2067, \"title\": \"bodends2 - PD Job failure--2204260040006441\", \"x\": 5.257694721221924, \"y\": 2.21104097366333}, {\"index\": 787, \"title\": \"Workspace notebook executions are inconsistent\", \"x\": 5.737925052642822, \"y\": 3.9668333530426025}, {\"index\": 1080, \"title\": \"Need to change the DBR version of SQL endpoint\", \"x\": 7.437756538391113, \"y\": 5.997404098510742}, {\"index\": 3530, \"title\": \"What are the ways to upload files?\", \"x\": 8.038128852844238, \"y\": 4.705045700073242}, {\"index\": 233, \"title\": \"Authentication error while logging to Databricks\", \"x\": 10.362588882446289, \"y\": 3.3652634620666504}, {\"index\": 5089, \"title\": \"User login issue due to Databricks SCIM API issue\", \"x\": 9.768194198608398, \"y\": 3.6921448707580566}, {\"index\": 2934, \"title\": \"cannot start cluster\", \"x\": 7.113893032073975, \"y\": 0.7532288432121277}, {\"index\": 4320, \"title\": \"Streaming cluster execution failed\", \"x\": 6.767723560333252, \"y\": 4.198301315307617}, {\"index\": 3259, \"title\": \"Corrupt Linux VMs are used\", \"x\": 8.192465782165527, \"y\": 6.494566440582275}, {\"index\": 1855, \"title\": \"ARDA SANDBOX Cluster is not getting started\", \"x\": 5.667285919189453, \"y\": 4.612456321716309}, {\"index\": 3211, \"title\": \"cx is facing CLOUD_PROVIDER_LAUNCH_FAILURE(CLOUD_FAILURE)\", \"x\": 6.242152690887451, \"y\": 4.981478691101074}, {\"index\": 5604, \"title\": \"Security Alert  for connectivity from AWS account owned by Databricks\", \"x\": 9.208131790161133, \"y\": 3.237996816635132}, {\"index\": 3741, \"title\": \"Databricks integration with NewRelic for logs and metrics\", \"x\": 7.962990760803223, \"y\": 1.149288296699524}, {\"index\": 2385, \"title\": \"ARR - 2203180040004316 - Walmart - PowerBI connectivity to SQL Endpoint failing with AAD\", \"x\": 4.977609157562256, \"y\": 1.6224660873413086}, {\"index\": 784, \"title\": \"Databricks - No VM SKUs available\", \"x\": 9.881587028503418, \"y\": 3.633883237838745}, {\"index\": 4194, \"title\": \"Error when loading R package\", \"x\": 10.267653465270996, \"y\": 5.459170818328857}, {\"index\": 2498, \"title\": \"terraform template limitation\", \"x\": 6.546736240386963, \"y\": 5.41869592666626}, {\"index\": 4043, \"title\": \"2203130040000905\", \"x\": 7.672297477722168, \"y\": 2.8137922286987305}, {\"index\": 5034, \"title\": \"E2-Migration cross account S3 mount doesn't work\", \"x\": 10.459213256835938, \"y\": 2.615511894226074}, {\"index\": 1675, \"title\": \"ARR | delta table question | 2204220050000908\", \"x\": 8.163439750671387, \"y\": 1.7010682821273804}, {\"index\": 785, \"title\": \"Job performance\", \"x\": 7.810594081878662, \"y\": 6.08009147644043}, {\"index\": 4376, \"title\": \"Unable to see the actual table column names in AWS Glue\", \"x\": 7.901051044464111, \"y\": 5.132049083709717}, {\"index\": 524, \"title\": \"Job is failing due to Connection pool shut down\", \"x\": 9.313779830932617, \"y\": 3.2412939071655273}, {\"index\": 714, \"title\": \"[ARR][Cluster terminated.Reason:Storage Download Failure]\", \"x\": 6.003781795501709, \"y\": 4.962264537811279}, {\"index\": 1973, \"title\": \"Using arrays in a select statement is causing query errors\", \"x\": 7.754891395568848, \"y\": 2.5090534687042236}, {\"index\": 5062, \"title\": \"test\", \"x\": 9.274423599243164, \"y\": 3.543318033218384}, {\"index\": 686, \"title\": \"bootstrap timeout for job\", \"x\": 8.592744827270508, \"y\": 1.1423324346542358}, {\"index\": 4741, \"title\": \"Databricks Job - Structured Streaming.\", \"x\": 10.058021545410156, \"y\": 2.8606677055358887}, {\"index\": 4397, \"title\": \"2203180040005194 | Job Failure\", \"x\": 6.6365838050842285, \"y\": 1.9859845638275146}, {\"index\": 500, \"title\": \"Autoloader tasks keep getting stuck \", \"x\": 8.691258430480957, \"y\": 4.394421100616455}, {\"index\": 5136, \"title\": \"Facing issue with Databricks cluster create API\", \"x\": 7.89908504486084, \"y\": 1.8093650341033936}, {\"index\": 1621, \"title\": \"Cluster Terminated. Reason: Azure VM Extension failure\", \"x\": 9.906444549560547, \"y\": 2.5139269828796387}, {\"index\": 2155, \"title\": \"Spark Streaming Job stops loading the data intermittently\", \"x\": 7.098163604736328, \"y\": 3.5331592559814453}, {\"index\": 3147, \"title\": \"[ARR] SR-2203310010000891 DsPredictionV3 messages are lost while publishing to TOPIC\", \"x\": 7.32157564163208, \"y\": 5.102581024169922}, {\"index\": 4938, \"title\": \"Unity Catalog: basic delta example from the doc is not working\", \"x\": 6.864582538604736, \"y\": 2.2065505981445312}, {\"index\": 857, \"title\": \"Customer is unable to write streaming data to Event Hub\", \"x\": 7.789565086364746, \"y\": 1.616368055343628}, {\"index\": 2837, \"title\": \"Help Center - New Authorized Contact - Unable to Log In\", \"x\": 8.796296119689941, \"y\": 5.710307598114014}, {\"index\": 1361, \"title\": \"Timeout Issues--2204220040007031 \", \"x\": 10.276000022888184, \"y\": 5.325887680053711}, {\"index\": 292, \"title\": \"0.4 LTS Runtime Issue\", \"x\": 10.109808921813965, \"y\": 4.855783462524414}, {\"index\": 4325, \"title\": \"how to get a job by name usinng databricks api\", \"x\": 10.364633560180664, \"y\": 5.00441837310791}, {\"index\": 2842, \"title\": \"Metastore\", \"x\": 8.128632545471191, \"y\": 4.9510016441345215}, {\"index\": 561, \"title\": \"CSS-ARR-SR#2205200030000589- Databricks jobs are slow\", \"x\": 9.917435646057129, \"y\": 5.441650390625}, {\"index\": 5334, \"title\": \"Job failing with Unknown host exception\", \"x\": 7.183108329772949, \"y\": 3.0195436477661133}, {\"index\": 5336, \"title\": \"CREATE TABLE doesn't work with existing delta files in table location\", \"x\": 5.551917552947998, \"y\": 3.973499059677124}, {\"index\": 1682, \"title\": \"ADF\", \"x\": 6.044370174407959, \"y\": 3.3957080841064453}, {\"index\": 93, \"title\": \"how to execute SQL file (Databricks detla tables quires ) using sql enpoint in commandline (CICD purpose).\", \"x\": 8.54252815246582, \"y\": 3.6320745944976807}, {\"index\": 4072, \"title\": \"2203250030000140\", \"x\": 6.495721340179443, \"y\": 2.667720317840576}, {\"index\": 2427, \"title\": \"2204180040002686 |  MyFedEx | ARR\", \"x\": 9.467000007629395, \"y\": 4.333705902099609}, {\"index\": 661, \"title\": \"Checkpointing not working for some queries\", \"x\": 8.977311134338379, \"y\": 6.636144638061523}, {\"index\": 4200, \"title\": \"create job from json file\", \"x\": 7.424468040466309, \"y\": 4.587228775024414}, {\"index\": 1061, \"title\": \"Creating additional Workspace in different AWS account. \", \"x\": 6.184734344482422, \"y\": 2.8862059116363525}, {\"index\": 3993, \"title\": \"Loading spark in MLFlow Project\", \"x\": 8.453680992126465, \"y\": 1.7610447406768799}, {\"index\": 4708, \"title\": \"6921036428829021 - 6d522634-b390-4f4f-a7e6-b2bc520e2f79 - 2203150040007785\", \"x\": 7.609407901763916, \"y\": 4.422725677490234}, {\"index\": 2984, \"title\": \"DirectQuery query folding to Databricks producing invalid SQL\", \"x\": 10.101800918579102, \"y\": 4.537106513977051}, {\"index\": 3738, \"title\": \"Fail to update a delta table: TypeError: 'StructField' object is not callable\", \"x\": 8.49210262298584, \"y\": 4.424893856048584}, {\"index\": 1417, \"title\": \"2205050050001878  notebook to store parquet files executing already for 60 hours.\", \"x\": 9.5984525680542, \"y\": 5.434748649597168}, {\"index\": 5189, \"title\": \" Remote RPC client disassociated\", \"x\": 8.33792781829834, \"y\": 5.978504180908203}, {\"index\": 279, \"title\": \"Provide Nexus credentials to clusters\", \"x\": 9.937204360961914, \"y\": 2.8209633827209473}, {\"index\": 5128, \"title\": \"AWS DNS resolution on Databricks platform\", \"x\": 8.861018180847168, \"y\": 5.1754631996154785}, {\"index\": 1703, \"title\": \"Huge Lag in performance in Notebook Navigation\", \"x\": 7.916217803955078, \"y\": 2.1076982021331787}, {\"index\": 4459, \"title\": \"Dropping and Creating Default Tables  from New Metastore\", \"x\": 8.129999160766602, \"y\": 2.7121012210845947}, {\"index\": 657, \"title\": \"Can we use pyspark udf in spark sql like scala udfs ?\", \"x\": 7.880404472351074, \"y\": 2.5266220569610596}, {\"index\": 1438, \"title\": \"Job failed intermittently with \\\"java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\\\"\", \"x\": 9.240214347839355, \"y\": 4.279839992523193}, {\"index\": 4274, \"title\": \"I keep getting SparkException: Job aborted.\", \"x\": 7.671514987945557, \"y\": 5.790602207183838}, {\"index\": 2365, \"title\": \"Spark Job constantly fails due to Executor Loss Failure\", \"x\": 6.499129772186279, \"y\": 0.9428412318229675}, {\"index\": 1939, \"title\": \"0153 - g - ARR - Starbucks - Broadcast(hint) inner join performance issues\", \"x\": 7.916074752807617, \"y\": 6.65915060043335}, {\"index\": 3198, \"title\": \"Production Jobs failing after moving to 10.4 Runtime\", \"x\": 9.477387428283691, \"y\": 5.560281753540039}, {\"index\": 4512, \"title\": \"how to add service principal instead of using SCIM Rest api\", \"x\": 10.685019493103027, \"y\": 2.995138645172119}, {\"index\": 5325, \"title\": \"Hudi dataformat on databricks\", \"x\": 10.798995018005371, \"y\": 3.675536632537842}, {\"index\": 2161, \"title\": \"follow up case 00141072 | call is needed with the customer | please advise on availability \", \"x\": 9.234143257141113, \"y\": 4.9184722900390625}, {\"index\": 1956, \"title\": \"ARR Customer: St Joseph - There is an issue with Databricks. we are not able to run any queries in Databricks workspace.\", \"x\": 6.682714462280273, \"y\": 2.2908637523651123}, {\"index\": 861, \"title\": \"2205160030001058 - Module not found when triggering jobs through ADF at least\", \"x\": 9.799919128417969, \"y\": 3.489363431930542}, {\"index\": 3262, \"title\": \"Access to non-admins to \\\"Create and trigger a one-time run\\\"\", \"x\": 8.09067440032959, \"y\": 6.117767810821533}, {\"index\": 938, \"title\": \"Unable to write from Databricks into Synapse\", \"x\": 7.2643208503723145, \"y\": 4.818541049957275}, {\"index\": 6, \"title\": \"Databricks Cluster creation time increased post fail over from west to east \", \"x\": 9.880084991455078, \"y\": 2.4186198711395264}, {\"index\": 1063, \"title\": \"File read from Container is taking very long time -2\", \"x\": 6.346582889556885, \"y\": 3.777681827545166}, {\"index\": 2704, \"title\": \"Databricks SCIM connector unable to sync certain users\", \"x\": 6.100776195526123, \"y\": 3.7036209106445312}, {\"index\": 4788, \"title\": \"databricks dbx: issues with deploying job clusters and installing python wheel on it\", \"x\": 8.941082000732422, \"y\": 2.4237561225891113}, {\"index\": 3783, \"title\": \"Need help for use case of encryption and decryption\", \"x\": 7.457965850830078, \"y\": 4.0525221824646}, {\"index\": 224, \"title\": \"DBR PROD -  Cannot broadcast the table that is larger than 8GB: 9 GB\", \"x\": 8.66395378112793, \"y\": 4.235193729400635}, {\"index\": 1523, \"title\": \"org.apache.spark.SparkException: Unable to fetch tables of db edl_dev_pd_publish\", \"x\": 8.312602043151855, \"y\": 2.220917224884033}, {\"index\": 436, \"title\": \"2205200030002012 | Python SDK\", \"x\": 7.10860538482666, \"y\": 3.9384920597076416}, {\"index\": 5083, \"title\": \"error with RegressionEvaluator.evaluate\", \"x\": 8.060168266296387, \"y\": 3.580955982208252}, {\"index\": 1043, \"title\": \"2205100040004471\", \"x\": 7.260249614715576, \"y\": 3.777811288833618}, {\"index\": 1358, \"title\": \" Issue with Databricks Auto Loader--2205040040007021 \", \"x\": 5.823861598968506, \"y\": 2.8330657482147217}, {\"index\": 3415, \"title\": \"Not able to login to Manage account\", \"x\": 7.20686674118042, \"y\": 3.103907346725464}, {\"index\": 3703, \"title\": \"Databricks - 429 Too many requests\", \"x\": 9.798095703125, \"y\": 3.016571521759033}, {\"index\": 602, \"title\": \"Download SQL Analytics dashboard via API\", \"x\": 6.014211177825928, \"y\": 4.008552551269531}, {\"index\": 5027, \"title\": \"Jobs failing intermittently\", \"x\": 10.091303825378418, \"y\": 4.798124313354492}, {\"index\": 2646, \"title\": \"ModuleNotFoundError: No module named 'pasde'\", \"x\": 4.7260422706604, \"y\": 1.7382745742797852}, {\"index\": 5261, \"title\": \"Multiple Databricks jobs are failing in a Production Workspace - Spark Exception & Timed Out\", \"x\": 9.326569557189941, \"y\": 1.980062484741211}, {\"index\": 1338, \"title\": \"2205060040000451\", \"x\": 9.441385269165039, \"y\": 2.17897891998291}, {\"index\": 4500, \"title\": \"Error : node stack overflow\", \"x\": 10.024900436401367, \"y\": 3.5707850456237793}, {\"index\": 1853, \"title\": \"Public network access to Azure SQL Server required by Databricks ???\", \"x\": 8.388738632202148, \"y\": 6.067698955535889}, {\"index\": 2451, \"title\": \"redis on DBR 7.3 ?\", \"x\": 6.427170753479004, \"y\": 0.8249191641807556}, {\"index\": 3594, \"title\": \"2204010030001983\", \"x\": 8.847709655761719, \"y\": 4.194451808929443}, {\"index\": 3766, \"title\": \" maxFilesPerTrigger and maxBytesPerTrigger spark settings are not working as expected\", \"x\": 10.319074630737305, \"y\": 3.2377846240997314}, {\"index\": 3275, \"title\": \"Databricks production is down and not able to access\", \"x\": 5.768131256103516, \"y\": 4.5283026695251465}, {\"index\": 4684, \"title\": \"ARR | 2203140030001189 | AholdDelhaize.com | Access token is invalid\", \"x\": 8.507017135620117, \"y\": 6.000540256500244}, {\"index\": 4204, \"title\": \"There is a pipeline on ADF which has databricks notebooks that used to run for 6 hours now for even a\", \"x\": 8.0947904586792, \"y\": 5.423094272613525}, {\"index\": 3715, \"title\": \"ETL Job Fail\", \"x\": 7.442906856536865, \"y\": 2.425816297531128}, {\"index\": 3041, \"title\": \" autoloader failing with error Input marker does not start with input path\", \"x\": 7.604516983032227, \"y\": 5.711912631988525}, {\"index\": 5235, \"title\": \"2203070030002493 | Overwatch\", \"x\": 7.025498390197754, \"y\": 4.656250953674316}, {\"index\": 464, \"title\": \"Snowflake connector\", \"x\": 9.84804630279541, \"y\": 3.299954652786255}, {\"index\": 568, \"title\": \"Cluster Terminating\", \"x\": 6.09199857711792, \"y\": 4.9629807472229}, {\"index\": 725, \"title\": \"Log files can be truncated\", \"x\": 5.274721145629883, \"y\": 1.9023386240005493}, {\"index\": 3761, \"title\": \"7451881325335682 - 55bde1f0-8591-432c-a37f-5c1bdff8c2f8 - 2203300030003256\", \"x\": 7.671177864074707, \"y\": 1.9033713340759277}, {\"index\": 1818, \"title\": \"Delta live tables not showing up under Jobs tab in Databricks--2204260040007692 \", \"x\": 8.868218421936035, \"y\": 3.852656841278076}, {\"index\": 1526, \"title\": \"Follow up of- 00140935\", \"x\": 6.5127434730529785, \"y\": 3.5122804641723633}, {\"index\": 3160, \"title\": \"New endpoints pending acceptance from databricks\", \"x\": 10.123246192932129, \"y\": 2.1624863147735596}, {\"index\": 395, \"title\": \"MOVE dag is failing\", \"x\": 8.351505279541016, \"y\": 6.117014408111572}, {\"index\": 4135, \"title\": \"Usage Report Generation\", \"x\": 10.068086624145508, \"y\": 3.0956084728240967}, {\"index\": 813, \"title\": \"Unable to connect to PostgreSQL remote db\", \"x\": 10.329808235168457, \"y\": 3.5367496013641357}, {\"index\": 1143, \"title\": \"2204180040002686 | MyFedEx | ARR\", \"x\": 9.624452590942383, \"y\": 2.117182970046997}, {\"index\": 1035, \"title\": \"Use from_avro or to_avro with confluence\", \"x\": 9.613690376281738, \"y\": 3.7653605937957764}, {\"index\": 4893, \"title\": \"2319222617072725 - 66ed11d2-7a99-4f56-8bc6-20c797a75d74 - 2203080050002763\", \"x\": 10.296022415161133, \"y\": 2.8848812580108643}, {\"index\": 1823, \"title\": \"Permission Error on Sqla Cluster\", \"x\": 9.83647632598877, \"y\": 2.435056209564209}, {\"index\": 3220, \"title\": \"Cluster very very slow\", \"x\": 9.33175277709961, \"y\": 3.195612907409668}, {\"index\": 5535, \"title\": \"follow up ticket for 00135879\", \"x\": 7.9800705909729, \"y\": 6.243792533874512}, {\"index\": 2193, \"title\": \"Vulnerability \", \"x\": 5.871457576751709, \"y\": 1.449554681777954}, {\"index\": 1815, \"title\": \"Install Libraries from a private gitlab maven repository\", \"x\": 7.80845308303833, \"y\": 5.018363952636719}, {\"index\": 3181, \"title\": \"Spark Driver does not come up due to SLF4J ClassNotFound Exception\", \"x\": 10.626169204711914, \"y\": 3.373328685760498}, {\"index\": 1962, \"title\": \"SQL Endpoint: Failed to Load Tables Error\", \"x\": 5.89279842376709, \"y\": 3.640324115753174}, {\"index\": 2606, \"title\": \"2204150030000081 job failed 3 times with 3 different errors\", \"x\": 9.099939346313477, \"y\": 3.668795585632324}, {\"index\": 773, \"title\": \"ARR | 2205160040000885 - Resource pool management \", \"x\": 8.932703018188477, \"y\": 6.157137870788574}, {\"index\": 4687, \"title\": \"Need assistance in setting up Overwatch\", \"x\": 7.198130130767822, \"y\": 1.9444776773452759}, {\"index\": 2423, \"title\": \"2204200010000958\", \"x\": 5.981571197509766, \"y\": 4.858762264251709}, {\"index\": 4579, \"title\": \"Require new IP whitelisted\", \"x\": 6.887554168701172, \"y\": 0.8168670535087585}, {\"index\": 4775, \"title\": \"Vnet injection cluster trying to access some blob\", \"x\": 7.8468475341796875, \"y\": 1.8423606157302856}, {\"index\": 3300, \"title\": \"While writing data from databricks into ADLS files , storage ACL permission (Mask bit) is not inheriting properly\", \"x\": 4.653938293457031, \"y\": 1.777138590812683}, {\"index\": 3352, \"title\": \"Vacuum on Delta table failing continuously || 2202280040007216\", \"x\": 7.842269420623779, \"y\": 1.5033528804779053}, {\"index\": 201, \"title\": \"Job fails with \\\"An error occurred while trying to connect to the Java server (127.0.0.1:35291)\\\"\", \"x\": 7.034646034240723, \"y\": 0.7107044458389282}, {\"index\": 5275, \"title\": \"Please help to check if the query had been sent to Databricks(follow up ticket 00136461)\", \"x\": 7.682789325714111, \"y\": 4.120823860168457}, {\"index\": 2070, \"title\": \"Z-order data skipping not working under certain conditions\", \"x\": 8.221883773803711, \"y\": 5.150874137878418}, {\"index\": 1986, \"title\": \"Customer needs to delete 2 service principals from Standard Workspace\", \"x\": 8.582616806030273, \"y\": 4.244746685028076}, {\"index\": 2641, \"title\": \"ARR | Overwatch - Null values in jrcp table - workspace 7 - 2204180040004421\", \"x\": 5.8729472160339355, \"y\": 2.7594332695007324}, {\"index\": 4045, \"title\": \"JAR For io.delta.tables.DeltaTable\", \"x\": 9.87691879272461, \"y\": 3.2373907566070557}, {\"index\": 2886, \"title\": \"java.lang.IllegalStateException: The transaction log has failed integrity checks.\", \"x\": 8.105025291442871, \"y\": 4.3292012214660645}, {\"index\": 5414, \"title\": \"Increase the performance of a Streaming Job\", \"x\": 9.901847839355469, \"y\": 3.2358603477478027}, {\"index\": 4282, \"title\": \"ARR: Incorrect results.exception with SQL 'WHERE EXISTS' clause: SR2203180040004518 \", \"x\": 9.77449893951416, \"y\": 2.326843738555908}, {\"index\": 790, \"title\": \"Databricks cluster is running from past 6 months without shutting down\", \"x\": 9.699187278747559, \"y\": 3.1786727905273438}, {\"index\": 139, \"title\": \"Upgrading to DBR 10.4 on PVC brakes DELTA MERGE operation\", \"x\": 7.031614780426025, \"y\": 0.2871089279651642}, {\"index\": 1886, \"title\": \"Unable to delete users from https://accounts.cloud.databricks.com/users\", \"x\": 7.905670642852783, \"y\": 2.371917486190796}, {\"index\": 338, \"title\": \"cannot retrieve driver log\", \"x\": 7.516542911529541, \"y\": 2.316220760345459}, {\"index\": 260, \"title\": \"Databricks DBU consumption\", \"x\": 8.449858665466309, \"y\": 6.156066417694092}, {\"index\": 3821, \"title\": \"job failure/long running without change to job.\", \"x\": 7.544849395751953, \"y\": 1.550764799118042}, {\"index\": 719, \"title\": \"2205160030000602 | ARR | unable to mount adls in databricks\", \"x\": 7.414870738983154, \"y\": 4.061814308166504}, {\"index\": 5419, \"title\": \"ATT  - unable to see SQLANALYTICS logs after enabling the diagnostic settings \", \"x\": 7.920230388641357, \"y\": 2.941047191619873}, {\"index\": 3189, \"title\": \"Delta table not able to be modified\", \"x\": 8.451645851135254, \"y\": 4.140552520751953}, {\"index\": 376, \"title\": \"Parsing RDD with read_csv behavior changed between 7.3LTS and 10.4LTS\", \"x\": 8.469712257385254, \"y\": 2.635281801223755}, {\"index\": 2303, \"title\": \"Cannot create Databricks mws endpoints from terraform\", \"x\": 6.314130783081055, \"y\": 2.12117600440979}, {\"index\": 2594, \"title\": \"2204180040005100 | Job metrics\", \"x\": 8.157449722290039, \"y\": 2.513423204421997}, {\"index\": 3099, \"title\": \"ARR - Walmart - 2204080030002206 - RCA - Job failure\", \"x\": 5.912374973297119, \"y\": 3.130281448364258}, {\"index\": 302, \"title\": \"CSS-ARR-S500-SR#2205200040003401-Unable to Launch Cluster\", \"x\": 9.708213806152344, \"y\": 2.2536816596984863}, {\"index\": 1075, \"title\": \"2205100050002215\", \"x\": 10.238594055175781, \"y\": 5.288119792938232}, {\"index\": 4794, \"title\": \"Need to re-create my Databricks account, I think\", \"x\": 10.382256507873535, \"y\": 4.260255813598633}, {\"index\": 4377, \"title\": \"In-continue to 00128562, Autoloader takes 22 means to start the first batch\", \"x\": 9.272114753723145, \"y\": 2.6170668601989746}, {\"index\": 3317, \"title\": \"Rstudio 1.4 initialization failure\", \"x\": 7.743475914001465, \"y\": 5.108919143676758}, {\"index\": 362, \"title\": \"Workspaces are not reachable\", \"x\": 9.883269309997559, \"y\": 3.349573850631714}, {\"index\": 1565, \"title\": \"Job is running slower than usual and is failing with time out\", \"x\": 6.475162029266357, \"y\": 2.9804558753967285}, {\"index\": 3709, \"title\": \"PythonSecurityException when copying a file\", \"x\": 10.84140682220459, \"y\": 3.703139543533325}, {\"index\": 3267, \"title\": \"Undefined function: getArgument\", \"x\": 9.320701599121094, \"y\": 3.443781852722168}, {\"index\": 953, \"title\": \"AutoML fails with RetryError: HTTPSConnectionPool(host='nvirginia.cloud.databricks.com', port=443)\", \"x\": 9.416634559631348, \"y\": 3.965163230895996}, {\"index\": 481, \"title\": \"Databricks:  When clicking on completed jobs get HTTP ERROR 403\", \"x\": 5.628680229187012, \"y\": 1.3290235996246338}, {\"index\": 3558, \"title\": \"Microsoft Azure Deployment Acceptable Use Policy Violation [#SIR9501524]\", \"x\": 9.894917488098145, \"y\": 3.348031759262085}, {\"index\": 5510, \"title\": \"Inconsistent behavior when running Databricks Job\", \"x\": 6.947414398193359, \"y\": 5.514330863952637}, {\"index\": 2165, \"title\": \"Migrate to E2\", \"x\": 5.890720367431641, \"y\": 3.9676952362060547}, {\"index\": 5570, \"title\": \"Frankfurt Dataplane Issues \", \"x\": 8.290487289428711, \"y\": 4.894347190856934}, {\"index\": 4303, \"title\": \"Having s3 access denied errors in new workspace\", \"x\": 9.465375900268555, \"y\": 5.8981709480285645}, {\"index\": 5044, \"title\": \"MLflow Model Registry Webhooks on Databricks\", \"x\": 7.8584980964660645, \"y\": 4.575716972351074}, {\"index\": 709, \"title\": \"2205180030000703 how to find out the service principle/Azure Active Directory application used to mount a container\", \"x\": 4.99553918838501, \"y\": 1.7202187776565552}, {\"index\": 3245, \"title\": \"AND operator raising UnsupportedOperationException: dataType with LAG\", \"x\": 8.996010780334473, \"y\": 6.357362270355225}, {\"index\": 4199, \"title\": \"129372011637026 - 42c0910c-ba8d-4218-96f2-e8bbcfdb8dc0 - 2203040040004381\", \"x\": 9.67601203918457, \"y\": 3.712615489959717}, {\"index\": 4876, \"title\": \"Insert data to S3 bucket then Error occured while calling o696.parquet\\u3010Cx is from  Grabtaxi Holdings Pte Ltd \\u3011\", \"x\": 8.245336532592773, \"y\": 4.892481803894043}, {\"index\": 4288, \"title\": \"Databricks non-prod clusters not starting\", \"x\": 9.329360008239746, \"y\": 1.9843039512634277}, {\"index\": 130, \"title\": \"unpinned clusters disappearing after library installation in E2 workspace\", \"x\": 7.080108642578125, \"y\": 3.6724555492401123}, {\"index\": 4480, \"title\": \"Databricks Job Performance Issues\", \"x\": 8.516730308532715, \"y\": 3.348097562789917}, {\"index\": 3483, \"title\": \"2204040040002707\", \"x\": 10.151458740234375, \"y\": 3.117418050765991}, {\"index\": 3923, \"title\": \"Unable to open Spark UI\", \"x\": 6.0065507888793945, \"y\": 2.976870059967041}, {\"index\": 490, \"title\": \"ARR | 2205200040002676 - databricks overwatch proxy configuration issue\", \"x\": 6.971823215484619, \"y\": 0.4453507661819458}, {\"index\": 89, \"title\": \"CSS-ARR-S500-SR#2205300030000938-Registering the model using adls gen2 path\", \"x\": 4.603982925415039, \"y\": 1.7087534666061401}, {\"index\": 1635, \"title\": \"1083652405284390 - 42c0910c-ba8d-4218-96f2-e8bbcfdb8dc0 - 2205030040005851\", \"x\": 8.294851303100586, \"y\": 3.4300427436828613}, {\"index\": 1992, \"title\": \"PYPI libray installation failed\", \"x\": 9.266780853271484, \"y\": 4.465897083282471}, {\"index\": 2441, \"title\": \"ec2 fleets cluster pool doesn't follow the region specification \", \"x\": 8.271459579467773, \"y\": 4.977090358734131}, {\"index\": 351, \"title\": \"ARR- ATT- Clusters with GPU won't start- 2205200040006767\", \"x\": 5.987953186035156, \"y\": 4.050405025482178}, {\"index\": 3459, \"title\": \"PowerBI query fails to pull 1Gb fact table from Azure Databricks\", \"x\": 10.595708847045898, \"y\": 3.2501111030578613}, {\"index\": 4186, \"title\": \"Account Console details updation\", \"x\": 7.364593029022217, \"y\": 1.4167673587799072}, {\"index\": 4171, \"title\": \"2203110030001083\", \"x\": 5.912400722503662, \"y\": 4.191969871520996}, {\"index\": 4227, \"title\": \"Cant Add New  Instance Profile\", \"x\": 7.082813739776611, \"y\": 4.785334587097168}, {\"index\": 1942, \"title\": \"2567 - ARR - ExxonMobile - Unable to access external storage from DeltaLive pipeline\", \"x\": 6.43929386138916, \"y\": 3.1543126106262207}, {\"index\": 399, \"title\": \"Airflow Connectivity problem with databricks.\", \"x\": 4.518438816070557, \"y\": 1.7649919986724854}, {\"index\": 2858, \"title\": \"Concurrency Control - Clarification to fix the multiple update/insert to the table\", \"x\": 7.932382106781006, \"y\": 6.600356101989746}, {\"index\": 4744, \"title\": \"Download csv\", \"x\": 10.457621574401855, \"y\": 3.9818851947784424}, {\"index\": 5576, \"title\": \"Broadcast join's are not effecient on the job\", \"x\": 9.262935638427734, \"y\": 4.165182590484619}, {\"index\": 1049, \"title\": \"Starting 22 March, there is a substantial increase in cost of the DBFS storage account\", \"x\": 8.144577026367188, \"y\": 5.461350917816162}, {\"index\": 3119, \"title\": \"ARR Customer JNJ: Ganglia UI is not working & Metrics are not accessible.\", \"x\": 7.042385578155518, \"y\": 0.3049517571926117}, {\"index\": 1821, \"title\": \"Extreme slowness in the ETL jobs.\", \"x\": 8.156939506530762, \"y\": 3.6371419429779053}, {\"index\": 1100, \"title\": \"Using S3 bucket in another account for init script\", \"x\": 8.498373031616211, \"y\": 5.4638285636901855}, {\"index\": 2948, \"title\": \"Jobs failing with cluster time-out issue\", \"x\": 8.635436058044434, \"y\": 1.8836790323257446}, {\"index\": 4553, \"title\": \"Cannot execute SQL statements to the Metastore\", \"x\": 5.9550018310546875, \"y\": 4.211499214172363}, {\"index\": 2024, \"title\": \"Followup of SF ticket 00141611\", \"x\": 6.1492838859558105, \"y\": 2.4130282402038574}, {\"index\": 5498, \"title\": \"Notebook getting timed out\", \"x\": 8.016119003295898, \"y\": 4.292716026306152}, {\"index\": 5500, \"title\": \"Unable to start new job or interactive clusters\", \"x\": 8.812634468078613, \"y\": 2.320988178253174}, {\"index\": 421, \"title\": \"PowerBi Failing To Connect To Databricks Table and Fetch Data\", \"x\": 8.652351379394531, \"y\": 3.779273748397827}, {\"index\": 1401, \"title\": \"Backend service unavailable - Unable to see list of clusters\", \"x\": 7.915406703948975, \"y\": 5.346885681152344}, {\"index\": 2456, \"title\": \"DBFS files in s3\", \"x\": 9.489241600036621, \"y\": 3.905670166015625}, {\"index\": 2083, \"title\": \"Get stuck at \\\"running command\\\" or \\\"cancelling command\\\" with DB Web interface\", \"x\": 6.408809185028076, \"y\": 0.9699708819389343}, {\"index\": 963, \"title\": \"Library issue\", \"x\": 9.506128311157227, \"y\": 4.5597944259643555}, {\"index\": 1101, \"title\": \"ARR:PYODBC driver cannot open lib \\u2018ODBC Driver 17 for SQL server\\u2019:file not found(0) while connecting to Azure SQL. Encountered error msg intermittently:SR2205060040003815 \", \"x\": 7.135025501251221, \"y\": 0.41835588216781616}, {\"index\": 5442, \"title\": \"Routing via Azure backbone\", \"x\": 7.358572006225586, \"y\": 2.2203404903411865}, {\"index\": 4396, \"title\": \"Cluster not available while running the jobs\", \"x\": 7.246455669403076, \"y\": 6.0819830894470215}, {\"index\": 4811, \"title\": \"Jobs getting failed due to library compatibility issue\", \"x\": 9.399558067321777, \"y\": 3.8058974742889404}, {\"index\": 435, \"title\": \"SQL endpoint failing to start\", \"x\": 5.36690616607666, \"y\": 2.2003347873687744}, {\"index\": 2313, \"title\": \"2204220060001123 \", \"x\": 5.842895030975342, \"y\": 4.455624103546143}, {\"index\": 829, \"title\": \"2205160040006990 | Job Failure\", \"x\": 10.649227142333984, \"y\": 3.6081223487854004}, {\"index\": 788, \"title\": \"\\u30a2\\u30ab\\u30a6\\u30f3\\u30c8\\u30b3\\u30f3\\u30bd\\u30fc\\u30eb\\u306eSAML2.0\\u3092\\u5229\\u7528\\u3057\\u305fSSO\\u8a2d\\u5b9a\\u306e\\u5b9f\\u73fe\\u6027\\u306b\\u3064\\u3044\\u3066\", \"x\": 10.14031982421875, \"y\": 4.976146221160889}, {\"index\": 5382, \"title\": \"Our Databricks workspaces are down\", \"x\": 8.274242401123047, \"y\": 1.3489748239517212}, {\"index\": 2560, \"title\": \"Jobs failing\", \"x\": 5.920686721801758, \"y\": 3.1409366130828857}, {\"index\": 456, \"title\": \"Error when connecting PowerBI with Databricks\", \"x\": 8.221234321594238, \"y\": 3.0545997619628906}, {\"index\": 4550, \"title\": \"ARR | 2202180040005938 | Follow up to 00135021\", \"x\": 9.219674110412598, \"y\": 4.323849201202393}, {\"index\": 2073, \"title\": \"Can not Produce/Consume message from spark to Confluent Kafka\", \"x\": 8.852716445922852, \"y\": 3.941960334777832}, {\"index\": 3357, \"title\": \"Python Pandas generates com.databricks.rpc.RPCResponseTooLarge error\", \"x\": 9.914053916931152, \"y\": 4.9098005294799805}, {\"index\": 4057, \"title\": \"Unable to create a delta table from databricks\", \"x\": 5.645329475402832, \"y\": 4.409102916717529}, {\"index\": 1105, \"title\": \"Databricks Job Cancelled\", \"x\": 5.955655097961426, \"y\": 2.26070237159729}, {\"index\": 3765, \"title\": \"1.Databricks-Log contains no information about causing column in case of type mismatch during Synapse-Write\", \"x\": 4.826608180999756, \"y\": 1.7089347839355469}, {\"index\": 1770, \"title\": \"2205020030000110\", \"x\": 6.20849084854126, \"y\": 0.6858609318733215}, {\"index\": 398, \"title\": \"2205230030001141 | ARR | Cluster Deleted\", \"x\": 5.680088996887207, \"y\": 4.188604831695557}, {\"index\": 5072, \"title\": \"Details Billing Report\", \"x\": 9.540409088134766, \"y\": 4.08925199508667}, {\"index\": 797, \"title\": \"Not able to mount storage account\", \"x\": 9.25949764251709, \"y\": 2.8322463035583496}, {\"index\": 4552, \"title\": \"SparkException SQL statement error in Databricks\", \"x\": 7.020720958709717, \"y\": 0.3508499562740326}, {\"index\": 4639, \"title\": \"Global Init Err.\", \"x\": 8.26789665222168, \"y\": 2.888211965560913}, {\"index\": 3823, \"title\": \"Our jobs are failing with local_disk0 filling for the Autoloader jobs\", \"x\": 7.140084743499756, \"y\": 2.1400153636932373}, {\"index\": 1458, \"title\": \"POST and GET API calls for the custom entity xomuog_well are experiencing long latency issues. The API calls are originating from Microsoft Azure Databricks.  The databricks jobs that used to run in 20 minutes is now taking 18 hours\", \"x\": 8.194220542907715, \"y\": 2.7942936420440674}, {\"index\": 1664, \"title\": \"Getting error \\\"Shuffle partition number too small\\\" when converting from 7.3 to 9.1 LTS\", \"x\": 6.458089351654053, \"y\": 2.9805870056152344}, {\"index\": 1334, \"title\": \"CSS-ARR-SR#2204250050001410-Maximum execution context or notebook attachment limit reached\", \"x\": 8.306841850280762, \"y\": 1.7704030275344849}, {\"index\": 755, \"title\": \"whitelist\", \"x\": 7.388422012329102, \"y\": 5.197125434875488}, {\"index\": 65, \"title\": \"The Spark UI link not acccessible\", \"x\": 5.9514384269714355, \"y\": 1.7211568355560303}, {\"index\": 177, \"title\": \"CSS-ARR-S500-SR#2205250030000548-Auto assignment user as admin role\", \"x\": 6.96287727355957, \"y\": 5.528961181640625}, {\"index\": 3501, \"title\": \"Cannot use OpenAPI spec yaml from Job API in Swagger Editor.\", \"x\": 5.564939498901367, \"y\": 1.652043104171753}, {\"index\": 4862, \"title\": \"Streaming jobs needs to delete the states\", \"x\": 9.40626335144043, \"y\": 4.351075649261475}, {\"index\": 2505, \"title\": \"followup of SF ticket 00140328\", \"x\": 7.353554725646973, \"y\": 4.971078395843506}, {\"index\": 26, \"title\": \"gar for 2205160050001134 \", \"x\": 7.72445821762085, \"y\": 5.589958667755127}, {\"index\": 525, \"title\": \"5244115429641560 - 77c41218-a808-4734-a49d-b82f9244bc93 - 2205200040005272\", \"x\": 7.743818283081055, \"y\": 3.668294668197632}, {\"index\": 4313, \"title\": \"SQL Endpoint not starting\", \"x\": 9.604217529296875, \"y\": 4.356537818908691}, {\"index\": 2698, \"title\": \"Terraform Support for Catalog, Schema and Table Grants in Unity\", \"x\": 7.563448905944824, \"y\": 5.229929447174072}, {\"index\": 2870, \"title\": \"Control Plane Request Failure\", \"x\": 8.810409545898438, \"y\": 3.837585210800171}, {\"index\": 664, \"title\": \"unable to add instance profile in new workspace\", \"x\": 7.758725166320801, \"y\": 2.432598114013672}, {\"index\": 4818, \"title\": \"SSL handshake failed\", \"x\": 8.865758895874023, \"y\": 4.11102819442749}, {\"index\": 1221, \"title\": \"ARR Customer Safeway: Databricks Workspace is intermittently inaccessible. This affects them logging into the workspace and attempting to start their notebooks. The Main screen loads, but selecting a query fails to bring up that query. \", \"x\": 9.315119743347168, \"y\": 4.2811431884765625}, {\"index\": 1742, \"title\": \"Metastore access issues.\", \"x\": 7.04473352432251, \"y\": 0.3344372510910034}, {\"index\": 3678, \"title\": \"Not able to load metadata with SimbaSparkJDBC42-2.6.22.1040.jar file\", \"x\": 9.146799087524414, \"y\": 3.831531524658203}, {\"index\": 332, \"title\": \"\\u5916\\u90e8\\u30c6\\u30fc\\u30d6\\u30eb\\u4f5c\\u6210\\u6642\\u306b\\u5fc5\\u8981\\u306aIAM\\u30dd\\u30ea\\u30b7\\u30fc\\u306b\\u3064\\u3044\\u3066\", \"x\": 7.62169075012207, \"y\": 3.786769390106201}, {\"index\": 598, \"title\": \"SQL Analytics unable to run query\", \"x\": 6.705620288848877, \"y\": 4.26055383682251}, {\"index\": 2543, \"title\": \"Long Running job cluster's which are sitting Ideal in more then 600 hours \", \"x\": 8.982109069824219, \"y\": 4.848871231079102}, {\"index\": 5366, \"title\": \"Server error\", \"x\": 9.53056812286377, \"y\": 5.359958648681641}, {\"index\": 1459, \"title\": \"Discrepancy in output of same SQL query when run in notebook against in DBSQL editor\", \"x\": 5.821590900421143, \"y\": 4.769112586975098}, {\"index\": 4246, \"title\": \"Not all users present in 'users' Databricks group\", \"x\": 9.151422500610352, \"y\": 6.684133052825928}, {\"index\": 3571, \"title\": \"Billing account\", \"x\": 8.289016723632812, \"y\": 6.191281795501709}, {\"index\": 739, \"title\": \"Job failing in PROD\", \"x\": 9.859903335571289, \"y\": 2.9968509674072266}, {\"index\": 2416, \"title\": \"Databricks run page url is not opening in browser\", \"x\": 8.537262916564941, \"y\": 6.042020320892334}, {\"index\": 228, \"title\": \"ValueError: year 0 is out of range when trying to parse json schema using spark.read.json\", \"x\": 8.76272964477539, \"y\": 3.876229763031006}, {\"index\": 5335, \"title\": \"Databricks cluster not starting\", \"x\": 10.273069381713867, \"y\": 2.8874614238739014}, {\"index\": 498, \"title\": \" Connectivity problem using azure-event-hubs-spark\", \"x\": 9.673568725585938, \"y\": 6.016091823577881}, {\"index\": 7, \"title\": \"Disable delta feature\", \"x\": 9.983264923095703, \"y\": 3.025285243988037}, {\"index\": 1977, \"title\": \"Photon: SparkNoSuchElementException: Key  does not exist\", \"x\": 8.953227996826172, \"y\": 3.453420639038086}, {\"index\": 902, \"title\": \"ARR 2205040030001503 | empty 4B delta files generation\", \"x\": 7.900343418121338, \"y\": 5.120285511016846}, {\"index\": 2575, \"title\": \"MOVE Dag is failing \", \"x\": 6.573298454284668, \"y\": 5.362929344177246}, {\"index\": 3551, \"title\": \"Spar 3.2.1 DBR giving error on PartitionBy() command\", \"x\": 9.644037246704102, \"y\": 4.27471399307251}, {\"index\": 2538, \"title\": \"All users were logged out of databricks workspace and cannot log in\", \"x\": 7.9244184494018555, \"y\": 6.588465690612793}, {\"index\": 1237, \"title\": \"Unable to create a workspace through the Admin Console UI\", \"x\": 8.650134086608887, \"y\": 2.0929317474365234}, {\"index\": 2304, \"title\": \".Databricks-Log contains no information about causing column in case of type mismatch during Synapse-Write\", \"x\": 9.534049034118652, \"y\": 4.449443817138672}, {\"index\": 1932, \"title\": \"[ARR] [Sev C] SR-2204140030001199-Compute usage details gathering\", \"x\": 8.488941192626953, \"y\": 2.4698028564453125}, {\"index\": 4688, \"title\": \" Unable to access MLFlow artifact from notebook\", \"x\": 8.497669219970703, \"y\": 4.133467197418213}, {\"index\": 949, \"title\": \"Databricks UI Loading times\", \"x\": 7.873464107513428, \"y\": 2.6818137168884277}, {\"index\": 1717, \"title\": \"Sometimes, queries use Photon runtime run slower than one without Photon  \", \"x\": 7.948690414428711, \"y\": 4.775796890258789}, {\"index\": 2183, \"title\": \" databricks UI not loading\", \"x\": 8.8972806930542, \"y\": 2.866370439529419}, {\"index\": 2550, \"title\": \"Metastore is down and activity is stack\", \"x\": 5.665952205657959, \"y\": 1.7499672174453735}, {\"index\": 4406, \"title\": \"Logs, monitoring, and ssh\", \"x\": 10.227157592773438, \"y\": 2.978550910949707}, {\"index\": 138, \"title\": \"RStudio Error\", \"x\": 7.762779712677002, \"y\": 3.6897449493408203}, {\"index\": 4465, \"title\": \"ExecutorLostFailure: Remote RPC Client Disassociated\", \"x\": 9.892565727233887, \"y\": 2.176424264907837}, {\"index\": 2017, \"title\": \"CSS-ARR-2204260050002175- Backward compatibility support for time parser policy and more\", \"x\": 9.731465339660645, \"y\": 6.751728057861328}, {\"index\": 251, \"title\": \"Job gets stuck on \\u201cuploading command\\\" \", \"x\": 9.017843246459961, \"y\": 5.269326686859131}, {\"index\": 4549, \"title\": \"Unable to Configure IP Access List\", \"x\": 10.414261817932129, \"y\": 2.713728904724121}, {\"index\": 1090, \"title\": \"Dedup Issue\", \"x\": 4.544737815856934, \"y\": 1.7598756551742554}, {\"index\": 2016, \"title\": \"AttributeError: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'\", \"x\": 9.445233345031738, \"y\": 4.033754825592041}, {\"index\": 3669, \"title\": \"ARR | 2203310010001152 | Access Databricks workspace with private IP only\", \"x\": 6.965879440307617, \"y\": 0.42042770981788635}, {\"index\": 2849, \"title\": \"Cluster Launch Failure\", \"x\": 6.679441928863525, \"y\": 5.506257057189941}, {\"index\": 2430, \"title\": \"[ARR][2204070030000183][Databricks job not able to complete as the cluster config has reached the max capacity]\", \"x\": 10.057380676269531, \"y\": 3.2284560203552246}, {\"index\": 912, \"title\": \"TEMPORARILY_UNAVAILABLE: HTTP Response code: 504\", \"x\": 7.693055152893066, \"y\": 2.891420364379883}, {\"index\": 3084, \"title\": \"gar for 2204080030002269 \", \"x\": 7.1902923583984375, \"y\": 3.833225965499878}, {\"index\": 1281, \"title\": \"Issues connecting SQL Endpoint in R Shiny Application\", \"x\": 7.75915002822876, \"y\": 3.0873990058898926}, {\"index\": 4358, \"title\": \"2203030010002530\", \"x\": 4.471057891845703, \"y\": 1.795434832572937}, {\"index\": 2118, \"title\": \"ARR | 2204210050004881 | Load SQL server data into Databricks\", \"x\": 7.354701042175293, \"y\": 4.2274088859558105}, {\"index\": 911, \"title\": \"All jobs are failing\", \"x\": 8.992884635925293, \"y\": 6.786144733428955}, {\"index\": 5405, \"title\": \"Unable to access feature store with databricks-connect==9.1.* and Runtime ML 9.1\", \"x\": 8.999452590942383, \"y\": 1.8423629999160767}, {\"index\": 2158, \"title\": \"Structured Streaming stuck as \\\"Stream Initializing\\\"\", \"x\": 10.260519981384277, \"y\": 4.769721984863281}, {\"index\": 3231, \"title\": \"gar for 2203180050000275 \", \"x\": 8.715629577636719, \"y\": 3.8616364002227783}, {\"index\": 1089, \"title\": \"File read from Container is taking very long time\", \"x\": 6.545944690704346, \"y\": 1.4766613245010376}, {\"index\": 4951, \"title\": \"2203030040006502 - Intermittent Databricks error connecting to Storage - AzureADAuthenticator.getTokenCall threw javax.net.ssl.SSLException : Connection reset\", \"x\": 7.75260591506958, \"y\": 1.4172595739364624}, {\"index\": 3126, \"title\": \"2204050040001272\", \"x\": 8.403066635131836, \"y\": 1.7874329090118408}, {\"index\": 936, \"title\": \"Unable to write from Databricks into Synapse\", \"x\": 9.894232749938965, \"y\": 3.5866775512695312}, {\"index\": 4044, \"title\": \"Out of memory error in Databricks with R code\", \"x\": 10.388985633850098, \"y\": 3.410177230834961}, {\"index\": 5425, \"title\": \"Cluster Deletion before 30 days of inactivity\", \"x\": 9.104955673217773, \"y\": 3.224703550338745}, {\"index\": 1250, \"title\": \"Facing issue while connecting Databricks and Azure SQL DB using Service Principal as authentication mode as token got expired after 1 hr\", \"x\": 8.751531600952148, \"y\": 3.829841136932373}, {\"index\": 5068, \"title\": \"Error of Invalid token when access using Job API\", \"x\": 8.601783752441406, \"y\": 4.608564376831055}, {\"index\": 5329, \"title\": \"DDL error\", \"x\": 10.126365661621094, \"y\": 2.556257486343384}, {\"index\": 5311, \"title\": \"Customer requesting for few lightweight options in DSv2 series and Dv2 series VMs for Azure Databricks\", \"x\": 5.657867431640625, \"y\": 4.666745662689209}, {\"index\": 4652, \"title\": \"Unable to run any queries against production endpoint\", \"x\": 6.804564476013184, \"y\": 5.084878921508789}, {\"index\": 3272, \"title\": \"check how to enbale delta live table\", \"x\": 10.232549667358398, \"y\": 2.621904134750366}, {\"index\": 4117, \"title\": \"Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues\", \"x\": 8.027630805969238, \"y\": 5.101544380187988}, {\"index\": 3340, \"title\": \"Failed network validation checks when creating Workspace\", \"x\": 7.028429985046387, \"y\": 0.33033502101898193}, {\"index\": 4661, \"title\": \"Is there any way to update group entitlement via CLI or via API call\", \"x\": 5.838879108428955, \"y\": 3.1391918659210205}, {\"index\": 4168, \"title\": \"databricks account password reset\", \"x\": 5.6950602531433105, \"y\": 2.4009292125701904}, {\"index\": 3577, \"title\": \"gar for 2204010050001066 \", \"x\": 8.610578536987305, \"y\": 5.22665548324585}, {\"index\": 4401, \"title\": \"Getting error while trying to start the cluster : Instance was not reachable.\", \"x\": 5.748075485229492, \"y\": 1.696478247642517}, {\"index\": 1792, \"title\": \"2204270040000071\", \"x\": 9.836845397949219, \"y\": 2.498368263244629}, {\"index\": 4426, \"title\": \"Cannot create cluster in Databricks\", \"x\": 8.046876907348633, \"y\": 2.464647054672241}, {\"index\": 3219, \"title\": \"Cluster is not starting\", \"x\": 7.480042457580566, \"y\": 2.179250955581665}, {\"index\": 3720, \"title\": \"otebook stated failing with Invalid call to qualifier on unresolved object, tree:\", \"x\": 8.827579498291016, \"y\": 3.7969868183135986}, {\"index\": 3191, \"title\": \"Max request rate for EC2 causing job failure\", \"x\": 7.04150390625, \"y\": 0.2763526737689972}, {\"index\": 2914, \"title\": \"Kafka GroupID and load Delta table as a stream source \", \"x\": 10.333428382873535, \"y\": 2.4491870403289795}, {\"index\": 4510, \"title\": \"Needed help in setting up Delta Sharing for this cluster\", \"x\": 8.603561401367188, \"y\": 3.319103717803955}, {\"index\": 5316, \"title\": \"R libraries failing for clusters\", \"x\": 6.193262577056885, \"y\": 4.076014518737793}, {\"index\": 4276, \"title\": \"Need some info regarding datalake time travel\", \"x\": 8.84328556060791, \"y\": 5.857430934906006}, {\"index\": 2418, \"title\": \"Not able to use Pandas function\", \"x\": 8.896774291992188, \"y\": 4.859445095062256}, {\"index\": 4484, \"title\": \"Whitelist ipaddres in Access list for feature store\", \"x\": 6.368117332458496, \"y\": 4.386782646179199}, {\"index\": 892, \"title\": \"2205130030000118  Cluster is running but 1 containers could not be added.Reason:Cloud provider launch failure\", \"x\": 8.181120872497559, \"y\": 4.878627777099609}, {\"index\": 5499, \"title\": \"ARR Customer: AT&T - Commands Not Working on the Databricks Cluster. \", \"x\": 5.774289608001709, \"y\": 1.8002911806106567}, {\"index\": 5279, \"title\": \"py4j.security.Py4JSecurityException\", \"x\": 7.01707649230957, \"y\": 3.8140201568603516}, {\"index\": 2761, \"title\": \"need help creating second workspace\", \"x\": 8.210162162780762, \"y\": 6.074872970581055}, {\"index\": 1725, \"title\": \"cifs_utils / mounting windows network file share fails\", \"x\": 8.758655548095703, \"y\": 1.7847399711608887}, {\"index\": 3933, \"title\": \"Import notebook from Qubole exported as JSON\", \"x\": 6.196702003479004, \"y\": 0.7151298522949219}, {\"index\": 2051, \"title\": \"2204270040000271 \", \"x\": 8.840484619140625, \"y\": 4.163848400115967}, {\"index\": 5050, \"title\": \"Notebook not found error\", \"x\": 4.805389881134033, \"y\": 1.7667016983032227}, {\"index\": 3697, \"title\": \"Cost analysis is associating VMs to wrong clusters\", \"x\": 7.107662200927734, \"y\": 0.3427412211894989}, {\"index\": 4481, \"title\": \"SQL Endpoint not starting \", \"x\": 7.644731044769287, \"y\": 3.4564406871795654}, {\"index\": 2501, \"title\": \"ARR | job api limit | 2204190040001460 | Capital Group\", \"x\": 6.569895267486572, \"y\": 2.7262766361236572}, {\"index\": 2046, \"title\": \"Attribute Error: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK''\", \"x\": 6.178529262542725, \"y\": 1.3585069179534912}, {\"index\": 2463, \"title\": \"Databricks to JDBC Using SSH Tunnel\", \"x\": 6.144737720489502, \"y\": 0.6954077482223511}, {\"index\": 403, \"title\": \"Followup of SF ticket 00145109\", \"x\": 10.340824127197266, \"y\": 2.331810474395752}, {\"index\": 5502, \"title\": \"ARR Customer: AT&T - Is DBR GPU 9.1 compatible with ESRI Big Data ToolKit?\", \"x\": 7.5424885749816895, \"y\": 1.2712249755859375}, {\"index\": 50, \"title\": \"2205180030000254 \", \"x\": 7.025357723236084, \"y\": 0.2674614489078522}, {\"index\": 2610, \"title\": \"ARR | 2204170050000058 | Oracle Connectivity Issue\", \"x\": 8.937921524047852, \"y\": 6.320217609405518}, {\"index\": 2589, \"title\": \"Production clusters wont launch\", \"x\": 9.72746753692627, \"y\": 3.098109483718872}, {\"index\": 3075, \"title\": \"[ARR] [Sev A] SR-2204100050000057 continue to #00141395 Errors execution Databricks jobs due to AZURE_RESOURCE_PROVIDER_THROTTLING\", \"x\": 6.0417633056640625, \"y\": 5.080226421356201}, {\"index\": 558, \"title\": \"GAR for 6036272996126062\", \"x\": 9.873507499694824, \"y\": 2.898402214050293}, {\"index\": 5578, \"title\": \"Databricks cluster issues\", \"x\": 9.470765113830566, \"y\": 5.601625919342041}, {\"index\": 3801, \"title\": \"All newly registered users  can only access one of the two Databricks workspaces\", \"x\": 7.116593837738037, \"y\": 4.114736557006836}, {\"index\": 2534, \"title\": \"Workspace request R package installation\", \"x\": 5.650138854980469, \"y\": 1.6574722528457642}, {\"index\": 1151, \"title\": \"ARR | Databricks permissions issue\", \"x\": 8.927154541015625, \"y\": 3.446399211883545}, {\"index\": 9, \"title\": \"How to export Hive metastore \", \"x\": 4.522612571716309, \"y\": 1.7532179355621338}, {\"index\": 1707, \"title\": \"numpy failed to import in notebook\", \"x\": 10.386000633239746, \"y\": 4.06305456161499}, {\"index\": 3475, \"title\": \"jobs take longer than expected\", \"x\": 7.662344932556152, \"y\": 3.011805295944214}, {\"index\": 1614, \"title\": \"2205040030000575 \", \"x\": 9.905056953430176, \"y\": 2.547266960144043}, {\"index\": 1071, \"title\": \"Follow up of 00139396\", \"x\": 9.474685668945312, \"y\": 6.50262975692749}, {\"index\": 5506, \"title\": \"2112160050001503 filter fields saved in log analytics using spark monitoring lbrary\", \"x\": 7.101721286773682, \"y\": 4.9017863273620605}, {\"index\": 5415, \"title\": \"[URGENT] Shard become very slow\", \"x\": 9.800912857055664, \"y\": 4.980085849761963}, {\"index\": 5328, \"title\": \"SQL Endpoints failed to start\", \"x\": 9.558303833007812, \"y\": 2.8876616954803467}, {\"index\": 716, \"title\": \"ARR| Adobe | Job api 2.1 response schema doesn't match document |SR: 2205180010000639 \", \"x\": 4.509766578674316, \"y\": 1.6942520141601562}, {\"index\": 1896, \"title\": \"Issue in connecting to AWS Postgres Server proxy using SSL\", \"x\": 6.521350383758545, \"y\": 1.1012725830078125}, {\"index\": 5082, \"title\": \"Python package error\", \"x\": 9.731762886047363, \"y\": 2.821263551712036}, {\"index\": 2123, \"title\": \"Runtime exception (Read timed out) while connecting to COSMOS\", \"x\": 4.596395969390869, \"y\": 1.7218235731124878}, {\"index\": 318, \"title\": \"Add Comment to Column Create Table AS\", \"x\": 6.598673343658447, \"y\": 3.713547468185425}, {\"index\": 3159, \"title\": \"Issue on Spark scheduler pools for streaming job\", \"x\": 4.509401321411133, \"y\": 1.7779955863952637}, {\"index\": 1672, \"title\": \"Read JSON/CSV files  from a Bitbucket repository that is directly connected to Databricks.\", \"x\": 5.5769758224487305, \"y\": 3.9141342639923096}, {\"index\": 3353, \"title\": \"Databricks Cluster restart\", \"x\": 8.85923957824707, \"y\": 6.323782920837402}, {\"index\": 427, \"title\": \"Connection error during python library installation\", \"x\": 8.505739212036133, \"y\": 2.79636549949646}, {\"index\": 3256, \"title\": \"Failed installation of event hub connector library from Maven repo in databricks cluster.\", \"x\": 9.713841438293457, \"y\": 3.329005241394043}, {\"index\": 546, \"title\": \"Follow up of - 00146348\", \"x\": 9.642035484313965, \"y\": 6.270324230194092}, {\"index\": 262, \"title\": \"SQL Data slow loading\", \"x\": 7.102869987487793, \"y\": 0.3159124553203583}, {\"index\": 254, \"title\": \"cannot start cluster on   northcentralus\", \"x\": 6.84354829788208, \"y\": 5.502453804016113}, {\"index\": 666, \"title\": \"Delta Sharing Issue\", \"x\": 6.871659755706787, \"y\": 4.296592712402344}, {\"index\": 5295, \"title\": \"Issues launching photon clusters\", \"x\": 5.720259189605713, \"y\": 3.414241075515747}, {\"index\": 171, \"title\": \" Notebook showing function does not exist\", \"x\": 9.302464485168457, \"y\": 3.978569269180298}, {\"index\": 1176, \"title\": \"File read from Container is taking very long time\", \"x\": 8.383979797363281, \"y\": 3.7282419204711914}, {\"index\": 3999, \"title\": \"AutoML Forcast\", \"x\": 10.13126277923584, \"y\": 2.237086057662964}, {\"index\": 1482, \"title\": \"All jobs in dx2 shard fail as Cancelled\", \"x\": 7.460622310638428, \"y\": 1.8999327421188354}, {\"index\": 1906, \"title\": \"display(dataframe) never finishes \", \"x\": 9.721275329589844, \"y\": 2.911905527114868}, {\"index\": 3639, \"title\": \"Select * on tables giving error\", \"x\": 10.442943572998047, \"y\": 3.8493359088897705}, {\"index\": 3610, \"title\": \"2204010030001163\", \"x\": 8.413844108581543, \"y\": 3.6137442588806152}, {\"index\": 1900, \"title\": \"Cluster startup takes more than 10mins\", \"x\": 9.011482238769531, \"y\": 4.630431175231934}, {\"index\": 693, \"title\": \"Spark job stalls\", \"x\": 6.454178810119629, \"y\": 0.8414576649665833}, {\"index\": 2852, \"title\": \"Would it be possible to have a deterministic hash based on the code inside a UDF?\", \"x\": 8.656854629516602, \"y\": 3.5610125064849854}, {\"index\": 4879, \"title\": \"Issue while parsing csv in databricks\", \"x\": 4.441527366638184, \"y\": 1.7246125936508179}, {\"index\": 3137, \"title\": \"Follow Up  SF 00141036\", \"x\": 9.156397819519043, \"y\": 4.484749794006348}, {\"index\": 2860, \"title\": \"2793632992259305 - e756ddea-d52c-404b-abea-81ee9b4d9164 - 2204130040006794\", \"x\": 7.847895622253418, \"y\": 4.814609050750732}, {\"index\": 2115, \"title\": \"Databricks Cluster - Commands are getting cancelled\", \"x\": 7.954544544219971, \"y\": 2.8379971981048584}, {\"index\": 4026, \"title\": \"Databricks notebook performance is degraded\", \"x\": 9.236573219299316, \"y\": 3.8999855518341064}, {\"index\": 2682, \"title\": \"\\\"java.lang.IllegalStateException: Connection pool shut down\\\" when my job executes spark.sql\", \"x\": 5.7999396324157715, \"y\": 4.178163528442383}, {\"index\": 781, \"title\": \"Databricks API support\", \"x\": 5.058956146240234, \"y\": 1.600217580795288}, {\"index\": 3127, \"title\": \"Running a Python script in Databricks that currently is in local computer\", \"x\": 7.507063388824463, \"y\": 5.9004364013671875}, {\"index\": 4607, \"title\": \" Need to increase quota\", \"x\": 10.521764755249023, \"y\": 4.050925254821777}, {\"index\": 3882, \"title\": \"DataBricks queries are failing\", \"x\": 10.318530082702637, \"y\": 4.120614051818848}, {\"index\": 541, \"title\": \"Unable to see the SQL persona in Databricks instances\", \"x\": 6.007979393005371, \"y\": 3.078770637512207}, {\"index\": 3828, \"title\": \"Cluster in a hung state\", \"x\": 10.338521957397461, \"y\": 5.028552055358887}, {\"index\": 3073, \"title\": \"2204050050002744 Spark job stuck in pending\", \"x\": 9.810023307800293, \"y\": 2.299736976623535}, {\"index\": 4740, \"title\": \"ARR 2203140050000737- Cluster unable to start with Docker Image\", \"x\": 6.554931640625, \"y\": 1.793010950088501}, {\"index\": 5448, \"title\": \"ARR | 2203020060001019 | The job failed intermittently due to ModuleNotFoundError though the library is installed\", \"x\": 7.109279632568359, \"y\": 0.26431190967559814}, {\"index\": 882, \"title\": \"Enabling EBS SSD GP3 in Workspace setting is picking throughput 128MBps by default instead of 125Mbps \", \"x\": 10.248119354248047, \"y\": 5.2876505851745605}, {\"index\": 2925, \"title\": \"2203290040001161 | Delta table\", \"x\": 5.214939117431641, \"y\": 1.4614919424057007}, {\"index\": 2654, \"title\": \"Init script failed\", \"x\": 6.725369453430176, \"y\": 3.942582368850708}, {\"index\": 4332, \"title\": \"gpu utilisation tracking \", \"x\": 9.527554512023926, \"y\": 4.018329620361328}, {\"index\": 5536, \"title\": \"[ARR] [Sev B] SR-2202230030000040 continue to #00135454 Job fails after hive metastore migration\", \"x\": 6.386579990386963, \"y\": 4.122242450714111}, {\"index\": 5071, \"title\": \"ARR - Adobe - 2202230010001928 - Cluster failure due to throttling\", \"x\": 9.449750900268555, \"y\": 3.8878161907196045}, {\"index\": 1171, \"title\": \"Follow Up 00145019\", \"x\": 9.903471946716309, \"y\": 5.3253865242004395}, {\"index\": 2983, \"title\": \"[ARR] [Sev B] SR-2204100040000077 RCA:Unable to acquire lock to files from ADLS on Databricks\", \"x\": 7.082840442657471, \"y\": 0.7550384402275085}, {\"index\": 1329, \"title\": \"Unable to register a scaler model built on my training data | ARR SR# 2205060030001100\", \"x\": 8.389906883239746, \"y\": 3.129279375076294}, {\"index\": 5443, \"title\": \"Azure databricks pipeline issue post migration\", \"x\": 8.265726089477539, \"y\": 3.6152307987213135}, {\"index\": 5416, \"title\": \"Databricks Job \", \"x\": 7.149028301239014, \"y\": 1.5111600160598755}, {\"index\": 75, \"title\": \"Notebook commands get hung on spark.read; Spark Job gets initiated but commands get stuck indefinitely.\", \"x\": 10.576948165893555, \"y\": 3.007951259613037}, {\"index\": 3940, \"title\": \"Databricks SQL Endpoint unable to connect to external metastore\", \"x\": 7.182312965393066, \"y\": 3.6188549995422363}, {\"index\": 4278, \"title\": \"Job failing with com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\", \"x\": 7.193972587585449, \"y\": 1.4645663499832153}, {\"index\": 3931, \"title\": \"Unable to set up new support users\", \"x\": 10.452481269836426, \"y\": 3.2354230880737305}, {\"index\": 2194, \"title\": \"Vulnerability Issue in nodes of databricks cluster\", \"x\": 6.197696208953857, \"y\": 3.763601064682007}, {\"index\": 1466, \"title\": \"ARR | 2204270030002047 - [Needs IST Support] Resource accidental deletion\", \"x\": 8.77125358581543, \"y\": 4.875870704650879}, {\"index\": 2387, \"title\": \"Need storage analysis\", \"x\": 6.74051570892334, \"y\": 2.5536437034606934}, {\"index\": 5201, \"title\": \"test \", \"x\": 9.019942283630371, \"y\": 4.432346343994141}, {\"index\": 3066, \"title\": \"2204080010002780 | ARR | Seeing metastore and DBFS down on eventlogs  frequently\", \"x\": 6.535228252410889, \"y\": 1.867851972579956}, {\"index\": 3733, \"title\": \"Access S3 buckets from AWS different accounts\", \"x\": 7.6092915534973145, \"y\": 3.647582530975342}, {\"index\": 945, \"title\": \"2205130030001572 | Cluster CRUD\", \"x\": 5.633208751678467, \"y\": 2.385056495666504}, {\"index\": 3069, \"title\": \"[ARR] [Sev B] SR-2204080030001854 - ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 157195 ms\", \"x\": 7.754177570343018, \"y\": 2.4740631580352783}, {\"index\": 5477, \"title\": \"user access \", \"x\": 8.119941711425781, \"y\": 4.958445072174072}, {\"index\": 842, \"title\": \"Solution for local unit test delta write\", \"x\": 7.989779472351074, \"y\": 5.686316967010498}, {\"index\": 3913, \"title\": \"Error occurred: 'Owner resource does not exist' during writing ttl property to cosmos documents for purging.\", \"x\": 8.735883712768555, \"y\": 6.668022155761719}, {\"index\": 3667, \"title\": \"ARR - AT&T - 2203290040006483 - Transformation jobs needed bigger clusters.. Need analysis\", \"x\": 10.611161231994629, \"y\": 4.691107749938965}, {\"index\": 4226, \"title\": \"Cluster regularly fails due to Spark exception received from driver. Driver down\", \"x\": 7.894162178039551, \"y\": 1.7856640815734863}, {\"index\": 3032, \"title\": \"Job failuer\", \"x\": 7.736750602722168, \"y\": 4.400062084197998}, {\"index\": 3888, \"title\": \"2203160030001363\", \"x\": 8.692740440368652, \"y\": 3.387115001678467}, {\"index\": 225, \"title\": \"Error while migrating database from hive to glue\", \"x\": 8.73371410369873, \"y\": 3.561922788619995}, {\"index\": 4408, \"title\": \"Databricks Cloud Provider Launch Failure\", \"x\": 7.630672931671143, \"y\": 1.5993289947509766}, {\"index\": 1856, \"title\": \"CSS-ARR-2204190030001595_Cluster unable to accquire additional worker nodes in upscaling\", \"x\": 9.778876304626465, \"y\": 6.814291954040527}, {\"index\": 4364, \"title\": \"Databricks hive meta table permission\", \"x\": 7.717555046081543, \"y\": 3.8611812591552734}, {\"index\": 845, \"title\": \"/etc path permission issue in DBFS for Unravel\", \"x\": 7.851978302001953, \"y\": 1.4718717336654663}, {\"index\": 572, \"title\": \"The spark driver has stopped unexpectedly and seems like a memory issue\", \"x\": 6.360783100128174, \"y\": 1.4477652311325073}, {\"index\": 4198, \"title\": \"Sparklyr and createOrReplaceTempView\", \"x\": 10.104656219482422, \"y\": 3.8814611434936523}, {\"index\": 4177, \"title\": \"Unable to connect from Tableau to Databricks in Azure\", \"x\": 8.507551193237305, \"y\": 5.493711948394775}, {\"index\": 4312, \"title\": \"Databricks Job Hanging Tasks\", \"x\": 9.847692489624023, \"y\": 4.363241195678711}, {\"index\": 154, \"title\": \"CSS-ARR-SR#2205240030000602-Support partial CSV File reading using Copy Into command\", \"x\": 9.680554389953613, \"y\": 3.9202427864074707}, {\"index\": 476, \"title\": \"GAR for 2205230030000890 \", \"x\": 7.703708648681641, \"y\": 1.7170978784561157}, {\"index\": 3388, \"title\": \"Databricks multi-task jobs are failing in an inconsistent fashion\", \"x\": 8.607609748840332, \"y\": 4.760937690734863}, {\"index\": 2991, \"title\": \"2204110060001139 \", \"x\": 9.37653923034668, \"y\": 4.363671779632568}, {\"index\": 1690, \"title\": \"Facing \\\"PERMISSION_DENIED\\\" error while configuring audit log delivery\", \"x\": 10.196295738220215, \"y\": 4.267278671264648}, {\"index\": 278, \"title\": \"[ARR][Public Analytics][2205250030002283 ] Missing Notebooks/Folders under FIONA/RBS in DEV ADB03\", \"x\": 8.799263954162598, \"y\": 3.7802226543426514}, {\"index\": 5033, \"title\": \"TWN | SEV A | Azure Databricks | Azure Data Bricks\\u7121\\u6cd5\\u5efa\\u5236\", \"x\": 7.70196533203125, \"y\": 4.64497184753418}, {\"index\": 5558, \"title\": \"need to know affect on Change in Databricks Token Authentication Error Message\", \"x\": 9.074899673461914, \"y\": 2.243699550628662}, {\"index\": 1087, \"title\": \"Databricks HC Cluster is running extremely slowly(Similar to 00145865)\", \"x\": 8.545402526855469, \"y\": 4.200467109680176}, {\"index\": 2632, \"title\": \"SQL endpoints are not working\", \"x\": 8.560068130493164, \"y\": 2.580899953842163}, {\"index\": 382, \"title\": \"Databricks Prod Issue Caused by SparkException\", \"x\": 10.191195487976074, \"y\": 5.3189215660095215}, {\"index\": 1099, \"title\": \"Databricks notebook performance is degraded\", \"x\": 8.221489906311035, \"y\": 4.788789749145508}, {\"index\": 3777, \"title\": \"ARR - No space left on device\", \"x\": 9.814988136291504, \"y\": 4.061407566070557}, {\"index\": 4030, \"title\": \"ARR |  NielsenIQ Coke Mastertrack - Cluster failure\", \"x\": 8.584298133850098, \"y\": 4.194830417633057}, {\"index\": 5287, \"title\": \"Spikes in the use Databricks resources at the time of scheduled system downtimes\", \"x\": 6.00578498840332, \"y\": 4.420540809631348}, {\"index\": 4144, \"title\": \"gar for  80d3e371-b095-44a1-821c-4e853285bc4f\", \"x\": 8.467517852783203, \"y\": 3.7347145080566406}, {\"index\": 918, \"title\": \"DBR version 10.X LTS DOES NOT WORK properly with Ranger\", \"x\": 8.888833045959473, \"y\": 4.606257438659668}, {\"index\": 4019, \"title\": \"2203250010002390 | package failure\", \"x\": 7.182622909545898, \"y\": 5.9717206954956055}, {\"index\": 3319, \"title\": \"Notebook does not execute.\", \"x\": 9.766925811767578, \"y\": 6.841662883758545}, {\"index\": 4883, \"title\": \"Azure Python SDK from azure.keyvault.keys.aio import KeyClient not able to fit into customer class\", \"x\": 9.662646293640137, \"y\": 3.8472213745117188}, {\"index\": 2922, \"title\": \"Cluster Fails to Start\", \"x\": 7.864184856414795, \"y\": 3.7582597732543945}, {\"index\": 297, \"title\": \"configuring delta cache for cluster\", \"x\": 7.741856575012207, \"y\": 5.21127986907959}, {\"index\": 4581, \"title\": \"Facing issue while running MLFLOW model using UDF on worker node.\", \"x\": 4.6118388175964355, \"y\": 1.7485599517822266}, {\"index\": 527, \"title\": \"Root cause for slow jobs\", \"x\": 7.48776912689209, \"y\": 4.206979751586914}, {\"index\": 4901, \"title\": \"Follow up case: 00134030. Driver was getting out of memory. The Ganglia UI showed an increase in memory, after running the job for certain days.\", \"x\": 9.958194732666016, \"y\": 2.816800117492676}, {\"index\": 442, \"title\": \"ARR Customer Engie: Issue - Problems accessing Databricks Workspace\", \"x\": 7.030128002166748, \"y\": 3.728821039199829}, {\"index\": 2120, \"title\": \"[ARR] [Sev B] SR-2204220060000915 continue to 00138803 - Mount_Azure_File sync for \", \"x\": 6.377267360687256, \"y\": 3.1746037006378174}, {\"index\": 1138, \"title\": \"Estimating required number of memory\", \"x\": 9.609405517578125, \"y\": 5.932950973510742}, {\"index\": 4457, \"title\": \"ARR | When trying to copy data from ADLS Gen2 to SQL DWH(Azure Synapse) we get the error with max char length issue \", \"x\": 8.72716236114502, \"y\": 5.193109035491943}, {\"index\": 3022, \"title\": \"xgboost==0.6a1 installation failing in in DBR 7.3 LTS ML\", \"x\": 7.060206413269043, \"y\": 4.464334487915039}, {\"index\": 4247, \"title\": \"dataset refresh failing\", \"x\": 10.149362564086914, \"y\": 5.617525100708008}, {\"index\": 2932, \"title\": \"A job that typically runs for 6 minutes only, ran for 58 hours this time\", \"x\": 7.849203109741211, \"y\": 4.520159721374512}, {\"index\": 2374, \"title\": \"Job Failing Databricks\", \"x\": 5.657830715179443, \"y\": 4.254214286804199}, {\"index\": 3085, \"title\": \" Databricks environment highly unstable\", \"x\": 8.24802017211914, \"y\": 2.26259183883667}, {\"index\": 4485, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2203180040005316\", \"x\": 8.149063110351562, \"y\": 2.844669818878174}, {\"index\": 1836, \"title\": \"Customer's job is facing Could not find ADLS Gen2 Token issue\", \"x\": 7.98874568939209, \"y\": 1.8429172039031982}, {\"index\": 2615, \"title\": \"Tags for instances in the pool\", \"x\": 9.346582412719727, \"y\": 3.9852981567382812}, {\"index\": 2008, \"title\": \"migrating workspace to different VPC and subnets \", \"x\": 9.29686164855957, \"y\": 4.523563385009766}, {\"index\": 3116, \"title\": \"Facing issue while trying to connect bigquery using jdbc from databricks notebook\", \"x\": 8.414600372314453, \"y\": 4.6863555908203125}, {\"index\": 47, \"title\": \"Logs delivery just before application shutdown\", \"x\": 7.117269992828369, \"y\": 1.315520167350769}, {\"index\": 2201, \"title\": \"CSS ARR | 2204040030002558 - Unexpected cost spike\", \"x\": 8.807263374328613, \"y\": 2.8056082725524902}, {\"index\": 3409, \"title\": \"unable to establish AAD authentication between Databricks and Synapse DW\", \"x\": 7.035120010375977, \"y\": 0.24454209208488464}, {\"index\": 397, \"title\": \"Ganglia stops working\", \"x\": 7.278564453125, \"y\": 5.999288558959961}, {\"index\": 1507, \"title\": \"Autoloader cloudfiles events by creation timestamp\", \"x\": 6.723773956298828, \"y\": 1.5434623956680298}, {\"index\": 2838, \"title\": \"Facing memory issue while performing ETL using Spark scripts\", \"x\": 8.010497093200684, \"y\": 3.8557302951812744}, {\"index\": 2726, \"title\": \"[ARR] Cluster starting failure\", \"x\": 6.716279029846191, \"y\": 1.5775359869003296}, {\"index\": 5205, \"title\": \"Getting failures for data type mismatch\", \"x\": 9.603952407836914, \"y\": 4.263683795928955}, {\"index\": 4296, \"title\": \"Single Node Cluster not starting\", \"x\": 8.86832332611084, \"y\": 2.6036908626556396}, {\"index\": 3108, \"title\": \"Streaming job failing in databricks\", \"x\": 7.713605880737305, \"y\": 1.4654604196548462}, {\"index\": 4962, \"title\": \"CSS-SFMC-2202280030000859-SparkOutOfMemoryError Issue\", \"x\": 9.715855598449707, \"y\": 3.171182632446289}, {\"index\": 986, \"title\": \"Job were running slow and few nodes getting skipped\", \"x\": 7.016116619110107, \"y\": 0.27258914709091187}, {\"index\": 4036, \"title\": \"Auto logout Issue\", \"x\": 6.173739433288574, \"y\": 2.6282413005828857}, {\"index\": 5112, \"title\": \"Databricks to ECS connectivity issue\", \"x\": 7.9388861656188965, \"y\": 3.4861316680908203}, {\"index\": 4013, \"title\": \"The spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\", \"x\": 4.817343711853027, \"y\": 1.7185322046279907}, {\"index\": 5215, \"title\": \"ARR -  Pyspark XGBoost Produce Inconsistent Result-  2202220040008191 \", \"x\": 5.9292497634887695, \"y\": 3.7001335620880127}, {\"index\": 237, \"title\": \"Stream Ingestion cloudfiles failure\", \"x\": 9.924745559692383, \"y\": 2.2945332527160645}, {\"index\": 2119, \"title\": \"Databricks merge delta table failing\", \"x\": 5.623132228851318, \"y\": 1.604797601699829}, {\"index\": 5460, \"title\": \"0055 - g - ARR - ATT - Streaming job failed\", \"x\": 9.798354148864746, \"y\": 3.1570279598236084}, {\"index\": 4632, \"title\": \"gar for 2203150040007401 \", \"x\": 8.64065933227539, \"y\": 4.394454479217529}, {\"index\": 1104, \"title\": \"Divergency of jobs status\", \"x\": 4.525590419769287, \"y\": 1.8047077655792236}, {\"index\": 3197, \"title\": \"unable to cancel job run\", \"x\": 7.013298511505127, \"y\": 0.2698560059070587}, {\"index\": 802, \"title\": \"Jobs are failing after changing DBR Version\", \"x\": 4.841488838195801, \"y\": 1.7125424146652222}, {\"index\": 4814, \"title\": \"SQL table shows \\\"NaN\\\" for too small values\", \"x\": 7.522861003875732, \"y\": 3.23252534866333}, {\"index\": 5365, \"title\": \"The internal Metastore is down\", \"x\": 8.778485298156738, \"y\": 4.164692401885986}, {\"index\": 3044, \"title\": \"Accessing AWS Athena from databricks\", \"x\": 8.573864936828613, \"y\": 4.328551769256592}, {\"index\": 3584, \"title\": \"Vulnerability \", \"x\": 9.722990989685059, \"y\": 6.885532855987549}, {\"index\": 4249, \"title\": \"Follow up of( 00137929) I get an error when doing division using DirectQuery to Azure Databricks.\", \"x\": 8.062239646911621, \"y\": 4.284276485443115}, {\"index\": 1737, \"title\": \"Need Help with Seperate work Spaces in the same accout?\", \"x\": 7.438563823699951, \"y\": 2.3201510906219482}, {\"index\": 5432, \"title\": \"One extra column being read by databricks called 'region' since yesterday\", \"x\": 10.530207633972168, \"y\": 2.5529966354370117}, {\"index\": 2947, \"title\": \"Databricks jobs failing in production\", \"x\": 8.946403503417969, \"y\": 2.326852798461914}, {\"index\": 4902, \"title\": \"MLFlow model artifact access\", \"x\": 9.034610748291016, \"y\": 4.227409839630127}, {\"index\": 3748, \"title\": \"Job aborted due to stage failure\", \"x\": 6.971611499786377, \"y\": 0.9124013185501099}, {\"index\": 34, \"title\": \"databricks-connect with token is deprecated\", \"x\": 8.07766342163086, \"y\": 2.599118232727051}, {\"index\": 2245, \"title\": \"2754134964726624 - 81b4ec93-f52f-4194-9ad9-57e636bcd0b6 - 2204110040003935\", \"x\": 8.71412467956543, \"y\": 4.195292949676514}, {\"index\": 2582, \"title\": \"Clusters not starting - Driver error messages in the event logs\", \"x\": 8.78825855255127, \"y\": 2.8453540802001953}, {\"index\": 3271, \"title\": \"[ARR] [Sev B] SR-2204040030002605 The cause of Mestastore, DBFS down\", \"x\": 8.441642761230469, \"y\": 3.1734652519226074}, {\"index\": 4843, \"title\": \"Unable to view ganglia on Jobs Clusters\", \"x\": 8.37898063659668, \"y\": 2.438791275024414}, {\"index\": 564, \"title\": \"CSS-ARR-2205170040002305-Unable to install library sets\", \"x\": 8.785216331481934, \"y\": 4.677615642547607}, {\"index\": 4622, \"title\": \"frequent scale-up and down\", \"x\": 5.845589637756348, \"y\": 3.262648344039917}, {\"index\": 74, \"title\": \"ARR | SevA | 2205300040003430 | Databricks jobs are failing after DBR upgrade in PROD\", \"x\": 10.229999542236328, \"y\": 2.8131754398345947}, {\"index\": 855, \"title\": \"2205130050000361 \", \"x\": 6.499220848083496, \"y\": 1.762184739112854}, {\"index\": 309, \"title\": \"Failed Databricks notebooks\", \"x\": 7.216691017150879, \"y\": 2.554375171661377}, {\"index\": 2864, \"title\": \"readStream & writeStream issue\", \"x\": 5.501351356506348, \"y\": 1.7785065174102783}, {\"index\": 282, \"title\": \"permissions changed on cluster\", \"x\": 7.761623859405518, \"y\": 5.286060333251953}, {\"index\": 2945, \"title\": \"Delta Live Table Overwrite mode\", \"x\": 10.260159492492676, \"y\": 4.087731838226318}, {\"index\": 2907, \"title\": \"need help setting up new worksapce\", \"x\": 7.567249298095703, \"y\": 1.7340303659439087}, {\"index\": 547, \"title\": \"ARR | Mysql Lock problem | 2205200030001217 \", \"x\": 8.187088966369629, \"y\": 5.082147121429443}, {\"index\": 3201, \"title\": \"SSO with Account level\", \"x\": 6.5804948806762695, \"y\": 1.9554523229599}, {\"index\": 5020, \"title\": \"Ordering of groups breaks when showing a visualization on a dashboard\", \"x\": 8.706930160522461, \"y\": 3.4904463291168213}, {\"index\": 1570, \"title\": \"_apply tables are appearing in target  metadata database;\", \"x\": 6.180679798126221, \"y\": 0.7245466709136963}, {\"index\": 1158, \"title\": \"Job failed with unknown error\", \"x\": 6.846284866333008, \"y\": 3.588911294937134}, {\"index\": 1680, \"title\": \"changing default timestamp for hive tables and databricks workspace to CET\", \"x\": 7.651566982269287, \"y\": 3.5755162239074707}, {\"index\": 720, \"title\": \"ARR | 2205160040000885 - Resource pool management | Feature request\", \"x\": 9.760414123535156, \"y\": 6.869290828704834}, {\"index\": 291, \"title\": \"CRITICAL Findings - CVE-2022-22824 - expat\", \"x\": 7.0852370262146, \"y\": 0.2850324213504791}, {\"index\": 540, \"title\": \"Follow up of 00122446. (Streaming delta table with changeDataFeed enabled breaks when zero rows are merged)\", \"x\": 5.968847274780273, \"y\": 3.722167491912842}, {\"index\": 2894, \"title\": \"Jobs taking too long - performance issue\", \"x\": 7.904183864593506, \"y\": 1.1410471200942993}, {\"index\": 2232, \"title\": \"Metastore is down error\", \"x\": 9.475587844848633, \"y\": 6.508152961730957}, {\"index\": 1036, \"title\": \"Jobs in Production and QA Databrciks are failing due to timeout error\", \"x\": 9.911877632141113, \"y\": 3.773751735687256}, {\"index\": 2987, \"title\": \"ARR- 2204010040007120- Inconsistent Results in Query\", \"x\": 8.892017364501953, \"y\": 4.660346031188965}, {\"index\": 1047, \"title\": \"Question about configuration in Databricks Cluster\", \"x\": 9.105535507202148, \"y\": 4.055788993835449}, {\"index\": 3382, \"title\": \"Add additional IP to Whitelist\", \"x\": 9.669695854187012, \"y\": 2.6158268451690674}, {\"index\": 3187, \"title\": \"Force cluster to use static ip on AWS\", \"x\": 6.057120323181152, \"y\": 2.4710400104522705}, {\"index\": 2315, \"title\": \"Cluster lost at least one node. Reason: Communication lost\", \"x\": 7.800326347351074, \"y\": 6.343003749847412}, {\"index\": 60, \"title\": \"unable to perform jdbc connection to synpase using aad authentication- Saved\", \"x\": 8.564556121826172, \"y\": 4.61676025390625}, {\"index\": 1599, \"title\": \"Access Azure Synapse from Azure Databricks with SQLDW spark connector\", \"x\": 8.423040390014648, \"y\": 4.398777961730957}, {\"index\": 5580, \"title\": \"Problem with saving data in table using 9.1 LTS\", \"x\": 6.931756019592285, \"y\": 5.637917995452881}, {\"index\": 1189, \"title\": \"Clusters not coming up - cannot create elastic disk cluster\", \"x\": 5.886168003082275, \"y\": 2.6868810653686523}, {\"index\": 3746, \"title\": \"ARR | AnalysisException: No such struct field steps in id, 1, 2, 3, 4 | 2203300050001855\", \"x\": 9.110392570495605, \"y\": 5.111192226409912}, {\"index\": 5165, \"title\": \"Job Abort error\", \"x\": 7.193509101867676, \"y\": 5.382460594177246}, {\"index\": 2443, \"title\": \"2777928729223047 - d4199cb9-58c6-44c0-901f-9824ed868a0c - 2204200040003791\", \"x\": 7.298364162445068, \"y\": 2.965721845626831}, {\"index\": 4324, \"title\": \"Assistance to understand VM utilization\", \"x\": 5.924344062805176, \"y\": 3.5509190559387207}, {\"index\": 535, \"title\": \"Customer stopped being able to access run of an experiment\", \"x\": 8.565216064453125, \"y\": 4.7328081130981445}, {\"index\": 4460, \"title\": \"ARR |  Gilead Sciences| Job aborted | SR: 2203190030000082 \", \"x\": 9.215072631835938, \"y\": 2.7287707328796387}, {\"index\": 5014, \"title\": \"Job falling and users loosing their access\", \"x\": 9.148643493652344, \"y\": 3.512963056564331}, {\"index\": 3134, \"title\": \"Clarification required on new r5a instance types\", \"x\": 8.468512535095215, \"y\": 4.884395122528076}, {\"index\": 5412, \"title\": \"Slow cluster performance \", \"x\": 8.903556823730469, \"y\": 4.585537910461426}, {\"index\": 581, \"title\": \"Cannot Launch Workspace \", \"x\": 7.877768039703369, \"y\": 3.4697601795196533}, {\"index\": 3207, \"title\": \"Cx cannot modify delta table during job run\", \"x\": 6.372358798980713, \"y\": 3.9130938053131104}, {\"index\": 195, \"title\": \"SSM Agent installation on Databricks Prod and Non-prod Worker notes\", \"x\": 7.248408794403076, \"y\": 1.9232659339904785}, {\"index\": 1263, \"title\": \"Cannot create and start cluster in Azure Databricks\", \"x\": 7.503839015960693, \"y\": 5.214471817016602}, {\"index\": 172, \"title\": \"job failures even after reties\", \"x\": 9.443928718566895, \"y\": 3.0813534259796143}, {\"index\": 3513, \"title\": \"Production jobs are failing due to Connection Timedout to Databricks\", \"x\": 5.582067966461182, \"y\": 4.446437835693359}, {\"index\": 4268, \"title\": \"We can't upgrade to current runtime version 10.x\", \"x\": 8.240917205810547, \"y\": 2.0458321571350098}, {\"index\": 924, \"title\": \"Investigate table access in logs\", \"x\": 5.670664310455322, \"y\": 4.384550094604492}, {\"index\": 1013, \"title\": \"java.io.IOException: Resource temporarily unavailable\", \"x\": 8.272890090942383, \"y\": 1.6384737491607666}, {\"index\": 4469, \"title\": \"2203110040005117 - Issue connecting to ADO repo on user AD authentication\", \"x\": 7.808254241943359, \"y\": 2.5786850452423096}, {\"index\": 1173, \"title\": \"can we set \\\"spark.sql.mapKeyDedupPolicy\\\" to \\\"LAST_WIN\\\" on Databricks SQL(sql endpoint))\", \"x\": 8.025750160217285, \"y\": 2.6729421615600586}, {\"index\": 1562, \"title\": \"Job Failure\", \"x\": 7.000829219818115, \"y\": 0.7108845710754395}, {\"index\": 1641, \"title\": \"Databricks E2 Privatelink Endpoints - Pending Acceptance\", \"x\": 8.515368461608887, \"y\": 2.579115390777588}, {\"index\": 3373, \"title\": \"Jobs failing with Netty OOM in E2 but they are passing in PVC\", \"x\": 6.987081527709961, \"y\": 0.3779628276824951}, {\"index\": 1430, \"title\": \"cluster\", \"x\": 5.7382283210754395, \"y\": 1.91146981716156}, {\"index\": 4710, \"title\": \"My cluster was not using the max of number of nodes\", \"x\": 9.192463874816895, \"y\": 3.281656265258789}, {\"index\": 3027, \"title\": \"Databricks Token Expiry\", \"x\": 8.1917085647583, \"y\": 2.2813973426818848}, {\"index\": 3371, \"title\": \"6398507633548715 - e33df5d1-ae22-417d-b794-8d9b6f338409 - 2204050040008980\", \"x\": 7.100831508636475, \"y\": 1.5607988834381104}, {\"index\": 2597, \"title\": \" job failed with error message Library installation failed for library due to infra fault\", \"x\": 8.65958023071289, \"y\": 1.9066308736801147}, {\"index\": 4694, \"title\": \"Follow up of SF ticket 00136158\", \"x\": 7.078855991363525, \"y\": 0.33935546875}, {\"index\": 4616, \"title\": \"files written to an s3 mount have incorrect permissions\", \"x\": 7.7834272384643555, \"y\": 1.5101104974746704}, {\"index\": 1239, \"title\": \"SparkException - Job Aborted due to stage failure\", \"x\": 6.064733505249023, \"y\": 2.8344521522521973}, {\"index\": 3949, \"title\": \"Unpredictable change on Databricks Environment\", \"x\": 9.040083885192871, \"y\": 6.41402530670166}, {\"index\": 415, \"title\": \"2205160030000400\", \"x\": 6.99302864074707, \"y\": 0.25019413232803345}, {\"index\": 4286, \"title\": \"Azure VM Extension Error\", \"x\": 6.2369303703308105, \"y\": 3.3052165508270264}, {\"index\": 982, \"title\": \"Slow performance observed in a frequently running job on Production\", \"x\": 9.098467826843262, \"y\": 3.6387953758239746}, {\"index\": 1164, \"title\": \"Can't able to install wheel files in the HC clusters\", \"x\": 6.072882652282715, \"y\": 2.779000997543335}, {\"index\": 1779, \"title\": \"Gnie\", \"x\": 8.511734008789062, \"y\": 4.076117992401123}, {\"index\": 5084, \"title\": \"Error in init script execution\", \"x\": 7.338564395904541, \"y\": 1.5654351711273193}, {\"index\": 5503, \"title\": \"Jobs started failing with 'ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))' as of Feb 28\", \"x\": 6.841938495635986, \"y\": 2.8019330501556396}, {\"index\": 586, \"title\": \"Jobs of migration are stopped with out rason\", \"x\": 6.743425369262695, \"y\": 3.991464376449585}, {\"index\": 595, \"title\": \"Create Query in Databricks SQL is not working\", \"x\": 9.83291244506836, \"y\": 6.511002540588379}, {\"index\": 1811, \"title\": \"Not able to run Marquez java library on the cluster\", \"x\": 6.303875923156738, \"y\": 3.1388473510742188}, {\"index\": 812, \"title\": \"Databricks not being able to connect to key-vault.\", \"x\": 9.203274726867676, \"y\": 3.786623477935791}, {\"index\": 1949, \"title\": \"New feature with Table Creation\", \"x\": 8.688586235046387, \"y\": 4.2261505126953125}, {\"index\": 5267, \"title\": \"2203020040002598\", \"x\": 7.730188369750977, \"y\": 5.6144185066223145}, {\"index\": 1167, \"title\": \"SQLRecoverableException: IO Error: Connection reset\", \"x\": 10.246015548706055, \"y\": 2.6699328422546387}, {\"index\": 1930, \"title\": \"ARR | SevA | 2204280040000761 | Job run for more than 9 hrs unexpectedly \", \"x\": 10.442780494689941, \"y\": 3.799128532409668}, {\"index\": 220, \"title\": \"ARR | 2205200030001371 | job intermittently failing with unexpected_exception | JNJ\", \"x\": 6.629411697387695, \"y\": 1.692652702331543}, {\"index\": 1065, \"title\": \"2205100040004471\", \"x\": 8.08116626739502, \"y\": 2.470489978790283}, {\"index\": 4354, \"title\": \"how to add service principal to \", \"x\": 7.025614261627197, \"y\": 0.3231728672981262}, {\"index\": 1692, \"title\": \"Databricks - Cluster Upgrade with gdal\", \"x\": 5.015199184417725, \"y\": 1.603806972503662}, {\"index\": 5428, \"title\": \"Jobs failed in production without  errors in code\", \"x\": 5.227123737335205, \"y\": 1.8834772109985352}, {\"index\": 3332, \"title\": \"Follow up case #00139520\", \"x\": 6.299460411071777, \"y\": 3.288215398788452}, {\"index\": 2013, \"title\": \"Production Job Failure\", \"x\": 10.063010215759277, \"y\": 3.206709384918213}, {\"index\": 3511, \"title\": \"Databricks Issue\", \"x\": 7.823155403137207, \"y\": 3.4515607357025146}, {\"index\": 774, \"title\": \"init script failing while installing library\", \"x\": 7.857302665710449, \"y\": 2.1480300426483154}, {\"index\": 305, \"title\": \"unable access to experiment in mlflow\", \"x\": 7.886416912078857, \"y\": 3.8663179874420166}, {\"index\": 439, \"title\": \"Same job sometimes runs for ever\", \"x\": 7.169581413269043, \"y\": 2.0995869636535645}, {\"index\": 713, \"title\": \"How to load excel(xlsx) file into dataframe?\", \"x\": 8.689234733581543, \"y\": 4.830626487731934}, {\"index\": 5126, \"title\": \"2203080060000023 -2\", \"x\": 7.251771450042725, \"y\": 2.0473110675811768}, {\"index\": 3905, \"title\": \"ADB error while reading XML data\", \"x\": 4.523280620574951, \"y\": 1.7476414442062378}, {\"index\": 1024, \"title\": \"billing question\", \"x\": 10.415250778198242, \"y\": 2.5071604251861572}, {\"index\": 4172, \"title\": \"not able to connect Mule with Databricks cluster\", \"x\": 9.152591705322266, \"y\": 4.904491424560547}, {\"index\": 1082, \"title\": \"2205110040001203\", \"x\": 9.70235824584961, \"y\": 4.0994873046875}, {\"index\": 3123, \"title\": \"Error starting the 'RStudio' cluster. Message is 'rstudio-install.sh failed: Script exit status is non-zero'\", \"x\": 7.893853187561035, \"y\": 2.7639613151550293}, {\"index\": 2293, \"title\": \"[ARR] [Sev B] SR-2204040030002605 continue to 00141003 | The impact on spark.databricks.hive.metastore.client.pool.size\", \"x\": 5.163625240325928, \"y\": 1.2928094863891602}, {\"index\": 734, \"title\": \"Cant connect to local computer/remote vm from Databricks workspace using ssh\", \"x\": 9.782890319824219, \"y\": 2.5454599857330322}, {\"index\": 1674, \"title\": \"2204210060006686\", \"x\": 8.217328071594238, \"y\": 2.0282323360443115}, {\"index\": 3757, \"title\": \"ARR Walmart - Databricks Jobs Failing\", \"x\": 6.943877220153809, \"y\": 4.395708084106445}, {\"index\": 4411, \"title\": \"Rstudio Cluster issues\", \"x\": 8.893156051635742, \"y\": 4.1933770179748535}, {\"index\": 4297, \"title\": \"Failure to access data in S3 Intelligent-Tiering Storage\", \"x\": 10.341729164123535, \"y\": 4.323258876800537}, {\"index\": 4521, \"title\": \" Rerouting cluster logs to adls gen2\", \"x\": 9.587577819824219, \"y\": 2.8302221298217773}, {\"index\": 4355, \"title\": \"Create a key vault backup scope with a Service Principal\", \"x\": 6.002302646636963, \"y\": 4.020033836364746}, {\"index\": 2113, \"title\": \"2204220060002233 | ARR | Git Enterprise Repo which is on-prem to intergrate with Azure Databricks \", \"x\": 6.385435581207275, \"y\": 2.724836826324463}, {\"index\": 1610, \"title\": \"Cluster is in terminated state and not available to receive jobs\", \"x\": 8.645962715148926, \"y\": 4.005003452301025}, {\"index\": 1643, \"title\": \"SQL endpoint error when running the same query in dashboard\", \"x\": 7.0296502113342285, \"y\": 0.2828180491924286}, {\"index\": 1219, \"title\": \"Need to create additional workspace on different AWS account.\", \"x\": 6.14480447769165, \"y\": 5.053301811218262}, {\"index\": 1604, \"title\": \"Docker image pull failure ECR\", \"x\": 8.138101577758789, \"y\": 2.1496665477752686}, {\"index\": 357, \"title\": \"Unable to login to Databricks workspace. \", \"x\": 7.872104644775391, \"y\": 5.026869297027588}, {\"index\": 2900, \"title\": \"Issue with executors\", \"x\": 7.525331497192383, \"y\": 3.534524440765381}, {\"index\": 530, \"title\": \"Need Assistance with Terraform for AWS - Issue in managing VPC endpoints and multiple workspaces within the same VPC\", \"x\": 7.696196556091309, \"y\": 3.163090467453003}, {\"index\": 5190, \"title\": \"The output of the notebook is too large\", \"x\": 7.539878845214844, \"y\": 5.400102615356445}, {\"index\": 5394, \"title\": \"ARR | Notebook java exception occurred while calling o3500.save, Job aborted due to stage failure | 2202210040004424\", \"x\": 7.235440731048584, \"y\": 1.5812972784042358}, {\"index\": 1093, \"title\": \"Setting up DBT to not export to DBFS\", \"x\": 8.017667770385742, \"y\": 3.8867199420928955}, {\"index\": 1765, \"title\": \"[ARR][Safeway][No Such Element]\", \"x\": 7.069681167602539, \"y\": 0.26823124289512634}, {\"index\": 2444, \"title\": \"DLT Cluster Configuration\", \"x\": 7.012800693511963, \"y\": 0.344072163105011}, {\"index\": 1935, \"title\": \"[ARR] [Sev B] SR-2204250050001910 \", \"x\": 8.067233085632324, \"y\": 2.928215503692627}, {\"index\": 985, \"title\": \"Change Databricks owner\", \"x\": 8.827249526977539, \"y\": 4.051043510437012}, {\"index\": 4210, \"title\": \"uninstalled libraries continue to be installed on cluster startup\", \"x\": 6.938834190368652, \"y\": 5.299932956695557}, {\"index\": 4273, \"title\": \"Notification Emails not being sent\", \"x\": 8.27619457244873, \"y\": 5.932403564453125}, {\"index\": 5550, \"title\": \"SQL Endpoints go into queued state and do not execute the first query when it is started by scheduled dashboard\", \"x\": 8.506054878234863, \"y\": 2.292531728744507}, {\"index\": 2337, \"title\": \"Invalid Access token\", \"x\": 7.87838888168335, \"y\": 3.079467296600342}, {\"index\": 2803, \"title\": \"Delta Live Table - Errors in Spark logs\", \"x\": 6.461697578430176, \"y\": 4.417400360107422}, {\"index\": 3053, \"title\": \"Rstudio 1.4 initialization failure\", \"x\": 7.758456707000732, \"y\": 3.522998332977295}, {\"index\": 3349, \"title\": \"MLFlow model version pending to be deployed forever\", \"x\": 7.84010648727417, \"y\": 3.4080755710601807}, {\"index\": 2787, \"title\": \"2204140050001360 | Job Failure\", \"x\": 6.892155170440674, \"y\": 2.6400868892669678}, {\"index\": 3854, \"title\": \"ARR |  Databricks Job UI page showing worker count wrongly\", \"x\": 8.4016695022583, \"y\": 2.9068186283111572}, {\"index\": 4721, \"title\": \"Dev Access to Feature Stores Not Working\", \"x\": 10.124424934387207, \"y\": 4.890542984008789}, {\"index\": 1288, \"title\": \"2204220010000657 | Job Failure\", \"x\": 6.603945732116699, \"y\": 5.650162220001221}, {\"index\": 2921, \"title\": \"Issue installing Rstudio on clusters\", \"x\": 7.746473789215088, \"y\": 2.693617820739746}, {\"index\": 137, \"title\": \"Notebook submitting unwanted Jobs to cluster\", \"x\": 6.895231246948242, \"y\": 5.562650680541992}, {\"index\": 973, \"title\": \"Azure Databricks Deployment getting failed\", \"x\": 8.641240119934082, \"y\": 3.2759077548980713}, {\"index\": 1648, \"title\": \"Install requirements.txt libs using Jobs 2.1 API\", \"x\": 9.106704711914062, \"y\": 2.811389684677124}, {\"index\": 2973, \"title\": \"Why AutoML takes only sample dataset?\", \"x\": 7.68324613571167, \"y\": 3.4873197078704834}, {\"index\": 615, \"title\": \"2205190030000562 | Performance\", \"x\": 5.580313682556152, \"y\": 4.576284408569336}, {\"index\": 5000, \"title\": \"Follow Up case for 00131300 - Metastore down\", \"x\": 10.343632698059082, \"y\": 3.983402967453003}, {\"index\": 4279, \"title\": \"Please increase workspace defined job quota from 1200 to 1500\", \"x\": 6.440003871917725, \"y\": 4.584284782409668}, {\"index\": 1858, \"title\": \"Follow Up ticket 00141210 and we'd like to confirm the flow of creating secret scope\", \"x\": 7.646319389343262, \"y\": 3.6231911182403564}, {\"index\": 5228, \"title\": \"Need to connect new network interface to existing workspace\", \"x\": 10.123835563659668, \"y\": 2.5595786571502686}, {\"index\": 4773, \"title\": \"[ARR] [Sev B] SR-2203080050001528 OS/IO error while connecting to SFTP server\", \"x\": 6.746119499206543, \"y\": 2.997582197189331}, {\"index\": 5160, \"title\": \"The batch ingestion tasks are stuck or very slow when running on DBR 10.3\", \"x\": 8.765917778015137, \"y\": 3.110950469970703}, {\"index\": 1711, \"title\": \"Training Access for LTK Employees\", \"x\": 9.001001358032227, \"y\": 2.1694722175598145}, {\"index\": 2268, \"title\": \"Get the size of /dbfs filesytme\", \"x\": 9.97706413269043, \"y\": 4.670249938964844}, {\"index\": 5459, \"title\": \"Timeout for Databricks sql connector for python\", \"x\": 9.020448684692383, \"y\": 4.920384407043457}, {\"index\": 5403, \"title\": \"ARR Customer: AT&T - Notebooks not executing on cluster, commands abruptly canceled. \", \"x\": 9.478742599487305, \"y\": 3.8739798069000244}, {\"index\": 2424, \"title\": \"Invalid endpoint when calling account api\", \"x\": 7.3217620849609375, \"y\": 2.5971992015838623}, {\"index\": 158, \"title\": \"New GEnie 2205190050000485 \", \"x\": 9.252388000488281, \"y\": 4.982494354248047}, {\"index\": 4066, \"title\": \"ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated\", \"x\": 5.224807262420654, \"y\": 1.6133224964141846}, {\"index\": 2768, \"title\": \"Private Links for us-east-1 for new workspace\", \"x\": 5.801233768463135, \"y\": 4.216389179229736}, {\"index\": 1366, \"title\": \"2777928729223047 - d4199cb9-58c6-44c0-901f-9824ed868a0c - 2205070040001063\", \"x\": 6.87648344039917, \"y\": 4.852598667144775}, {\"index\": 414, \"title\": \"2205090040004711\", \"x\": 7.829654693603516, \"y\": 3.7641353607177734}, {\"index\": 3290, \"title\": \"\\\"Bad Request\\u201d error message when trying to access the Ganglia UI Live Metrics through UI\", \"x\": 7.707338333129883, \"y\": 1.9822032451629639}, {\"index\": 2330, \"title\": \"7325620703232380 - 4da48ca4-4455-422a-be4c-300209649149 - 2204210040012860\", \"x\": 10.295049667358398, \"y\": 4.552796840667725}, {\"index\": 4369, \"title\": \"ARR | 2203220040000035 | java.lang.OutOfMemoryError even though increasing the cluster size\", \"x\": 7.8533244132995605, \"y\": 3.288727045059204}, {\"index\": 3665, \"title\": \"Jobs failing with library not being installed on clusters\", \"x\": 9.648494720458984, \"y\": 2.606221914291382}, {\"index\": 2753, \"title\": \"ARR- ATT : Getting connection error from Java code - 2204130040010988 \", \"x\": 10.151122093200684, \"y\": 2.7614476680755615}, {\"index\": 4154, \"title\": \"Cluster Configuration changed with new Hardware Types\", \"x\": 7.018688201904297, \"y\": 0.30262327194213867}, {\"index\": 1040, \"title\": \"Files in Repos issue\", \"x\": 6.875755310058594, \"y\": 3.258208990097046}, {\"index\": 5430, \"title\": \"How to re-index delta data\", \"x\": 8.960783004760742, \"y\": 4.071059703826904}, {\"index\": 2899, \"title\": \"Geni access request\", \"x\": 9.99023151397705, \"y\": 3.195369243621826}, {\"index\": 2910, \"title\": \"Unexpected error while running query in Redshift external data source\", \"x\": 7.958077430725098, \"y\": 3.3166375160217285}, {\"index\": 3858, \"title\": \"AWS Databricks.com Oauth Connection problem to Ping ID in Azure\", \"x\": 8.645764350891113, \"y\": 4.626497268676758}, {\"index\": 4849, \"title\": \"Databricks SQL Dashboard Refresh\", \"x\": 8.81728458404541, \"y\": 3.968043565750122}, {\"index\": 508, \"title\": \"5491164172891362 - b8d79299-dbc4-42b6-b557-01b3a7cf0605 - 2205200040003030\", \"x\": 10.200784683227539, \"y\": 4.88435173034668}, {\"index\": 3769, \"title\": \"Not able to install msodbcsql driver on job clusters getting below error.\", \"x\": 9.6344575881958, \"y\": 3.7866430282592773}, {\"index\": 4147, \"title\": \"Enable Private Preview of arbitrary column name support in Delta tables\", \"x\": 7.980188846588135, \"y\": 5.14623498916626}, {\"index\": 2448, \"title\": \"Databricks Kafka SASL Error\", \"x\": 8.493526458740234, \"y\": 2.6473608016967773}, {\"index\": 1781, \"title\": \"Job status view is broken\", \"x\": 10.043318748474121, \"y\": 2.6465158462524414}, {\"index\": 4630, \"title\": \"Driver issue\", \"x\": 6.07079553604126, \"y\": 3.1284189224243164}, {\"index\": 4981, \"title\": \"Connecting external data source using JDBC connection from Notebook \", \"x\": 6.553847312927246, \"y\": 2.865013360977173}, {\"index\": 4234, \"title\": \"Error/slow  establishing connection/fetching data from SQL Endpoint\", \"x\": 5.294597148895264, \"y\": 2.239060163497925}, {\"index\": 3042, \"title\": \"Databricks jobs are failing to trigger the automation account Runbooks\", \"x\": 6.472891330718994, \"y\": 2.208738327026367}, {\"index\": 4143, \"title\": \"databricks dbx: issues with deploying job clusters and installing python wheel on it\", \"x\": 8.273502349853516, \"y\": 1.0165201425552368}, {\"index\": 543, \"title\": \"Job cluster creation is failing due to being unable to fetch secrets\", \"x\": 9.585494041442871, \"y\": 3.9599599838256836}, {\"index\": 2993, \"title\": \"Need oncall engineer to pin Intuit PRD workspace back to custom image\", \"x\": 8.478439331054688, \"y\": 1.62974214553833}, {\"index\": 4203, \"title\": \"Unity Catalog can't Unlink Metastore from a Workspace\", \"x\": 4.538373947143555, \"y\": 1.8015505075454712}, {\"index\": 223, \"title\": \"Cluster terminated.Reason:Storage Download Failure\", \"x\": 6.205197334289551, \"y\": 5.094478130340576}, {\"index\": 4382, \"title\": \"gar for 2203210030001444 \", \"x\": 6.723297119140625, \"y\": 4.827437400817871}, {\"index\": 3730, \"title\": \"Spark task hangs, repeating same stage over and over\", \"x\": 8.090673446655273, \"y\": 3.7256534099578857}, {\"index\": 1402, \"title\": \"feature request(db connect using secret scopes) Case\", \"x\": 7.790959358215332, \"y\": 1.8934438228607178}, {\"index\": 1872, \"title\": \"7763010224164137 - b8080fe8-25bc-47b2-85e7-fd02dd3aee1e - 2204280040004711\", \"x\": 5.124285697937012, \"y\": 1.75919508934021}, {\"index\": 3447, \"title\": \"Jobs failing with library not being installed on dataclusters\", \"x\": 6.306690216064453, \"y\": 2.0900657176971436}, {\"index\": 2136, \"title\": \"GPU sharing between spark executors \", \"x\": 8.351957321166992, \"y\": 5.830844879150391}, {\"index\": 180, \"title\": \"[ARR] [Sev B] SR-2205260030001963-Cs_new_fix_issue\", \"x\": 8.80848217010498, \"y\": 5.424338340759277}, {\"index\": 3461, \"title\": \"Support service principals at the account level for automation\", \"x\": 7.042052268981934, \"y\": 0.2530857026576996}, {\"index\": 2173, \"title\": \"Exception equests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://centralus.azuredatabricks.net/api/2.0/jobs/list\", \"x\": 6.673325538635254, \"y\": 1.6510744094848633}, {\"index\": 4737, \"title\": \"Configure Audit Logging\", \"x\": 5.650012969970703, \"y\": 1.6369208097457886}, {\"index\": 2774, \"title\": \" Inconsistent connectivity issue. Rerunning the same fix the issue.\", \"x\": 7.045872211456299, \"y\": 0.3198826014995575}, {\"index\": 1322, \"title\": \"describe , optimize commands are not working as expected \", \"x\": 7.657436370849609, \"y\": 1.7629947662353516}, {\"index\": 3067, \"title\": \"Databricks jobs are failing to trigger the automation account Runbooks\", \"x\": 9.35029125213623, \"y\": 2.652661085128784}, {\"index\": 3638, \"title\": \"Access multiple workspaces\", \"x\": 5.8884100914001465, \"y\": 3.210772752761841}, {\"index\": 539, \"title\": \"Databricks:  When clicking on completed jobs get HTTP ERROR 403\", \"x\": 9.73483943939209, \"y\": 3.3523576259613037}, {\"index\": 289, \"title\": \"S500 | Highly escalated| job performance degredation \", \"x\": 7.71152400970459, \"y\": 2.5731887817382812}, {\"index\": 5474, \"title\": \"Databricks jobs failing with ShuffleMapStage error \", \"x\": 7.742801666259766, \"y\": 4.215195178985596}, {\"index\": 4142, \"title\": \"Jobs hangs randomly - Driver - Executor registration failure.\", \"x\": 6.6368489265441895, \"y\": 4.523159980773926}, {\"index\": 975, \"title\": \" Spark Structured Streaming Job Not Running\", \"x\": 9.767681121826172, \"y\": 2.452263832092285}, {\"index\": 2404, \"title\": \"CSS-ARR-2204210030000061-Job failure \", \"x\": 8.559061050415039, \"y\": 5.057392120361328}, {\"index\": 4414, \"title\": \"Random Data Drop In Sales Xpopd Data Service\", \"x\": 10.400195121765137, \"y\": 2.4780290126800537}, {\"index\": 4134, \"title\": \"unable to delete account for colleague\", \"x\": 6.657045364379883, \"y\": 1.5690397024154663}, {\"index\": 3739, \"title\": \"Failure to add user to a Databricks workspace with an error\", \"x\": 8.17926025390625, \"y\": 2.178220510482788}, {\"index\": 3670, \"title\": \"issue reconfiguring cluster on fly using db connect \", \"x\": 9.389878273010254, \"y\": 3.2159998416900635}, {\"index\": 41, \"title\": \"Job is stuck\", \"x\": 6.858315944671631, \"y\": 3.172680377960205}, {\"index\": 4779, \"title\": \"support for resuming multi-task job at failure\", \"x\": 5.488332271575928, \"y\": 1.5668401718139648}, {\"index\": 4538, \"title\": \"Bug in the Databricks cluster UI\", \"x\": 6.892392158508301, \"y\": 1.920944333076477}, {\"index\": 3683, \"title\": \"Databricks job got stuck for 7 Days \", \"x\": 4.980124473571777, \"y\": 1.585213303565979}, {\"index\": 3936, \"title\": \"Run result unavailable: job failed with error message An upstream task failed.\", \"x\": 9.059148788452148, \"y\": 2.1136221885681152}, {\"index\": 5591, \"title\": \"SSL error\", \"x\": 5.737266540527344, \"y\": 2.6368954181671143}, {\"index\": 3588, \"title\": \"Vulnerability Issue in nodes of databricks cluster\", \"x\": 8.280587196350098, \"y\": 3.0269289016723633}, {\"index\": 4093, \"title\": \"Running a dashboard with CLI\", \"x\": 8.80593204498291, \"y\": 3.9989888668060303}, {\"index\": 2702, \"title\": \"2204150010001935\", \"x\": 5.90284538269043, \"y\": 4.202371120452881}, {\"index\": 4051, \"title\": \"ARR | 2203030010001180 | The streaming job failed due to Not able to validate external location because The remote server returned an error: (503)\", \"x\": 9.724037170410156, \"y\": 2.150291681289673}, {\"index\": 876, \"title\": \"2205120030001514 | ARR | Is it possible and supported to create a job using service principal?\", \"x\": 9.839162826538086, \"y\": 3.3587563037872314}, {\"index\": 2383, \"title\": \"Disable and Re-Enable SSO Integration for root account with Azure AD\", \"x\": 6.310279369354248, \"y\": 3.60273814201355}, {\"index\": 1391, \"title\": \"Fail to install python library glpk \", \"x\": 7.606226921081543, \"y\": 2.096254348754883}, {\"index\": 506, \"title\": \"2205220030000028 \", \"x\": 5.556436538696289, \"y\": 1.3545459508895874}, {\"index\": 4233, \"title\": \"Abnormal beahviour on cluster\", \"x\": 7.474582195281982, \"y\": 3.974769115447998}, {\"index\": 1014, \"title\": \"Unable to find valid certification path to requested target\", \"x\": 8.033159255981445, \"y\": 2.511876106262207}, {\"index\": 4055, \"title\": \"CalledProcessError: Command 'pip install 'opencensus.ext.azure'' returned non-zero exit status 1\", \"x\": 8.454352378845215, \"y\": 5.141589641571045}, {\"index\": 387, \"title\": \"Notebooks/cluster Detach and Re-attcah issue\", \"x\": 7.089598178863525, \"y\": 4.709130764007568}, {\"index\": 4103, \"title\": \"HIPAA exemption request for public preview feature: Service Principals / OBO tokens\", \"x\": 8.874642372131348, \"y\": 6.716691493988037}, {\"index\": 307, \"title\": \"Terraform error for mounting a bucket\", \"x\": 6.798521995544434, \"y\": 1.8734337091445923}, {\"index\": 1336, \"title\": \"ARR | GAP | Job cluster with specific pool launch fail with Array Index error. | SR: 2205030010000748  \", \"x\": 7.906316757202148, \"y\": 3.291175603866577}, {\"index\": 1863, \"title\": \"Job getting aborted due to Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages\", \"x\": 8.4233980178833, \"y\": 2.472649335861206}, {\"index\": 3844, \"title\": \"Data bricks cluster ran longer & Failed with Timed out error\", \"x\": 7.968200206756592, \"y\": 6.012265682220459}, {\"index\": 4119, \"title\": \"Account owner/admin for 2nd Acc\", \"x\": 5.734901428222656, \"y\": 4.621067047119141}, {\"index\": 5319, \"title\": \"Kryo serialization failed: Buffer overflow. Available: 0, required: 9\", \"x\": 6.189922332763672, \"y\": 3.0977611541748047}, {\"index\": 1255, \"title\": \"2205090030001499 \", \"x\": 8.820296287536621, \"y\": 5.529731273651123}, {\"index\": 5595, \"title\": \"one AWS account for multiple databricks workspace\", \"x\": 8.28495979309082, \"y\": 2.000059127807617}, {\"index\": 340, \"title\": \"Geni access request\", \"x\": 6.634769439697266, \"y\": 5.371109485626221}, {\"index\": 1801, \"title\": \"ARR - 2204290040005232 - Servicio De Administraci\\u00f3n Tributaria - Spark job hung for over 6 hours\", \"x\": 7.797936916351318, \"y\": 4.7482686042785645}, {\"index\": 5174, \"title\": \"Unable to start cluster \", \"x\": 7.190027713775635, \"y\": 5.21099328994751}, {\"index\": 1955, \"title\": \"Facing issue while creating job in Databricks E2 shards\", \"x\": 6.439192295074463, \"y\": 3.541640281677246}, {\"index\": 4913, \"title\": \" Increasing scope limit from 150 to 250\", \"x\": 6.2730021476745605, \"y\": 2.9194889068603516}, {\"index\": 3555, \"title\": \"Unable to mount and read ADLS file system\", \"x\": 8.952877044677734, \"y\": 6.3994669914245605}, {\"index\": 2004, \"title\": \"gar for 2204270030001343 \", \"x\": 10.229945182800293, \"y\": 2.9337399005889893}, {\"index\": 2595, \"title\": \"PVC Databricks RDS Instance types query\", \"x\": 9.746878623962402, \"y\": 5.031747341156006}, {\"index\": 747, \"title\": \"EDL - Databricks CLI access issue\", \"x\": 10.044116020202637, \"y\": 2.7539892196655273}, {\"index\": 3330, \"title\": \"Python shell exits with 143 exception and REPL gets restarted continously | ARR SR# 2204060030001258\", \"x\": 10.363715171813965, \"y\": 4.070853233337402}, {\"index\": 4260, \"title\": \"ARR | 2203150040006011 | mlflow.xgboost.load(xgboost model) returned __init__() got an unexpected keyword argument 'tensor-spec' error\", \"x\": 4.560348033905029, \"y\": 1.7314784526824951}, {\"index\": 2053, \"title\": \"Library installation failed when jar was present in storage account\", \"x\": 9.355782508850098, \"y\": 3.230954170227051}, {\"index\": 3416, \"title\": \"CSS-ARR-SR#2204010030001163-Not able to provision the All purpose clusters\", \"x\": 6.617799282073975, \"y\": 3.39127779006958}, {\"index\": 1503, \"title\": \"PowerBI to Databricks connections\", \"x\": 7.953622817993164, \"y\": 2.243457078933716}, {\"index\": 4653, \"title\": \"Assume role from notebook for specific bucket\", \"x\": 10.366765975952148, \"y\": 3.3203048706054688}, {\"index\": 5266, \"title\": \"Unauthorized access error message from Databricks Role.\", \"x\": 5.920403480529785, \"y\": 1.6059050559997559}, {\"index\": 1780, \"title\": \"2787931338254557 - 135d2085-e95f-4550-b69c-4b5ec0c17f9a - 2204300010000278\", \"x\": 9.707975387573242, \"y\": 2.613147020339966}, {\"index\": 3193, \"title\": \"Guidance on how to get access logs for when new user is added or removed from DBx.\", \"x\": 9.303841590881348, \"y\": 6.708710670471191}, {\"index\": 765, \"title\": \"Trying to achieve a shuffle-less join with partitionBy\", \"x\": 8.030766487121582, \"y\": 3.55745792388916}, {\"index\": 4308, \"title\": \"Audit logs are stopped published to configured S3 buckets \", \"x\": 5.9415812492370605, \"y\": 4.896489143371582}, {\"index\": 925, \"title\": \"Analytics cluster driver became unresponsive to to GC Errors\", \"x\": 8.057465553283691, \"y\": 3.8369388580322266}, {\"index\": 856, \"title\": \"2204260040007251003\", \"x\": 10.059024810791016, \"y\": 2.4746031761169434}, {\"index\": 3659, \"title\": \"Difference in JSON response of get job API and that visible in UI (both UI and json view) for jobs having instance pools\", \"x\": 7.268991947174072, \"y\": 1.9808844327926636}, {\"index\": 5457, \"title\": \"ARR | MGM | Recovery delta tables for a recovered workspace | 2203020030001991\", \"x\": 7.39163875579834, \"y\": 1.5530763864517212}, {\"index\": 1270, \"title\": \"2204270030000882\", \"x\": 5.028057098388672, \"y\": 1.5888032913208008}, {\"index\": 4829, \"title\": \"Databricks PVC Upgrade - US and EU\", \"x\": 8.579141616821289, \"y\": 2.589416265487671}, {\"index\": 3879, \"title\": \"Notebook interaction performance\", \"x\": 7.818387031555176, \"y\": 3.8332018852233887}, {\"index\": 1114, \"title\": \"Autoloader can not ingest *.snappy.parquet files\", \"x\": 7.864346027374268, \"y\": 1.7227109670639038}, {\"index\": 5288, \"title\": \"Use of auditd socket for auditing\", \"x\": 6.832242012023926, \"y\": 4.405299663543701}, {\"index\": 5194, \"title\": \"Integration to AWS\", \"x\": 7.003523826599121, \"y\": 1.840294361114502}, {\"index\": 751, \"title\": \"SQL end point connection with Microstra\", \"x\": 6.060643196105957, \"y\": 1.6771129369735718}, {\"index\": 2703, \"title\": \"User Unable to Access Workspace\", \"x\": 6.593504905700684, \"y\": 1.1303424835205078}, {\"index\": 4672, \"title\": \"Whitelist spark configuration for SQL endpoints | ARR SR# 2203160030001206 \", \"x\": 9.146416664123535, \"y\": 4.804605007171631}, {\"index\": 4872, \"title\": \"Azure Databricks Connector ssl_connect error from PowerBI Desktop\", \"x\": 9.050929069519043, \"y\": 4.527072906494141}, {\"index\": 151, \"title\": \"Unable to read java file\", \"x\": 6.7645416259765625, \"y\": 1.7020364999771118}, {\"index\": 1032, \"title\": \"Unable to run notebooks from repo tab in databricks\", \"x\": 7.890332221984863, \"y\": 2.506657123565674}, {\"index\": 2508, \"title\": \"java.net.SocketTimeoutException: Read timed out\", \"x\": 7.617015361785889, \"y\": 2.794785976409912}, {\"index\": 5032, \"title\": \"job-cluster-config-change-does-not-take-affect\", \"x\": 8.514355659484863, \"y\": 4.7070488929748535}, {\"index\": 346, \"title\": \"Enable Unity catalog\", \"x\": 8.309816360473633, \"y\": 1.7535592317581177}, {\"index\": 4482, \"title\": \"ARR - AT&T - 2203180040005316 - Downgrade python version\", \"x\": 7.1825995445251465, \"y\": 2.391551971435547}, {\"index\": 2630, \"title\": \"One of our Databricks environments is much slower than our other Databricks environments\", \"x\": 9.008835792541504, \"y\": 4.765748977661133}, {\"index\": 1670, \"title\": \"Open a New Tab from Recents\", \"x\": 7.073076248168945, \"y\": 0.6187435984611511}, {\"index\": 706, \"title\": \"Performance Tuning the job\", \"x\": 8.634161949157715, \"y\": 4.95507287979126}, {\"index\": 5202, \"title\": \"CSS-SFMC-OSA US BARO Fact Calc : SparkOutOfMemoryError Issue\", \"x\": 10.330574989318848, \"y\": 2.759063959121704}, {\"index\": 2675, \"title\": \"Getting Timed Out Error\", \"x\": 6.43655252456665, \"y\": 3.4783992767333984}, {\"index\": 2978, \"title\": \"2204110050001893 \", \"x\": 9.588153839111328, \"y\": 2.107452869415283}, {\"index\": 1746, \"title\": \"Unable to create empty delta tables in the ADLS stoarage\", \"x\": 9.891018867492676, \"y\": 3.576413154602051}, {\"index\": 1400, \"title\": \"User unable to log in\", \"x\": 8.134024620056152, \"y\": 1.5424258708953857}, {\"index\": 1017, \"title\": \"Swithcing E1 from ADFS to Azure AD SSO - Errors\", \"x\": 7.7898688316345215, \"y\": 1.7644727230072021}, {\"index\": 3237, \"title\": \"Resource Required to append to a delta table \", \"x\": 9.145567893981934, \"y\": 3.944939613342285}, {\"index\": 1649, \"title\": \"JDBC Connectivity from Tableau\", \"x\": 8.490352630615234, \"y\": 3.5453031063079834}, {\"index\": 2240, \"title\": \"External AWS Glue metastore is not accessible from Databricks Dataplane via Assumerole\", \"x\": 7.566475868225098, \"y\": 3.5778698921203613}, {\"index\": 3006, \"title\": \" issues with library and all the jobs are failling in databricks\", \"x\": 8.777925491333008, \"y\": 5.707620620727539}, {\"index\": 1502, \"title\": \"customer interested in consultancy services\", \"x\": 9.198307037353516, \"y\": 4.820250988006592}, {\"index\": 87, \"title\": \"Error on MetaStore when using uploaded JAR for DB connection with AWS PrivateLink\", \"x\": 5.745730876922607, \"y\": 4.49065637588501}, {\"index\": 2699, \"title\": \"Failure to start cluster workspace authorization request\", \"x\": 10.298608779907227, \"y\": 4.2613525390625}, {\"index\": 521, \"title\": \"SQL endpoint cluster has senstive config in plain text\", \"x\": 8.659265518188477, \"y\": 2.018526077270508}, {\"index\": 5204, \"title\": \"Error on previewing the data in Power BI cinnecting to Databricks source\", \"x\": 7.108279705047607, \"y\": 3.9496521949768066}, {\"index\": 1556, \"title\": \"GENIE ACCESS\", \"x\": 6.525296688079834, \"y\": 1.163008689880371}, {\"index\": 2964, \"title\": \"Performance issue in our databricks code\", \"x\": 9.494344711303711, \"y\": 3.222576141357422}, {\"index\": 3685, \"title\": \"ARR | Hive Data Table click-thru ends in error | 2203300040003793\", \"x\": 8.956053733825684, \"y\": 5.445254802703857}, {\"index\": 3185, \"title\": \"2204050010001036\", \"x\": 8.456009864807129, \"y\": 5.971434593200684}, {\"index\": 2889, \"title\": \"Follow up ticket for 00137952 | 2203140030000472 \", \"x\": 9.597764015197754, \"y\": 2.550969362258911}, {\"index\": 5597, \"title\": \"2202240030000721001\", \"x\": 9.62291431427002, \"y\": 3.854102373123169}, {\"index\": 1201, \"title\": \"Issue while accessing certain s3 bucktes from Databricks\", \"x\": 7.8535661697387695, \"y\": 3.6371679306030273}, {\"index\": 2135, \"title\": \"Run result unavailable: job failed with error message Failed to acquire MySQL lock job-1.\", \"x\": 10.358480453491211, \"y\": 2.8042821884155273}, {\"index\": 5281, \"title\": \"Trouble Calling Java Class from Scala Code\", \"x\": 8.023380279541016, \"y\": 6.594240665435791}, {\"index\": 5178, \"title\": \"Okta Push Groups failing for groups with parent\", \"x\": 9.599980354309082, \"y\": 5.005035400390625}, {\"index\": 5196, \"title\": \"Error during  job execution\", \"x\": 9.416019439697266, \"y\": 3.9043378829956055}, {\"index\": 3359, \"title\": \"Query profile not available\", \"x\": 9.106452941894531, \"y\": 3.993149518966675}, {\"index\": 5471, \"title\": \"9592 - g - ARR - meijer - Streaming job failure\", \"x\": 8.025931358337402, \"y\": 6.000997066497803}, {\"index\": 2735, \"title\": \"faced error when running 3-table join Spark SQL\", \"x\": 9.98672103881836, \"y\": 4.0036940574646}, {\"index\": 3802, \"title\": \" Error in configuring SCIM for databricks(Private Link is enabled )[S500]\", \"x\": 8.832033157348633, \"y\": 4.6295166015625}, {\"index\": 1142, \"title\": \"Data persistency issues\", \"x\": 8.07575798034668, \"y\": 2.6358227729797363}, {\"index\": 2819, \"title\": \"Security Group Missing \", \"x\": 4.502567768096924, \"y\": 1.797956109046936}, {\"index\": 10, \"title\": \"Metadata perfomance issue\", \"x\": 6.445991516113281, \"y\": 4.048285007476807}, {\"index\": 2998, \"title\": \"Databricks User within IAM Passthrough cluster unable to execute Query - Issue already reported.\", \"x\": 6.413425445556641, \"y\": 3.321228265762329}, {\"index\": 4241, \"title\": \"[com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near '<'.] when run spark_df = spark.read \", \"x\": 9.030433654785156, \"y\": 2.1313581466674805}, {\"index\": 4723, \"title\": \"Error on a SQL query.\", \"x\": 9.575282096862793, \"y\": 3.626100540161133}, {\"index\": 3939, \"title\": \"Issue with databricks 10.4 while writing data in to snowflake table having timestamp column and no source record to be written in snowflake table\", \"x\": 6.7464141845703125, \"y\": 1.775928258895874}, {\"index\": 4002, \"title\": \"Unable to whitelist 10.x.x.x IP address on storage account firewall\", \"x\": 5.934615135192871, \"y\": 3.880448579788208}, {\"index\": 2296, \"title\": \"2204220010000657\", \"x\": 8.523050308227539, \"y\": 4.167827606201172}, {\"index\": 4131, \"title\": \"Missing log entries from Azure Data Factory\", \"x\": 5.793592929840088, \"y\": 3.816089391708374}, {\"index\": 2671, \"title\": \"ARR | 2204180030000346 | GEP | Job failed intermittently with cluster doesn't exist \", \"x\": 9.322578430175781, \"y\": 3.098839282989502}, {\"index\": 1845, \"title\": \"Jobs failing and cluster is being terminated\", \"x\": 6.128251075744629, \"y\": 3.589679718017578}, {\"index\": 2099, \"title\": \"IP Whitelist is blocking all logins\", \"x\": 7.83275842666626, \"y\": 1.164304494857788}, {\"index\": 569, \"title\": \"Understanding the impact of a health advisory\", \"x\": 5.787155628204346, \"y\": 2.745927572250366}, {\"index\": 3616, \"title\": \"Cluster takes long time to become available to end-users\", \"x\": 7.180271148681641, \"y\": 1.1099411249160767}, {\"index\": 4704, \"title\": \"Cannot show graphs in notebooks in HTML format- Saved\", \"x\": 8.921609878540039, \"y\": 2.560683012008667}, {\"index\": 5533, \"title\": \"Python error : not able to convert leap seconds : 'ValueError: second must be in 0..59'\", \"x\": 5.530312538146973, \"y\": 4.238201141357422}, {\"index\": 2276, \"title\": \"2204210050003993 Incorrect working directories when executing commands in notebook\", \"x\": 9.173190116882324, \"y\": 6.41160774230957}, {\"index\": 1408, \"title\": \"follow-up for SF # 00143521\", \"x\": 8.06015396118164, \"y\": 4.3584675788879395}, {\"index\": 5286, \"title\": \"whether the azure databrick will create the kv automatically\", \"x\": 7.439074993133545, \"y\": 3.3449528217315674}, {\"index\": 604, \"title\": \"Problem with Metastore in Account console\", \"x\": 6.621808052062988, \"y\": 3.091073751449585}, {\"index\": 4077, \"title\": \"[ARR][Streaming job failed multiple times]\", \"x\": 9.2028169631958, \"y\": 3.613571882247925}, {\"index\": 3950, \"title\": \"ARR:Follow Up 00139476:VM name and ID request for 00139476:SR 2203240040003987 \", \"x\": 8.55765438079834, \"y\": 2.482525587081909}, {\"index\": 5188, \"title\": \"Unable to connect to Azure Devops Repo\", \"x\": 8.078129768371582, \"y\": 5.675217628479004}, {\"index\": 4982, \"title\": \"SQL ODBC Connection Issue\", \"x\": 9.811595916748047, \"y\": 2.329376697540283}, {\"index\": 1332, \"title\": \"View \\\"exceeds the maximum view resolution depth (100)\\\"\", \"x\": 9.85178279876709, \"y\": 2.988283157348633}, {\"index\": 1647, \"title\": \"error message on main dbx dashboard\", \"x\": 9.105134010314941, \"y\": 5.846030235290527}, {\"index\": 2701, \"title\": \"Rename Deployment name prefix\", \"x\": 9.636061668395996, \"y\": 6.719437122344971}, {\"index\": 2939, \"title\": \"Graviton instances not working\", \"x\": 9.29076862335205, \"y\": 3.2336738109588623}, {\"index\": 3529, \"title\": \"DBFS is down on almost all cluster on 4/1~current\", \"x\": 8.10443115234375, \"y\": 5.225199222564697}, {\"index\": 688, \"title\": \"java.lang.ClassCastException when we try runtime 10.5\", \"x\": 7.1330037117004395, \"y\": 4.601931095123291}, {\"index\": 822, \"title\": \"Collibra Unable to acquire Metadata from Databricks Schema via JDBC.\", \"x\": 8.3428955078125, \"y\": 3.715824842453003}, {\"index\": 5573, \"title\": \"Need support to optimize data processing job\", \"x\": 5.619457721710205, \"y\": 4.370204925537109}, {\"index\": 4049, \"title\": \"activate dbutils.secrets.get through databricks-connect\", \"x\": 8.886039733886719, \"y\": 2.6792492866516113}, {\"index\": 3019, \"title\": \"issue parsing json schema\", \"x\": 8.923110961914062, \"y\": 2.2467238903045654}, {\"index\": 1677, \"title\": \" I got stuck to an error in data bricks , the error is ' Command result size exceeds limit: Exceeded 20971520 bytes (current = 20972463)'\", \"x\": 9.92786979675293, \"y\": 2.6858034133911133}, {\"index\": 1149, \"title\": \"Conda on 9.1 LTS ML Runtime\", \"x\": 8.972762107849121, \"y\": 6.702851295471191}, {\"index\": 5324, \"title\": \"Slow runtimes for jobs with many DDL changes\", \"x\": 7.008098602294922, \"y\": 0.37484633922576904}, {\"index\": 5495, \"title\": \"Databricks Autoscaling: Scale down not happening\", \"x\": 7.95365047454834, \"y\": 3.143383741378784}, {\"index\": 1493, \"title\": \"ARR Customer: UniParks - Databricks to storage/Synapse connection issue\", \"x\": 8.2948637008667, \"y\": 2.127206325531006}, {\"index\": 3643, \"title\": \"Unable to add user to Databricks application\", \"x\": 9.823412895202637, \"y\": 2.386090040206909}, {\"index\": 2114, \"title\": \"[ARR] [Sev B] SR-2204240060000475 How to read parquet from SQL Analytics\", \"x\": 7.484865188598633, \"y\": 1.6285557746887207}, {\"index\": 5530, \"title\": \"Databricks timeout problem\", \"x\": 5.891611099243164, \"y\": 3.027165412902832}, {\"index\": 1323, \"title\": \"unable to create Db and tables\", \"x\": 6.909545421600342, \"y\": 2.319582939147949}, {\"index\": 4216, \"title\": \"NTH_VALUE func diff (Scala vs SQL)\", \"x\": 9.241666793823242, \"y\": 6.684296131134033}, {\"index\": 5176, \"title\": \"Access tocken for applicatio\", \"x\": 8.890270233154297, \"y\": 3.9094631671905518}, {\"index\": 4211, \"title\": \"Metrics unavailable for an old Databricks cluster\", \"x\": 7.34865140914917, \"y\": 1.9681663513183594}, {\"index\": 3521, \"title\": \"How to sync s3 data to another workspace\", \"x\": 8.018233299255371, \"y\": 4.3895697593688965}, {\"index\": 3316, \"title\": \"SSO issue for https://pwc-nonprod.cloud.databricks.com/\", \"x\": 5.6674299240112305, \"y\": 2.5269198417663574}, {\"index\": 2087, \"title\": \"Error when using databricks-connect 10.4.0b0 with DBR 10.4-LTS\", \"x\": 8.34135627746582, \"y\": 3.5591413974761963}, {\"index\": 917, \"title\": \"[ARR][2205130040006196][AT & T]issue when writing huge number of records\", \"x\": 7.582375526428223, \"y\": 5.925971031188965}, {\"index\": 3215, \"title\": \"docker image is not working\", \"x\": 5.94592809677124, \"y\": 4.478617191314697}, {\"index\": 1601, \"title\": \"Keep data table up to data\", \"x\": 8.014276504516602, \"y\": 1.918673038482666}, {\"index\": 3539, \"title\": \"Job Aborted\", \"x\": 7.044131278991699, \"y\": 1.5935206413269043}, {\"index\": 559, \"title\": \"2205110030001121\", \"x\": 8.577954292297363, \"y\": 5.012248516082764}, {\"index\": 5120, \"title\": \"[ARR] [Sev B] SR-2203070030001594  Unable to pull repo\", \"x\": 4.619563102722168, \"y\": 1.7129818201065063}, {\"index\": 3732, \"title\": \"Unknown Error in Spark Streaming Job\", \"x\": 6.312362194061279, \"y\": 1.4032591581344604}, {\"index\": 707, \"title\": \"VPC\\u30ab\\u30b9\\u30bf\\u30e0\\u8a2d\\u5b9a\\u306b\\u304a\\u3051\\u308b\\u30af\\u30ed\\u30b9\\u30a2\\u30ab\\u30a6\\u30f3\\u30c8IAM\\u30ed\\u30fc\\u30eb\\u306b\\u4ed8\\u4e0e\\u3059\\u308b\\u30dd\\u30ea\\u30b7\\u30fc\\u306b\\u3064\\u3044\\u3066\\u306e\\u8cea\\u554f\", \"x\": 4.9655890464782715, \"y\": 1.603083848953247}, {\"index\": 4789, \"title\": \"CSS-ARR-S500-SR#2203150030000341-ModuleNotFoundError: No module named 'ConfigParser'\", \"x\": 10.109109878540039, \"y\": 3.034411668777466}, {\"index\": 583, \"title\": \"Failed to connect Azure SQL Server using ActiveDirectoryIntegrated in Databricks\", \"x\": 9.174330711364746, \"y\": 4.308531761169434}, {\"index\": 1208, \"title\": \"Metastore is Down- This error is appearing intermittently and during this time Externalk Hivemetastore is not available\", \"x\": 9.686413764953613, \"y\": 4.366964817047119}, {\"index\": 2970, \"title\": \"Cluster logs grows rapidly and uncontrollable, driving high costs, manual purge is unresponsive and unstable\", \"x\": 9.249606132507324, \"y\": 2.752276659011841}, {\"index\": 422, \"title\": \"is there any release on 20th or 21st May could impact the cypher in request socket\", \"x\": 7.932932376861572, \"y\": 5.130780220031738}, {\"index\": 2633, \"title\": \"2203180040006672SF || All Databricks pipelines failing at library installation\", \"x\": 7.441456317901611, \"y\": 3.59944486618042}, {\"index\": 3446, \"title\": \"ARR |  Data issue on enabling Photon\", \"x\": 7.707618236541748, \"y\": 3.9927685260772705}, {\"index\": 875, \"title\": \"ARR | 2205100040004471 | Unexpected failure while waiting for the cluster | JNJ \", \"x\": 9.534435272216797, \"y\": 3.914332151412964}, {\"index\": 5022, \"title\": \"ARR | 2111010060002465 | Follow up case of 00131582\", \"x\": 6.499078750610352, \"y\": 0.7347805500030518}, {\"index\": 4486, \"title\": \"Spark job reading a parquet file from a delta table failed with Connection reset error\", \"x\": 6.959848403930664, \"y\": 4.019381999969482}, {\"index\": 4444, \"title\": \"Getting Dataset refresh issue in Power BI dashboard\", \"x\": 5.7706756591796875, \"y\": 1.7131551504135132}, {\"index\": 3894, \"title\": \"Delta Live Tables tab not shown\", \"x\": 9.669938087463379, \"y\": 3.503805160522461}, {\"index\": 1566, \"title\": \"Editing clusters throwing errors\", \"x\": 7.668811321258545, \"y\": 4.19204568862915}, {\"index\": 3121, \"title\": \"SQL option not avaiable\", \"x\": 9.08554744720459, \"y\": 6.516714572906494}, {\"index\": 1183, \"title\": \"Bunch of ADF jobs \\\"Cancelled\\\" on single day\", \"x\": 8.37539005279541, \"y\": 2.0640969276428223}, {\"index\": 227, \"title\": \"Job performance\", \"x\": 8.60789966583252, \"y\": 5.169170379638672}, {\"index\": 826, \"title\": \"Unable to query View which has a complex structure column\", \"x\": 6.506379127502441, \"y\": 4.076047420501709}, {\"index\": 3724, \"title\": \" Perda de dados com conex\\u00e3o JDBC.\", \"x\": 5.898726463317871, \"y\": 4.389317989349365}, {\"index\": 4087, \"title\": \"Spark streaming job fails with a checkpoint not found error\", \"x\": 7.627323150634766, \"y\": 1.6810287237167358}, {\"index\": 2321, \"title\": \"0125 - g - ARR - cibc - Job time out\", \"x\": 7.392406940460205, \"y\": 1.887539267539978}, {\"index\": 4817, \"title\": \"query on time travel in delta tables\", \"x\": 8.266267776489258, \"y\": 3.9275479316711426}, {\"index\": 1636, \"title\": \"GPU Cluster Terminated - GCP Resource Stockout\", \"x\": 6.454364776611328, \"y\": 2.85439133644104}, {\"index\": 2828, \"title\": \"Python packages pip install takes too long\", \"x\": 7.075939655303955, \"y\": 0.31772008538246155}, {\"index\": 3948, \"title\": \"2203160010002405 | Job Slowness\", \"x\": 7.031988620758057, \"y\": 0.3132530450820923}, {\"index\": 3974, \"title\": \"Databricks private workspace\", \"x\": 8.764225006103516, \"y\": 4.474661350250244}, {\"index\": 4058, \"title\": \"New  Databricks E2 URL\", \"x\": 8.846781730651855, \"y\": 3.6321299076080322}, {\"index\": 1190, \"title\": \"Java error message in the Job execution - spark jar task\", \"x\": 7.065706729888916, \"y\": 0.4242442846298218}, {\"index\": 5548, \"title\": \"job is stuck at DBFS health check\", \"x\": 7.558236122131348, \"y\": 1.6495764255523682}, {\"index\": 1374, \"title\": \"6772 - ARR - Morgan Stanley - stderr stdout are not accessible on the executors.\", \"x\": 10.18333625793457, \"y\": 4.034336090087891}, {\"index\": 1569, \"title\": \"init script not getting installed while cluster creation using api\", \"x\": 6.595612525939941, \"y\": 0.8615711331367493}, {\"index\": 3704, \"title\": \"Vacuum not working as expected\", \"x\": 6.952081680297852, \"y\": 3.657562732696533}, {\"index\": 2134, \"title\": \"Error with data ingestion jobs\", \"x\": 9.89907169342041, \"y\": 2.6335198879241943}, {\"index\": 4575, \"title\": \"Databricks Job failure due to \\u201corg.apache.spark.SparkException: Job aborted\\u201d occasionally\", \"x\": 7.849710941314697, \"y\": 6.277034759521484}, {\"index\": 1808, \"title\": \"DLT UI Reverted to Beta Interface\", \"x\": 7.679832935333252, \"y\": 1.8726626634597778}, {\"index\": 825, \"title\": \"powerbi certificate stale issue\", \"x\": 8.780743598937988, \"y\": 6.24601936340332}, {\"index\": 4992, \"title\": \"need assistance with troubleshooting single-sign on SAML provider\", \"x\": 6.821930885314941, \"y\": 0.60111004114151}, {\"index\": 4021, \"title\": \"Digio's workspace not loading\", \"x\": 6.529881000518799, \"y\": 0.8330931663513184}, {\"index\": 2480, \"title\": \"Databricks Tables Restoration\", \"x\": 8.035767555236816, \"y\": 4.797146797180176}, {\"index\": 499, \"title\": \"IAM\\u30dd\\u30ea\\u30b7\\u30fc\\u306b\\u7d10\\u3065\\u3051\\u308bs3\\u30d0\\u30b1\\u30c3\\u30c8\\u3078\\u306e\\u30a2\\u30af\\u30b7\\u30e7\\u30f3\\u6a29\\u9650\\u306b\\u3064\\u3044\\u3066\", \"x\": 4.985971450805664, \"y\": 1.6086066961288452}, {\"index\": 4827, \"title\": \"9074 - ARR - UBS Workspace jobs more than 1500 error\", \"x\": 8.490609169006348, \"y\": 5.285538196563721}, {\"index\": 4651, \"title\": \"Cannot use Git Repo from Databricks\", \"x\": 9.025074005126953, \"y\": 6.412957191467285}, {\"index\": 816, \"title\": \"3432 - g - ARR - t-mobile - long-running jobs\", \"x\": 10.019607543945312, \"y\": 6.204371452331543}, {\"index\": 1316, \"title\": \"Error while migrating database from hive to glue\", \"x\": 9.432069778442383, \"y\": 6.52281379699707}, {\"index\": 3660, \"title\": \"ARR - AT&T - 2202230040008781 - executor failure\", \"x\": 7.490894317626953, \"y\": 1.9308606386184692}, {\"index\": 2533, \"title\": \"Cluster creation taking a long time even then using nodes from pool\", \"x\": 7.369653701782227, \"y\": 2.2465248107910156}, {\"index\": 3107, \"title\": \"AutoML experiment giving unknown error\", \"x\": 9.754938125610352, \"y\": 4.133091926574707}, {\"index\": 509, \"title\": \"AWS cloudwatch metrics\", \"x\": 9.792750358581543, \"y\": 6.791294574737549}, {\"index\": 146, \"title\": \"ARR | 2205270030000850 - slowness in job\", \"x\": 7.830723285675049, \"y\": 1.1508525609970093}, {\"index\": 4689, \"title\": \"ARR | Connectivity issue with Tableau Server  | 2203100030002115\", \"x\": 9.763452529907227, \"y\": 3.004870891571045}, {\"index\": 5238, \"title\": \"Invite User Email results in error\", \"x\": 6.590056896209717, \"y\": 2.6108460426330566}, {\"index\": 629, \"title\": \"ARR| AzureResourceProviderThrottling during cluster upsize | 2205170040004867 \", \"x\": 6.489654064178467, \"y\": 0.8725594282150269}, {\"index\": 1536, \"title\": \"follow-up of SF # 00140194\", \"x\": 6.933969020843506, \"y\": 1.4870010614395142}, {\"index\": 5590, \"title\": \"Integrate Power Apps with Azure Databricks - Connector\", \"x\": 6.970261096954346, \"y\": 3.4258906841278076}, {\"index\": 1196, \"title\": \"workspace access request, HC Cluster is running slowly\", \"x\": 8.089296340942383, \"y\": 3.776994228363037}, {\"index\": 5314, \"title\": \"Increasing transaction log and deleted file retention\", \"x\": 6.0652971267700195, \"y\": 4.440212726593018}, {\"index\": 470, \"title\": \"ARR 2205230050000756 - Databricks jobs stopped\", \"x\": 10.599733352661133, \"y\": 4.327216625213623}, {\"index\": 4181, \"title\": \"SAMLv2 Databricks metadata file\", \"x\": 8.205191612243652, \"y\": 6.334960460662842}, {\"index\": 3113, \"title\": \"Databricks SQL - Dashboard update error presentation\", \"x\": 10.358840942382812, \"y\": 4.052009582519531}, {\"index\": 1789, \"title\": \"ARR: multiple notebook run happening at the same time with different parameters:SR2204210060006686\", \"x\": 6.515436172485352, \"y\": 1.9764797687530518}, {\"index\": 3991, \"title\": \"Can we send a failure with the error logs when job failed\", \"x\": 7.664833068847656, \"y\": 1.6928958892822266}, {\"index\": 4636, \"title\": \"Follow-up case on 00129213\", \"x\": 9.503649711608887, \"y\": 4.086860179901123}, {\"index\": 204, \"title\": \"TypeError when importing feature store\", \"x\": 6.3134660720825195, \"y\": 3.32995867729187}, {\"index\": 2822, \"title\": \"Reading Delta tables using Hive not working as expected\", \"x\": 7.8481035232543945, \"y\": 2.555290699005127}, {\"index\": 2005, \"title\": \"[Follow-up 00141646] ARR | Intermitten error: Table or view not found\", \"x\": 8.448577880859375, \"y\": 5.415493965148926}, {\"index\": 5198, \"title\": \"ARR 2203080040000533 job cluster launch failure\", \"x\": 6.990174293518066, \"y\": 0.9449201822280884}, {\"index\": 2941, \"title\": \"2204040040005575 | Synapse failure\", \"x\": 6.255396366119385, \"y\": 2.839480400085449}, {\"index\": 3642, \"title\": \"Unable to set up SSO for workspace\", \"x\": 10.635716438293457, \"y\": 4.288712024688721}, {\"index\": 2650, \"title\": \"Configuring Delta Live Tables Cluster with Access Role\", \"x\": 9.624709129333496, \"y\": 2.60007905960083}, {\"index\": 2747, \"title\": \"Cluster unusual memory and stuck in bad state\", \"x\": 8.300816535949707, \"y\": 6.3959479331970215}, {\"index\": 2352, \"title\": \"Secrets not being redacted in driver log - 2204140010001013\", \"x\": 9.367931365966797, \"y\": 3.3434274196624756}, {\"index\": 1754, \"title\": \"Faced [adb-dp-8081006549057212.12.azuredatabricks.net refused to connect] on Ganglia tabs except  [Main] table\", \"x\": 4.49972677230835, \"y\": 1.7272988557815552}, {\"index\": 1215, \"title\": \"Query stuck in running state\", \"x\": 9.971848487854004, \"y\": 4.308662414550781}, {\"index\": 3841, \"title\": \"Z-Ordering on column for which stats is not collected\", \"x\": 6.456912994384766, \"y\": 4.424092769622803}, {\"index\": 4960, \"title\": \"To Address ConcurrentDeleteReadException in Dev Environment\", \"x\": 7.856784343719482, \"y\": 1.2762895822525024}, {\"index\": 4542, \"title\": \"ARR | Inquiry Regarding GPU Usage On Databricks Cluster over period of time 2/8 - 2/14 | 2203140040008801\", \"x\": 9.252985954284668, \"y\": 2.9971373081207275}, {\"index\": 1451, \"title\": \"Failed to open Delta Live Tables Pipeline page.UI related issue.(Continuation  of 00143631)\", \"x\": 7.641073226928711, \"y\": 4.3950395584106445}, {\"index\": 2215, \"title\": \"UI bug in Repos following deployment\", \"x\": 10.38354778289795, \"y\": 2.77384090423584}, {\"index\": 3581, \"title\": \"Failed to load directory: socket closed - When accessing the workspace\", \"x\": 8.047503471374512, \"y\": 4.2112135887146}, {\"index\": 1903, \"title\": \"Add IP to allow list on shard003.cloud.databricks.com\", \"x\": 9.528225898742676, \"y\": 2.799835205078125}, {\"index\": 979, \"title\": \"Databricks Migrate lab solution Export script is failing while exporting for Notebook ACL in Production Workspace\", \"x\": 9.585784912109375, \"y\": 3.775895357131958}, {\"index\": 549, \"title\": \"2205100050002215\", \"x\": 9.907075881958008, \"y\": 3.076016426086426}, {\"index\": 5357, \"title\": \"New VPC Security group ID\", \"x\": 10.101216316223145, \"y\": 4.0527167320251465}, {\"index\": 4395, \"title\": \"service down\", \"x\": 5.715167045593262, \"y\": 4.483185768127441}, {\"index\": 2266, \"title\": \"ARR | throttling leading to cluster creation failure | 2204220060002439\", \"x\": 6.022034168243408, \"y\": 1.9830870628356934}, {\"index\": 3909, \"title\": \"correlated subquery with non-equality predicate\", \"x\": 6.552972793579102, \"y\": 4.533905982971191}, {\"index\": 1425, \"title\": \"Increase input rate for DLT\", \"x\": 6.772054672241211, \"y\": 5.484551429748535}, {\"index\": 3644, \"title\": \"pip install only once when a notebook is attached to a cluster\", \"x\": 6.900935649871826, \"y\": 2.405010461807251}, {\"index\": 4125, \"title\": \"Databricks not writing custom logs to Log4J & Log Analytics\", \"x\": 5.156918048858643, \"y\": 0.8925619125366211}, {\"index\": 576, \"title\": \"ARR- ATT - 2205190040007411 - job is running more than 18 hours\", \"x\": 10.142400741577148, \"y\": 3.090022087097168}, {\"index\": 2807, \"title\": \"users are not able to start the endpoint\", \"x\": 10.590723037719727, \"y\": 2.9621334075927734}, {\"index\": 1452, \"title\": \"[ARR] [Sev B] SR-2205050010002241 java.lang.OutOfMemoryError: GC overhead limit exceeded\", \"x\": 7.894092559814453, \"y\": 6.282448768615723}, {\"index\": 4012, \"title\": \"Mlflow Model servicing goes to pending state frequently\", \"x\": 8.152644157409668, \"y\": 4.371315956115723}, {\"index\": 445, \"title\": \"ARR Customer Giliead Sciences: Issue - Module OS has no attribute PathLike\", \"x\": 6.970973968505859, \"y\": 0.41191115975379944}, {\"index\": 3985, \"title\": \"GCP kubernetes cluster high cost\", \"x\": 7.121450901031494, \"y\": 2.3514018058776855}, {\"index\": 3467, \"title\": \"Intermittent File not found error when accessing ADLS Mounted locations\", \"x\": 8.244232177734375, \"y\": 4.460692405700684}, {\"index\": 627, \"title\": \"request to know the steps to create service account and service principal via databricks UI\", \"x\": 6.553919315338135, \"y\": 0.8461817502975464}, {\"index\": 347, \"title\": \"Job aborted due to stage failure\", \"x\": 6.251977443695068, \"y\": 3.031806468963623}, {\"index\": 341, \"title\": \"{CSAT Impacting}Cannot Add Scope for Key Vault in the Databricks side\", \"x\": 8.92600154876709, \"y\": 5.035450458526611}, {\"index\": 4507, \"title\": \"ARR | Standard Chartered Bank | JupyterDash cannot be displayed in databricks notebook| SR:2203090030001237\", \"x\": 7.091002464294434, \"y\": 0.3165283501148224}, {\"index\": 5407, \"title\": \"Job IDs have different scale and are no longer sequential\", \"x\": 6.3032050132751465, \"y\": 2.1381170749664307}, {\"index\": 777, \"title\": \"Job_Aborted_Error\", \"x\": 5.3694353103637695, \"y\": 2.1746084690093994}, {\"index\": 4160, \"title\": \"Permissions bug in webhooks API\", \"x\": 8.16779899597168, \"y\": 5.339629173278809}, {\"index\": 5232, \"title\": \"Unity Catalog help\", \"x\": 8.597308158874512, \"y\": 6.192809104919434}, {\"index\": 3656, \"title\": \"Error messages unclear and not detailed to the problem that actually exists in DBSQL \", \"x\": 7.784611701965332, \"y\": 1.9985731840133667}, {\"index\": 5262, \"title\": \"Unclear documentation\", \"x\": 7.2657060623168945, \"y\": 4.940327167510986}, {\"index\": 303, \"title\": \"Spark image processing very slow\", \"x\": 6.066751003265381, \"y\": 4.353108882904053}, {\"index\": 926, \"title\": \"Engineering cluster required restart due to unresponsive notebooks\", \"x\": 6.820112705230713, \"y\": 1.407038927078247}, {\"index\": 3822, \"title\": \" job failure/long running without change to job.\", \"x\": 5.556553840637207, \"y\": 4.219658374786377}, {\"index\": 675, \"title\": \"follow-up for 00144952\", \"x\": 7.5992937088012695, \"y\": 5.772414207458496}, {\"index\": 4679, \"title\": \"DBU Costs\", \"x\": 9.610318183898926, \"y\": 3.539726495742798}, {\"index\": 2308, \"title\": \"Assistance required in performance tuning for data ingestion from RDS \", \"x\": 9.810612678527832, \"y\": 3.6628379821777344}, {\"index\": 584, \"title\": \"Deserializing JSON to a class produces strange stdout outputs.\", \"x\": 7.842466831207275, \"y\": 4.455769062042236}, {\"index\": 2012, \"title\": \"Urgent Customer Queries regarding  SIMBA JDBC Delta Lake\", \"x\": 5.266843795776367, \"y\": 2.196929454803467}, {\"index\": 3904, \"title\": \"how to get host ip address in databricks cluster aws\", \"x\": 8.54444408416748, \"y\": 3.6478235721588135}, {\"index\": 5151, \"title\": \"Dashboards disappeared\", \"x\": 7.726314544677734, \"y\": 3.731879234313965}, {\"index\": 3839, \"title\": \"2203290030003731 | cluster not starting\", \"x\": 6.523261070251465, \"y\": 3.7433395385742188}, {\"index\": 2869, \"title\": \"Jobs failing with cluster time-out issue\", \"x\": 8.273630142211914, \"y\": 6.0121169090271}, {\"index\": 241, \"title\": \"Facing error while running job\", \"x\": 8.895419120788574, \"y\": 3.028137683868408}, {\"index\": 1862, \"title\": \"2204290030000035\", \"x\": 6.782774925231934, \"y\": 1.5674105882644653}, {\"index\": 1479, \"title\": \"Power BI connector not working\", \"x\": 6.996487140655518, \"y\": 2.0757200717926025}, {\"index\": 2014, \"title\": \"[S500] Databricks library installations are failing\", \"x\": 5.806434154510498, \"y\": 3.4314470291137695}, {\"index\": 3778, \"title\": \"Databricks GPU cluster is not working\", \"x\": 7.311036586761475, \"y\": 2.075867176055908}, {\"index\": 786, \"title\": \"[ARR] Very long runtime for the job\", \"x\": 8.282526016235352, \"y\": 3.647329092025757}, {\"index\": 3307, \"title\": \"Permissions issue on databricks cluster while getting Audit logs\", \"x\": 6.999314308166504, \"y\": 0.3460884392261505}, {\"index\": 170, \"title\": \"updating tags fails.\", \"x\": 7.94630765914917, \"y\": 4.827589988708496}, {\"index\": 2536, \"title\": \"databricks_permissions in terraform overwriting resource instead of appending it\", \"x\": 8.855717658996582, \"y\": 3.251664400100708}, {\"index\": 4052, \"title\": \"sample case creation testing\", \"x\": 6.308543682098389, \"y\": 1.703412652015686}, {\"index\": 896, \"title\": \"library on DBFS failed to install\", \"x\": 6.8376383781433105, \"y\": 3.058377265930176}, {\"index\": 4197, \"title\": \"Can't connect to synapse from the Databricks\", \"x\": 7.0862650871276855, \"y\": 0.3260958194732666}, {\"index\": 2775, \"title\": \"Install Gaitpy in DBR with Python 3.8\", \"x\": 9.69016170501709, \"y\": 2.497358560562134}, {\"index\": 1376, \"title\": \"2205 - g - ARR - GAP - External data access using ACLs\", \"x\": 5.982030391693115, \"y\": 3.711402416229248}, {\"index\": 4120, \"title\": \"2203230040005430 \\\"\\u2018JavaPackage\\u2019 object is not callable\\\" when using pypmml-spark library in Databricks\", \"x\": 8.737152099609375, \"y\": 5.180953502655029}, {\"index\": 4584, \"title\": \"Cluster issue\", \"x\": 9.46245288848877, \"y\": 3.22391676902771}, {\"index\": 5244, \"title\": \"Issues with Delta Concurrency Control\", \"x\": 8.579834938049316, \"y\": 4.368526458740234}, {\"index\": 4078, \"title\": \"SCIM API GA schedule\", \"x\": 6.491809844970703, \"y\": 2.7115445137023926}, {\"index\": 477, \"title\": \"ARR | 2205230030000498 - Databricks_API_issue\", \"x\": 8.434272766113281, \"y\": 2.9394516944885254}, {\"index\": 5433, \"title\": \"GAR for WS 201750907031347\", \"x\": 9.63846492767334, \"y\": 5.802745819091797}, {\"index\": 337, \"title\": \"Need to check why the model training for so long and there has a good run for this.\", \"x\": 6.400118350982666, \"y\": 1.149576187133789}, {\"index\": 1895, \"title\": \"Cell stuck in \\\"Running Command ...\\\"\", \"x\": 9.746576309204102, \"y\": 3.540579319000244}, {\"index\": 320, \"title\": \"Cluster terminated.Reason:Self Bootstrap Failure\", \"x\": 6.618609428405762, \"y\": 1.6348949670791626}, {\"index\": 2129, \"title\": \"7441 - ARR - UHG - Fairlearn dashboard spins with CORS error where databricksusercontent.com content is attempting to retrieve from the azuredatabricks.net\", \"x\": 8.033297538757324, \"y\": 1.913501262664795}, {\"index\": 5327, \"title\": \"Streaming Hangs\", \"x\": 4.554293632507324, \"y\": 1.7352796792984009}, {\"index\": 4568, \"title\": \"Data getting rejected, going into _corrupted_record\", \"x\": 7.237325191497803, \"y\": 2.0689592361450195}, {\"index\": 635, \"title\": \"2205180030001357 CLOUD_PROVIDER_LAUNCH_FAILURE(CLOUD_FAILURE)\", \"x\": 8.754308700561523, \"y\": 3.7087178230285645}, {\"index\": 620, \"title\": \"958145594491036 - 10280a44-7716-4c2a-8238-1b1796d4fde3 - 2205190040004859\", \"x\": 10.27552318572998, \"y\": 3.20393443107605}, {\"index\": 3676, \"title\": \"Cluster terminates due to inactivity during training \", \"x\": 7.945525169372559, \"y\": 3.1569876670837402}, {\"index\": 5040, \"title\": \"Gennie Access\", \"x\": 7.049911975860596, \"y\": 0.34564509987831116}, {\"index\": 1658, \"title\": \"ARR - Sev A - Cluster terminated.Reason:Azure Vm Extension Failure\", \"x\": 8.324207305908203, \"y\": 6.073943138122559}, {\"index\": 2102, \"title\": \"Init script\", \"x\": 10.178914070129395, \"y\": 3.75520920753479}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['x'] = umap_embeds[:,0]\n",
    "df1['y'] = umap_embeds[:,1]\n",
    "\n",
    "# Plot\n",
    "chart = alt.Chart(df1).mark_circle(size=60).encode(\n",
    "    x=#'x',\n",
    "    alt.X('x',\n",
    "        scale=alt.Scale(zero=False),\n",
    "        axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
    "    ),\n",
    "    y=\n",
    "    alt.Y('y',\n",
    "        scale=alt.Scale(zero=False),\n",
    "        axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
    "    ),\n",
    "    tooltip=['title']\n",
    ").configure(background=\"#FDF7F0\"\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400,\n",
    "    title='Ask HN: top 3,000 posts'\n",
    ")\n",
    "\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5eda76-feeb-4540-8a0e-632087dcae73",
   "metadata": {},
   "source": [
    "# 4- Cluster the posts to identify the major common themes\n",
    "Let's proceed the cluster the embeddings using KMeans from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "818e60df-2d9a-4b5d-b0ba-e8eebec37882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the number of clusters\n",
    "n_clusters=8\n",
    "\n",
    "# Cluster the embeddings\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "classes = kmeans_model.fit_predict(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6100f4-54d6-40f6-a17d-6e36f5b4167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 0, ..., 5, 7, 0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd167f9-d06f-4a63-bc61-bd3069e47064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('data_server')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.enable('data_server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d8fd1a-4c51-4b1f-b875-c8e55be3cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the keywords for each cluster\n",
    "documents =  df1['title']\n",
    "documents = pd.DataFrame({\"Document\": documents,\n",
    "                          \"ID\": range(len(documents)),\n",
    "                          \"Topic\": None})\n",
    "documents['Topic'] = classes\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\").fit(documents_per_topic.Document)\n",
    "count = count_vectorizer.transform(documents_per_topic.Document)\n",
    "words = count_vectorizer.get_feature_names()\n",
    "ctfidf = ClassTFIDF().fit_transform(count).toarray()\n",
    "words_per_class = {label: [words[index] for index in ctfidf[label].argsort()[-10:]] for label in documents_per_topic.Topic}\n",
    "df1['cluster'] = classes\n",
    "df1['keywords'] = df1['cluster'].map(lambda topic_num: \", \".join(np.array(words_per_class[topic_num])[:]))\n",
    "#Plot with clusters and keywords information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea88cc1-b4ca-4009-8ccb-665a091a310d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>cluster</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2624</td>\n",
       "      <td>Deprecated Cluster Runtime 6.4 unable to hit API'S</td>\n",
       "      <td>6.804984</td>\n",
       "      <td>4.725465</td>\n",
       "      <td>7</td>\n",
       "      <td>error, workspace, access, failure, jobs, job, arr, unable, cluster, databricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5493</td>\n",
       "      <td>Heartbeat issues for larger dataset</td>\n",
       "      <td>6.361811</td>\n",
       "      <td>3.225446</td>\n",
       "      <td>5</td>\n",
       "      <td>jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5565</td>\n",
       "      <td>Delta files saving data type as String for Boolean column</td>\n",
       "      <td>9.575385</td>\n",
       "      <td>2.680216</td>\n",
       "      <td>0</td>\n",
       "      <td>data, sql, unable, jobs, issue, cluster, error, arr, job, databricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3553</td>\n",
       "      <td>Databricks Spring Core Vulnerability Impact</td>\n",
       "      <td>6.184659</td>\n",
       "      <td>2.479959</td>\n",
       "      <td>5</td>\n",
       "      <td>jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1198</td>\n",
       "      <td>Not able to create JDBC connection</td>\n",
       "      <td>9.715446</td>\n",
       "      <td>6.891146</td>\n",
       "      <td>3</td>\n",
       "      <td>workspace, spark, cluster, query, sql, error, unable, job, arr, databricks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                      title         x  \\\n",
       "0   2624         Deprecated Cluster Runtime 6.4 unable to hit API'S  6.804984   \n",
       "1   5493                        Heartbeat issues for larger dataset  6.361811   \n",
       "2   5565  Delta files saving data type as String for Boolean column  9.575385   \n",
       "3   3553                Databricks Spring Core Vulnerability Impact  6.184659   \n",
       "4   1198                         Not able to create JDBC connection  9.715446   \n",
       "\n",
       "          y  cluster  \\\n",
       "0  4.725465        7   \n",
       "1  3.225446        5   \n",
       "2  2.680216        0   \n",
       "3  2.479959        5   \n",
       "4  6.891146        3   \n",
       "\n",
       "                                                                         keywords  \n",
       "0  error, workspace, access, failure, jobs, job, arr, unable, cluster, databricks  \n",
       "1        jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks  \n",
       "2            data, sql, unable, jobs, issue, cluster, error, arr, job, databricks  \n",
       "3        jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks  \n",
       "4      workspace, spark, cluster, query, sql, error, unable, job, arr, databricks  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6124480d-d1ca-4814-a75c-9c6f8f4c2ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-15a8e12206f741ec83da298e9cbd5407\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-15a8e12206f741ec83da298e9cbd5407\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-15a8e12206f741ec83da298e9cbd5407\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"background\": \"#FDF7F0\"}, \"data\": {\"url\": \"http://localhost:52470/f80cdd581eded5ed2d62e14358cf630a.json\"}, \"mark\": {\"type\": \"circle\", \"opacity\": 0.3, \"size\": 60, \"stroke\": \"#666\", \"strokeWidth\": 1}, \"encoding\": {\"color\": {\"field\": \"keywords\", \"legend\": {\"columns\": 1, \"labelFontSize\": 14, \"symbolLimit\": 0}, \"type\": \"nominal\"}, \"href\": {\"field\": \"url\", \"type\": \"nominal\"}, \"opacity\": {\"condition\": {\"value\": 1, \"selection\": \"selector002\"}, \"value\": 0.2}, \"tooltip\": [{\"field\": \"title\", \"type\": \"nominal\"}, {\"field\": \"keywords\", \"type\": \"nominal\"}, {\"field\": \"cluster\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false}, \"field\": \"x\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false}, \"field\": \"y\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}}, \"height\": 500, \"selection\": {\"selector002\": {\"type\": \"multi\", \"fields\": [\"keywords\"], \"bind\": \"legend\"}, \"selector003\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Data\", \"transform\": [{\"calculate\": \"('https://news.ycombinator.com/item?id=' + datum.id)\", \"as\": \"url\"}], \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = alt.selection_multi(fields=['keywords'], bind='legend')\n",
    "\n",
    "chart = alt.Chart(df1).transform_calculate(\n",
    "    url='https://news.ycombinator.com/item?id=' + alt.datum.id\n",
    ").mark_circle(size=60, stroke='#666', strokeWidth=1, opacity=0.3).encode(\n",
    "    x=#'x',\n",
    "    alt.X('x',\n",
    "        scale=alt.Scale(zero=False),\n",
    "        axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
    "    ),\n",
    "    y=\n",
    "    alt.Y('y',\n",
    "        scale=alt.Scale(zero=False),\n",
    "        axis=alt.Axis(labels=False, ticks=False, domain=False)\n",
    "    ),\n",
    "    href='url:N',\n",
    "    color=alt.Color('keywords', \n",
    "                    legend=alt.Legend(columns=1, symbolLimit=0, labelFontSize=14)\n",
    "                   ),\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    tooltip=['title', 'keywords', 'cluster']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=500\n",
    ").add_selection(\n",
    "    selection\n",
    ").configure_legend(labelLimit= 0).configure_view(\n",
    "    strokeWidth=0\n",
    ").configure(background=\"#FDF7F0\").properties(\n",
    "    title='Data'\n",
    ")\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4490938b-6f10-40fe-b207-cb5323fa8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0dd5687-af55-415c-b4b0-0b2f2ed92d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobs, delta, sql, spark, failing, job, error, arr, cluster, databricks            562\n",
       "data, sql, unable, jobs, issue, cluster, error, arr, job, databricks              539\n",
       "jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks          414\n",
       "data, access, failing, error, job, issue, sql, cluster, arr, databricks           391\n",
       "sql, data, failing, issue, unable, error, cluster, arr, job, databricks           336\n",
       "error, workspace, access, failure, jobs, job, arr, unable, cluster, databricks    327\n",
       "workspace, spark, cluster, query, sql, error, unable, job, arr, databricks        224\n",
       "sql, sr, jobs, job, table, issue, cluster, arr, error, databricks                 207\n",
       "Name: keywords, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.keywords.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6296528-ef41-4761-994d-093b7c001d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['jobs, delta, sql, spark, failing, job, error, arr, cluster, databricks','data, sql, unable, jobs, issue, cluster, error, arr, job, databricks  '\n",
    "              'jobs, sql, failing, issue, failure, error, arr, job, cluster, databricks','data, access, failing, error, job, issue, sql, cluster, arr, databricks',\n",
    "              'sql, data, failing, issue, unable, error, cluster, arr, job, databricks','error, workspace, access, failure, jobs, job, arr, unable, cluster, databricks','workspace, spark, cluster, query, sql, error, unable, job, arr, databricks',\n",
    "              'sql, sr, jobs, job, table, issue, cluster, arr, error, databricks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb299df-902c-4e2a-a9d7-885fa7f92c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = hierarchy.linkage(kmeans_model.cluster_centers_, 'single')\n",
    "dn = hierarchy.dendrogram(Z, orientation='right'\n",
    "                         )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8355ff5e-d55a-4382-92c7-4cd0cacc3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://txt.cohere.ai/combing-for-insight-in-10-000-hacker-news-posts-with-text-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "783a7439-6e47-4fa5-8aaf-ec0f95e84f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://os.cohere.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e8d17-b32a-451e-8370-4bd180addbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
